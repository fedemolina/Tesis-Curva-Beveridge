---
title: "Entrega Final \n Maestría en Economía"
author: "Federico Molina Magne"
date: "Viernes 14 de Diciembre"
output:
  pdf_document:
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
lang: es
geometry: margin=2cm
fontsize: 10pt
---
 
\newpage
```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	results = 'asis',
	options(xtable.comment = FALSE)
)
library(ggplot2)
library(dplyr)
library(tidyr)
library(skimr)
library(tidyverse)
library(reshape2)
library(forcats)
rm(list = ls())
theme_set(theme_minimal())
```

[Ver mi repositorio](https://github.com/fedemolina/scraping)

# Introducción

La presente entrega tiene por objetivo aproximarse a la caracterización de la demanda laboral por parte de las empresas en Uruguay en el periodo de noviembre a diciembre de 2018. Ello quiere decir, analizar cuales son los niveles técnicos solicitados, las áreas (dentro de la firma) y los departamentos a los cuales corresponden los avisos. Como adelanto, cerca del 90-95% de las publicaciones corresponden al departamento de Montevideo, por lo cual, el análisis es mas bien a nivel departamental.

Dado que en Uruguay no existe ninguna encuesta referida a vacantes laborales solicitadas por las empresas, se opto por obtener los datos de avisos laborales publicados en internet, técnica conocida como 'scraping'. Las fuentes de datos son dos, el portal web 'el gallito' y la página web computrabajo, mientras que la unidad de análisis es el aviso laboral. 

El presente trabajo, es el primer paso dentro de un proyecto superior cuyo objetivo es la creación de una serie de vacantes laborales, como proxy a la demanda laboral, representativa al menos de Montevideo. Para ello, deben ser identificados los avisos laborales que son repetidos tanto en el espacio como en el tiempo. Es decir, los avisos que se repiten en un intervalo de tiempo a determinar, los cuales serán considerados repetidos, y, los que se repiten en un mismo momento del tiempo en distintos sitios web pero que deben ser considerados una sola vez. Dicha tarea tiene la dificultad de que la estructura de las páginas web es distinta, por lo cual, el mismo aviso puede ser publicado de forma diferente. La forma más intuitiva de solucionarlo es identificar puesto, empresa, fecha y similitud, este último mediante análisis de texto (por ejemplo, similaridad de cosenos).

# Obtención y análisis de datos

Respecto a las dificultades ya mencionadas en la introducción, aparece una adicional al realizar el scraping de cada página web. Existen dos opciones, la primera es iterar en cada portal web a través de cada página que contiene en general unos 20 avisos, entonces si hay 180 páginas completas existen 3600 avisos. La segunda, es obtener el link de cada aviso (iterando previamente por cada página) e iterar individualmente sobre cada link obteniendo toda la información detallada, sin embargo, aquí surge una dificultad adicional, ya que, la información detallada no sigue una estructura sino que se modifica en cada aviso laboral (esto es lo que sucede con computrabajo).

La estrategia utilizada fue primero iterar en cada portal web a nivel de página y obtener la información de cada aviso, incluido el link del aviso el cual actuará posteriormente como identificador. Luego realizar una exploración a nivel individual y guardar cada set de datos por separado. Esto último debido a que al momento de iterar por cada aviso individualmente, se produjeron errores en algunos links y el tiempo para llevar a cabo la tarea aumenta dramáticamente.

Los conjuntos de datos obtenidos del scraping son guardados a nivel local, dado que los avisos van desapareciendo de las páginas web a medida que pasa el tiempo, los mismos son guardados en un repositorio a nivel local. Los códigos utilizados están disponibles en el repositorio de github escrito anteriormente.

Se realiza una limpieza de datos de ambas páginas web el cual se encuentra en [ver código limpieza de datos](https://github.com/fedemolina/scraping), dicha información es cargada posteriormente para realizar el análisis exploratorio de los datos.

## El Gallito

[Ver código scraping 'El Gallito'](https://github.com/fedemolina/scraping/blob/master/scraping_gallito.R).
El código referido a la obtención de los datos de la página web 'el gallito' pueden verse en el link superior. A continuación se analiza la información obtenida.

```{r 'carga datos'}
ga <- readRDS(paste(getwd(),'/analisis_gallito/',dir('analisis_gallito')[1],sep = ""))
```
El primer análisis realizado es en que días de la semana se concentra la publicación de avisos laborales en 'El Gallito'. En este caso, los días con mayor cantidad de publicaciones son los miércoles y lunes, por el contrario el domingo es de menor cantidad de publicaciones. Es imporatante destacar que la fecha no es exacta dado que, a medida que transcurren los días en la página web aparece el texto "publicado hace 1 semana", "publicado hace 2 semanas", "publicado hace 1 mes". Fueron dichos patrones los que se transformaron en segundos y los mismos fueron restados a la fecha de scraping obteniendo la supuesta fecha de publicación.

```{r 'fecha1', fig.cap = 'Avisos laborales por día de la semana'}
ga %>% ggplot(., aes(x = dia, y = ..count..)) +
  geom_bar(aes(fill= ..count..)) +
  labs(x = "", y = "cantidad de avisos", 
       title = "Avisos laborales por día de la semana",
       caption = "Cantidad de avisos laborales publicados en portal web 'El Galito' 12/Nov - 12/Dic 2018",
       fill = "Cantidad\n avisos")
```

A continuación se crea una serie de tiempo desde noviembre a diciembre de los avisos publicados, como se mencionó previamente, observar la fecha exacta de publicación no es posible (a menos que se realice una obtención de datos diaria o semanal), pero si puede realizarse una comparación a nivel semanal. Lo que se observa es que en los últimos días de noviembre se concentra la mayor cantidad de publicaciones. 

```{r 'fecha2', fig.cap='Avisos laborales por fecha de publicación'}
ga %>% count(fpub) %>% ggplot(., aes(x=fpub, y = n)) +
  geom_line(color = "#FC4E07", size = 2) +
  labs(x = 'fecha de publicación', y = 'cantidad de avisos', 
       title = "Avisos laborales por fecha de publicación",
       caption = "Avisos laborales publicados en portal web 'El Galito' 12/Nov - 12/Dic 2018. La fecha de publicación no es exacta")
```


```{r 'area1', fig.cap='Áreas de actividad solicitadas por las empresas'}
ga %>% ggplot(., aes(x = fct_infreq(f = area, ordered = TRUE))) +
  geom_bar(aes(fill = ..count..)) +
  labs(y = "cantidad de avisos", x = "Áreas", fill = "cantidad", 
       title = "Áreas de actividad solicitadas",
       caption = "Cantidad de avisos laborales publicados en 'El Galito' 12/Nov - 12/Dic 2018") +
  coord_flip() +
  geom_hline(yintercept = c(50,100,150), linetype = "dotted", 
             color = "red", size = 1)
```

```{r 'nivel', fig.cap='Nivel técnico solicitado por las empresas'}
ga %>% ggplot(., aes(nivel)) +
  geom_bar() +
  coord_polar(theta = "x",direction = 1, start = -50) +
  labs(y = "Cantidad de avisos", x = "Nivel" , title = "Avisos laborales portal web 'El Gallito'",
       caption = "Avisos laborales 'El Gallito' 12/Nov - 12/Dic 2018. Nivel técnico solicitado por empresas")
```

```{r 'area y nivel', fig.cap='Nivel técnico y actividad solicitado por las firmas '}
ga  %>%  
  ggplot(., aes(area, nivel)) +
  geom_count(aes(color = ..n.., size = ..n..)) +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(y = "Nivel", x = "Área", title = "Nivel técnico y Área laboral", 
       size = "cantidad\n avisos", color = "cantidad\n avisos",
       caption = "Avisos laborales publicados en portal web 'El Gallito' \n entre 12/Nov - 12/Dic de 2018. Área y nivel técnico solicitado")
```

```{r 'dpto', fig.cap='Departamentos al que pertecenen las firmas que solicitan os puestos laborales'}
table(ga$dpto)
ga %>% ggplot(., aes(dpto)) +
  geom_bar() +
  labs(x = "Departamento", y = "Cantidad de avisos")
 # coord_polar()
```


## Computrabajo



# Pasos siguientes
