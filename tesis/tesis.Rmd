---
author: 'Federico Molina Magne'
date: '13 de abril de 2020'
institution: 'Universidad de la República'
division: 'Facultad de Ciencias Económicas y de Administración'
advisor: 'Rodrigo Ceni'
# If you have more two advisors, un-silence line 7
#altadvisor: 'Your Other Advisor'
department: 'Economía'
degree: 'Magíster en Economía'
title: 'Curva de Beveridge'
knit: "bookdown::render_book"
always_allow_html: true
site: bookdown::bookdown_site
output: 
  # html_notebook
  thesisdown::thesis_pdf: default
  # thesisdown::thesis_gitbook: default
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default
# If you are creating a PDF you'll need to write your preliminary content (e.g., abstract, acknowledgements) here or
# use code similar to line 22-23 for the .RMD files. If you are NOT producing a PDF, you can delete or silence lines 21-32 in this YAML header.
abstract: |
  `r if(knitr:::is_latex_output()) paste(readLines("00-abstract.Rmd"), collapse = '\n  ')`
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab is 
# needed on the line after the `|`.
acknowledgements: |
  Quiero agradecer a toda mi familia y amigos en el extranjero, a mi tía Maritza, a Norberto y mis primas Michelle y Melanie. Mi abuela _Lulo_ y tíos, Jorge y Paola. A todos los familiares en Juan Lacaze, especialmente Teresita, Caro, Nico, Valeria, Antonio, Rubén y _la Guti_. A mis grandes amigos, casi hermanos, Pancho (y sus padres y _la Tere_), Casteglioni, Wismael, Paoli, Motuco, Manolete (y su padre), a la banda _Supercheto_ y otros tantos que quedan por mencionar. A mis amigos en Uruguay, Rafa, Joaco, Claudio, Erika, Germán, Augusto y Juan. También a Natalia con quien trabajé el último año en el IESTA y fue una gran experiencia. A Silvia de quién fui ayudante el año pasado y este año ya soy ayudante oficial. Y especialmente a mi tutor, Rodrigo que me motivó reiteradamente cuando bajó mi ritmo de estudio y me ayudó fuertemente para que el trabajo saliese lo mejor posible, leyendo y corrigiendo una y otra vez mis continuos errores.
  
  Por último quiero agradecer encarecidamente a Papá, mi abuela Mirna, mi novia Vanessa, Mamá y mi hermana. Porque en cada momento que me ha tocado estar con ustedes me han ayudado de forma permanente. 
  A todos muchas gracias.
dedication: |
  Dedico este trabajo a mi abuelo quien acaba de fallecer y me dejo muchas enseñanzas, de las cuales quiero destacar dos. La primera es que no importa la circunstancia, siempre se debe actuar de forma correcta y ser honesto. La segunda es que fue quien me inculco que estudiar es la única forma de superarse y salir adelante, como siempre me decía (y yo no hacía caso porque era un vago) "¿Qué es lo que tiene que hacer un estudiante? Estudiar, estudiar y estudiar!". A mi abuelo, a quien no pude ver por la distancia que nos separaba, le mando un abrazo.
# preface: |
#   Definir prólogo
# Specify the location of the bibliography below
bibliography: [bib/thesis.bib, bib/TesisBeveridgeCurve.bib]
# Download your specific csl file and refer to it in the line below.
csl: csl/apa.csl
lot: true
lof: true
link-citations: yes
# biblio-style: "apalike"
# If you prefer blank lines between paragraphs, un-silence lines  40-41 (this requires package tikz)
header-includes:
#- \usepackage{tikz}
# - \usepackage[spanish]{babel}
- \usepackage{footmisc}
- \usepackage{amsmath}
- \usepackage{subfig}
# - \usepackage{fnpct}
# - \usepackage{natbib}
# - \usepackage{wrapfig}
# - \usepackage{fullpage}
- \usepackage{pdflscape}
- \usepackage[labelfont=bf]{caption}
# \usepackage[spanish,es-tabla]{babel} 
# - \usepackage{hyperref}
# - \hypersetup{colorlinks=yes,linkcolor=blue, urlcolor = blue, linktocpage}
# - \hypersetup{colorlinks = true, linkcolor = blue, urlcolor = blue}
# - \hypersetup{
#      colorlinks = yes,
#      linkcolor = blue,
#      anchorcolor = blue,
#      citecolor = blue,
#      filecolor = blue,
#      urlcolor = blue,
#      linktocpage}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete the section entirely, or silence (add #) them. 

If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.

If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(thesisdown))
  devtools::install_github("ismayc/thesisdown")

# Cargar paquetes
paquetes <- c("thesisdown", "ggplot2", "plotly", "magrittr", "data.table", "bvarsv", "kableExtra", "grid","ggthemes")
sapply(paquetes, require, character.only = TRUE)

# Definir opciones generales del documento
knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE, 
                      warning=FALSE,
                      cache = TRUE, 
                      fig.align ="center")

plot_regular <- knit_hooks$get("plot")

knit_hooks$set(plot = function(x, options, .notas = notas, .fuentes = fuentes, .addFuentes = FALSE) {
  if(.addFuentes) {
  paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\textsc{Fuentes} -- ", .fuentes,
        "\n\\end{figure}\n",
        sep = '')
  } else {
      paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\end{figure}\n",
        sep = '')
  }
})
plot_notes <- knit_hooks$get("plot")
knit_hooks$set(plot = function(x, options, .notas = notas, .fuentes = fuentes, .addFuentes = TRUE) {
  if(.addFuentes) {
  paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\textsc{Fuentes} -- ", .fuentes,
        "\n\\end{figure}\n",
        sep = '')
  } else {
      paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\end{figure}\n",
        sep = '')
  }
})
plot_notes_sources <- knit_hooks$get("plot")
```




```{r}
# Temas
theme_Publication <- function(base_size=12, base_family="Helvetica", position_legend = "bottom", angulo_x = 0, angulo_y = 0) {
      library(grid)
      library(ggthemes)
      (ggthemes::theme_foundation(base_size=base_size, base_family=base_family)
       + theme(plot.title = element_text(size = rel(0.6), hjust = 0),
               text = element_text(),
               panel.background = element_rect(colour = NA),
               plot.background = element_rect(colour = NA),
               panel.border = element_rect(colour = NA),
               axis.title = element_text(size = rel(0.9)),
               axis.title.y = element_text(angle=90,vjust =2),
               axis.title.x = element_text(vjust = -0.2),
               # axis.text = element_text(),
               axis.text.x = element_text(angle = angulo_x),
               axis.text.y = element_text(angle = angulo_y),
               axis.line = element_line(colour="black"),
               axis.ticks = element_line(),
               panel.grid.major = element_line(colour="#f0f0f0"),
               panel.grid.minor = element_blank(),
               legend.key = element_rect(colour = NA),
               legend.position = position_legend,
               legend.direction = "horizontal",
               legend.key.size= unit(0.2, "cm"),
               legend.margin = unit(0, "cm"),
               # legend.spacing = unit(0, "cm"),
               legend.title = element_text(face="italic"),
               plot.margin=unit(c(10,5,5,5),"mm"),
               strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
               strip.text = element_text(face="bold")
          ))
}

scale_fill_Publication <- function(...){
      library(scales)
      discrete_scale("fill","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)

}

scale_colour_Publication <- function(...){
      library(scales)
      discrete_scale("colour","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)

}

font_size_table = 8
```

<!-- On ordering the chapter files:
There are two options:
1. Name your chapter files in the order in which you want them to appear (e.g., 01-Inro, 02-Data, 03-Conclusions). 
2. Otherwise, you can specify the order in which they appear in the _bookdown.yml (for PDF only).

Do not include 00(two-hyphens)prelim.Rmd and 00-abstract.Rmd in the YAML file--they are handled in the YAML above differently for the PDF version.
-->

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers
on chapters.
-->

<!-- # Introduction {.unnumbered} -->

<!-- ```{r} -->
<!-- options(bookdown.post.latex = function(x) { -->

<!--   # x is the content of the LaTeX output file -->
<!--   # str_replace can take a named vector of patterns and replacements -->
<!--   replace_patterns <- c( -->
<!--     "^\\\\bibliography\\{bib/thesis.bib\\}" = "",         # Remove the default bibliography -->
<!--     "^BIBLIOGRAPHY" = "\\\\bibliography{bib/thesis.bib}"  # Add it in a new spot -->
<!--   ) -->

<!--   stringr::str_replace_all(x, replace_patterns) -->
<!-- }) -->
<!-- ``` -->



<!--chapter:end:index.Rmd-->

# Introducción {#intro}

<!-- Presentación de la CB, componentes y su utilización -->
El análisis teórico de la dinámica agregada del mercado laboral ha estado organizado en base a dos relaciones, la curva de Phillips y la curva de Beverige [@Blanchard1989]. La curva de Beveridge (en adelante CB) analizada por primera vez por @Beveridge y @Dicks-Mireaux1958 plantea una relación negativa entre la tasa de vacantes laborales y la tasa de desempleo y, resume información esencial sobre el funcionamiento del mercado de trabajo y shocks que afecten al mismo [@Blanchard1989]. Las vacantes se entienden como aquella posición dentro de la firma que el empleador busca cubrir activamente durante un periodo de referencia mientras los desocupados se definen como aquellas personas que durante un periodo de tiempo buscan trabajo remunerado activamente pero no logran obtenerlo.<!-- ^[Ambas variables suelen ser normalizadas por la población económicamente activa (PEA), personas que pertenecer a la población en edad de trabajar (PET) y buscar hacerlo activamente.] -->

El objetivo principal de esta investigación es profundizar en esta relación negativa y probar si existe estabilidad parámetrica a lo largo del tiempo, por lo tanto, la pregunta de investigación busca responder si la curva de Beveridge de Uruguay entre 1980 y 2019 presenta cambios de pendientes y/o traslados paralelos en cualquier momento del periodo considerado. La hipótesis sostenida es que la relación entre vacantes y desempleo no puede ser estable para el periodo de estudio.

La medición de la tasa de trabajadores desocupados se mide regularmente a lo largo del mundo con una metodología estandarizada y validada. Mientras las vacantes <!-- Las vacantes se utilizan como proxy de la demanda laboral y se cuantifican mediante los avisos laborales publicados en la prensa o portales de Internet.  --> son raramente calculadas bajo una metodología sistemática, no suelen ser recolectadas, ni existen encuestas al respecto a excepción de algunos países, especialmente de la OCDE [@Elsby2015].<!-- ^[Una explicación es que la recolección y medición de vacantes es una tarea compleja para las oficinas estadísticas encargadas de recabar dicha información, debido a la difícil definición de qué es una vacante y cuando una empresa identifica que existen vacantes y buscan llenarla [@Elsby2015].] --> En Uruguay no existe una serie de vacantes oficial que se publique de forma pública y sistemática. Por lo tanto, el primer objetivo específico de este trabajo es crear el primer indicador de vacantes laborales reproducible, extenso y sistemático. La base de datos que se construye abarca casi 40 años, incorporando para los últimos quince años tres fuentes de datos y el análisis de datos a traves de _text mining_ [@Jurasky2019].

<!-- Pregunta de investigación -->
Finalizado el indicador se obtiene el primer resultado empírico, el cual es una correlación positiva con el producto hasta el año 2013 y una relación negativa con la tasa de desempleo. 
<!-- El objetivo principal de esta investigación es profundizar en esta relación negativa y probar si existe estabilidad parámetrica a lo largo del tiempo, por lo tanto, la pregunta de investigación busca responder si la curva de Beveridge de Uruguay entre 1980 y 2019 presenta cambios de pendientes y/o traslados paralelos en cualquier momento del periodo considerado. -->

<!-- Fuentes de datos -->
Para responder la pregunta de investigación se utiliza la tasa de desempleo calculada por el Instituto Nacional de Estadística (INE), la Encuesta Continua de Hogares (ECH) sistematizada por parte del Instituto de Economía (IECON), el Producto Interno Bruto (PIB) calculado por el Banco Central del Uruguay (BCU) y el indicador de vacantes laborales construido a partir de avisos de el diario El País (sección _Gallito_), los portales laborales _Buscojobs_, _Computrabajo_, _Uruguay Concursa_, el Índice Ceres de Demanda Laboral (ICDL) y los datos de vacantes provenientes de @Rama1988 y @Urrestarazu1997.

<!-- Explicación de la hipótesis (fundamentos teóricos)-->
La hipótesis sostenida es que la relación entre vacantes y desempleo no puede ser estable para el periodo de estudio, deberíamos observar al menos uno de los siguientes efectos: cambios de pendiente, traslados de la CB y/o alteraciones en la varianza de los errores en un modelo lineal. <!-- Esta parte debería mejorar la redacción --> Para fundamentar la hipótesis y explicar los traslados de la CB se utiliza la teoría de búsqueda y emparejamiento o marco DMP [@Pissarides2000], basado fundamentalmente en los trabajos de @Pissarides1985, @Mortensen1994 y @Diamond1982.^[Adicionalmente están los los trabajos de @Hall1979, @Pissarides1979, @Diamond1982B]

Los cambios observados en la CB pueden deberse principalmente a factores institucionales [@Gujarati1972;@Blanchard2000;@Nickell2002; @Bouvet2012], factores tecnológicos [@Elsby2015;@Lubik2013] o a factores asociados al ciclo económico [@Abraham1987;@Blanchard1989;@Hobijn2013]. Según el marco DMP [@Pissarides2000] los movimientos sobre la curva se asocian a factores de ciclo económico mientras traslados paralelos (mayor o menor desempleo para una tasa de vacantes dada) se interpretan como cambios estructurales que afectan el _matching_ entre puestos y trabajadores desempleados.<!-- Falta hablar de los movimientos (ej, shock de demanda, de oferta, sectoriales) --> Los traslados pueden ser por aumentos en la tasa de destrucción de puestos laborales o modificaciones en la función de matching. Cambios en la función de matching pueden deberse a cambios en los costos de contratación, despido, seguro de desempleo y salario mínimo [@Bouvet2012]; existencia de derecho a huelga, ocupación y sindicalización [@Nickell2002]; facilidad de la mano de obra para moverse geográficamente [@Bewley1979; @Hobijn2013]; modificaciones en el comportamiento de las firmas [@Haltiwanger2012] y, alteraciones tecnológicas que puedan cambiar los costos de búsqueda y la probabilidad de encontrarse [@Barnichon2012;@Elsby2015].

<!-- Reescribir, primero factores institucionales, luego tecnológicos y finalmente cíclicos. -->
<!-- Según el marco DMP los movimientos sobre la curva se asocian a factores de ciclo económico mientras traslados paralelos (mayor o menor desempleo para una tasa de vacantes dada) se interpretan como cambios estructurales que afectan el _matching_ entre puestos y trabajadores desempleados [@Bouvet2012]. Los traslados pueden ser por aumentos en la tasa de destrucción de puestos laborales o modificaciones en la función de matching. Cambios en la función de matching pueden deberse a cambios en los costos de contratación y despido, salario mínimo, existencia de derecho a huelga y ocupación, sindicalización, facilidad de la mano de obra para moverse geográficamente (cambios en la PEA), modificaciones en las habilidades de los trabajadores y alteraciones tecnológicas que puedan cambiar los costos de búsqueda y la probabilidad de encontrarse, como la creación de portales laborales en Internet. <!-- Falta una explicación de los cambios de ciclo económico -->

<!-- Explicación de la hipótesis fundamentos empíricos: estructurales y ciclo -->
Muchas de estas causas estructurales se observan en Uruguay. Por ejemplo, en la década de 1980 retorna la democracia (1985), con ella aumenta la sindicalización y se reinstauran los consejos de salarios los cuales funcionaron junto a una elevada negociación por rama de actividad del orden del 94% [@Filgueira2003].^[Entre 1985 y 1999 la negociación por rama era de 94 %, entre 1990-1994 de 76.4% y en 1995-1999 de 35%. La negociación por empresa entre 1985-1989 era de 6%, entre 1990-1994 de 21.2% y entre 1995-1999 de 64.4% [@Filgueira2003]. El trabajo de @Quinones2001 muestra la caída de la cantidad de afiliados a sindicatos privados entre 1987 y 2000. No hay datos para periodos previos, sin embargo, el aumento planteado es relativo al periodo previo de dictadura en el cual la central CNT fue declarada ilegal, luego el decreto 622/73 permitió la creación de sindicatos pero mediante elecciones secretas.] En los años 90, el efecto es inverso, disminuye la sindicalización prácticamente a la mitad [@Quinones2001], cae la negociación por rama de actividad a un 35% [@Filgueira2003] y se dejan de convocar los consejos de salarios a partir de 1992 [@Antia2001]. A partir de 2005, se promulgan un conjunto de reformas estructurales que producen un cambio sustancial en las reglas de juego de la economía [@Bergara2017]. Los gobiernos vuelven a convocar los consejos de salarios, fomentan la negociación por rama de actividad, establecen la protección de los trabajadores en procesos de terciarización, aumentan el salario mínimo, limitan la jornada laboral de trabajadores rurales, regulan el trabajo del servicio doméstico y aprueban la ley de responsabilidad penal empresarial [@Bergara2017].^[Esto pudo aumentar el poder de negociación de los trabajadores por el aumento de la densidad sindical, ya que, la central obrera PIT-CNT cuadruplicó sus afiliados entre 2005 y 2020.] 

Además hay cambios tecnológicos relevantes como la penetración de Internet que fomentan el ingreso de nuevos actores en el sector de prensa laboral apareciendo los portales _Buscojobs_, _Computrabajo_, <!-- ^[Dichos portales comienzan a funcionar formalmente en Uruguay a partir de 2003 en el caso de _Computrabajo_ y 2007 _Buscojobs_, aunque en torno a 2009-2010 comienzan a tener un nivel de actividad relevante]  --> el funcionamiento de _Gallito_ a través de Internet y la creación del portal _Uruguay Concursa_ donde se centralizan las solicitudes de empleos públicos. Por último, la crisis 2002 genera una enorme emigración de población y desalienta a los trabajadores, lo cual repercute en una caída importante de la población económicamente activa (PEA) como se observa en la Figura \@ref(fig:PeaMontevideo).

<!-- Causas cíclicas luego --> Las dos crisis económicas previamente mencionadas, sumado a una breve recesión en 1995 pueden dar explicaciones asociadas al ciclo económico. Entre 1982-1984 la crisis de la _tablita_ genera una caída de casi 16% del producto, el salario real medio cae un 28% y el desempleo en Montevideo llega al 14% [@Antia2001]. Entre 1985-2000 el índice de horas trabajadas cae a la mitad, los índices de productividad se duplican [@Quinones2001] y la economía tiene años (1996-1998) de elevado crecimiento de la actividad por sobre el 5% [@Antia2001]. Posteriormente, entre 1999 y 2003 comienza una recesión (caída de 3.8% y 7.7% en la tasa de crecimiento) que culmina en una grave crisis económica y social. Desde 2003 en adelante la economía crece de forma sistemática (entre 2004 y 2011 la tasa de crecimiento oscila entre 4.1% y 7.8%), hay un shock externo de commodities y un flujo importante de inversión extranjera, con lo cual hasta 2011 la tasa promedio anual de variación del IVF del PIB es 5.9% y el desempleo se ubica en sus menores valores (5-6%) en cuarenta años.

Dada esta multiplicidad de transformaciones, se buscan estrategías empíricas lo suficientemente flexibles para captar posibles cambios parámetricos continuos o discretos. <!-- Los test de fluctuación generalizada [@Kuan1995;@Ploberger1989;@Brown1975;@Society1988] y los test basados en el estadístico F [@Andrews1993;@Andrews1994;@Hansen1992] --> A modo de exploración se utilizan los test de fluctuación generalizada [@Kuan1995;@Ploberger1989;@Brown1975;@Society1988] con los cuales se encuentra evidencia de posibles quiebres estructurales tanto en media como en varianza. Se profundiza con test de quiebres estructurales de tipo F generalizados que no requieren definir el momento del cambio estructural para poner a prueba la hipótesis nula de no existencia de un cambio estructural en la relación de vacantes y desempleo [@Andrews1991;@Hansen1992;@Andrews1993;@Andrews1994], de esta forma se encuentra una modificación en la media del proceso en 1990, resultado obtenido previamente por @Urrestarazu1997. Posteriormente, se utilizan test generalizados los cuales permiten obtener múltiples cambios estructurales sin imponer los momentos de dichos quiebres y, se agrega la posibilidad de un quiebre en la varianza de la relación [@BaiPerron1998;@BaiPerron2003;@Zeileis2010]. De esta forma se obtienen cuatro CB, todas con relación negativa entre vacantes y desempleo y, con alteraciones tanto en la pendiente como traslados paralelos. Las causas podrían estar asociadas a factores cíclicos, institucionales o tecnológicos.

A continuación se pleantean vectores autorregresivos con parámetros variables y volatilidad estocástica, TVP-VAR o TVP-VAR-SV [@Nakajima2011;@Benati2013;@Primiceri2005;@Lubik2016b], identificado bajo un modelo básico de búsqueda y emparejamiento, con restricciones de identificación de Cholesky y estimado de forma bayesiana mediante el muestreo de Gibbs. Esta estrategia se elige en la medida que si bien es lineal condicional en los parámetros, el modelo completo es altamente no lineal y extremadamente flexible al permitir que los coeficientes del modelo (interceptos, rezagos y covarianzas de errores) evolucionen como un paseo aleatorio [@Lubik2016b]. De su estimación surge que tanto interceptos como rezagos no cambian a lo largo del tiempo, aunque si lo hace levemente la varianza de los errores, indicando que alteraciones en la CB podrían ser causadas por shocks de distinta indole, por ejemplo, de productividad. Esto se fortalece con el análisis de las funciones de impulso respuesta (FIR) que muestran como un shock de producto a vacantes genera un efecto positivo y duradero, mientras en el desempleo el efecto es negativo.

Ambas estrategias concuerdan en que las varianzas de los errores se modifican a lo largo del tiempo, sin embargo llegan a conclusiones diferentes en cuanto a los interceptos y rezagos. Es un resultado común en la literatura de TVP-VAR la posibilidad de no encontrar variabilidad en interceptos y rezagos pese a que si debería existir y, asociarlo a la matriz de varianzas y covarianzas [@Lubik2016b]. Los resultados indican distintas fases de la CB que pueden deberse a cambios por el ciclo económico, dadas las crisis y periodos de alto y bajo crecimiento, alteraciones de la PEA o shocks de productividad. A mejoras tecnológicas que faciliten el _match_ como la creación de portales laborales web. O la causa puede ser de origen institucional debido a la elevada cantidad de reformas estructurales que tuvo la economía, en especial de la década de 2000 en adelante. Por último, la enorme disminución observada en las vacantes laborales y el leve aumento del desempleo (especialmente en la comparación histórica) indican que el mercado laboral uruguayo debería haberse vuelto notoriamente más eficiente en su proceso de matching pese a reformas institucionales que podrían ir sentido opuesto [@Nickell2002;@Bouvet2012]. Sin embargo, para corroborar esta afirmación es necesario agregar un análisis de flujos laborales, identificar shocks de productividad y aislar el efecto de otras variables relevantes que puedan tener influencia. Es de interés que trabajos posteriores se enfoquen en identificar las causas de los movimientos en la CB, lo cual es de suma importancia para la política económica.

<!-- Por ejemplo, en la década de los noventa Uruguay transitó una apertura de su cuenta corriente y de capitales lo cual genero un efecto relevante en el mercado laboral, ya que, las empresas locales no fueron capaces de competir con los productos importados, siendo un periodo de crecimiento económico y alto desempleo [@Antia2001]. El gobierno dejó de participar de los consejos de salarios a partir de 1992 dando libertad de acción a la negociación entre empresas y trabajadores [@Antia2001]. Fue una clara promoción a la negociación por empresa en contraposición a la negociación por rama de actividad, mientras la primera aumento de forma considerable la segunda cayó abruptamente [@Filgueira2003].  -->

<!-- La hipótesis se fundamenta en las transformaciones mencionadas, ya que, según -->
<!-- el marco DMP shocks o cambios estructurales en la economía generan corrimientos -->
<!-- de la curva, debido a un aumento o disminución del mismatch. Al igual que políticas -->
<!-- públicas de carácter institucional que busquen aumentar la efectividad del match en -->
<!-- el mercado laboral volviéndolo más eficiente (Elsby et al., 2015; Rodenburg, 2007) o -->
<!-- modificaciones tecnológicas que puedan cambiar los costos de búsqueda tanto para -->
<!-- trabajadores como empresas así como la probabilidad de encontrarse -->

<!-- @Elsby2015 muestran que la CB se utiliza a nivel macroeconómico como un marco de análisis para el entendimiento de los mercados laborales tanto a nivel agregado como desagregado, para el análisis de la volatilidad y la naturaleza de los shocks que generan las fluctuaciones del mercado laboral, la coexistencia de desempleo y vacantes y como una aproximación al proceso de matching y eficiencia del mercado laboral^[Por si sola la CB solo puede ser una aproximación a la eficiencia del mercado laboral puesto que es necesario ampliar el análisis y tomar en cuenta los flujos de trabajadores, tanto la creación como la destrucción de puestos laborales]. -->


<!-- El matching se fundamenta en el marco DMP -->
<!-- El proceso de matching se relaciona con los diferentes componentes del desempleo, estructural y friccional\footnote{El desempleo cíclico o keyneasiano, se asocia al desempleo generado por el ciclo económico. En nuestro caso, no es necesario agregar esta categoría puesto que lo analizamos de la misma forma que el desempleo estructural, como movimientos sobre la CB}, los cuales puede ser analizados mediante la CB a través del marco DMP. -->

<!-- Análisis de los distintos tipos de desempleo, relacionar a las reformas uruguayas -->
<!-- El desempleo friccional refiere a personas que buscan trabajo o un cambio laboral\footnote{Esta clase de desempleo se dice que es intrínsecamente voluntario. Sin embargo, el motivo por el cual el sujeto busca o cambia de trabajo se asocia a situaciones involuntarias.}. Se ve afectado por modificaciones en la función de matching o aumentos en la tasa de destrucción de puestos laborales. Esto puede ser interpretado como alteraciones en la eficiencia laboral generando traslados paralelos de la CB, un corrimiento hacia el origen implicaría una mejora del match (un traslado hacia afuera un empeoramiento). Causas de cambios en la función de matching pueden ser los costos de contratación y despido, modificaciones sobre el salario mínimo, la existencia de derecho a huelga y ocupación, la sindicalización, la facilidad de la mano de obra para moverse geográficamente (cambios en la PEA), modificaciones en las habilidades de los trabajadores y creación de portales laborales en Internet que disminuyan el tiempo de búsqueda. -->

<!-- El desempleo estructural es la discordancia entre habilidades ofrecidas por trabajadores y demandadas por firmas. Se asocia a cambios tecnológicos (shocks de productividad) los cuales requieren menor nivel de empleo y una demanda laboral por habilidades que el trabajador no puede ofrecer generando un exceso de oferta laboral. Esto implica mayor tasa de desempleo y menor cantidad de vacantes laborales o sea movimientos sobre la CB. El lugar donde nos encontramos sobre la curva puede estar indicando en que parte del ciclo económico se sitúa la economía. Si la posición es un punto con altas vacantes y bajo desempleo sería una etapa expansiva, en caso contrario una fase recesiva\footnote{Lo cual explicaría el desempleo de cíclico}. -->

<!-- Por ejemplo, en la década de los noventa Uruguay transitó una apertura de su cuenta corriente y de capitales lo cual genero un efecto relevante en el mercado laboral, ya que, las empresas locales no fueron capaces de competir con los productos importados, siendo un periodo de crecimiento económico y alto desempleo [@Antia2001]. El gobierno dejó de participar de los consejos de salarios a partir de 1992 dando libertad de acción a la negociación entre empresas y trabajadores [@Antia2001]. Fue una clara promoción a la negociación por empresa en contraposición a la negociación por rama de actividad, mientras la primera aumento de forma considerable la segunda cayó abruptamente [@Filgueira2003].  -->

<!-- Construida la serie de vacantes, se sistematizan los datos de desempleo y se busca responder la pregunta y testear las hipótesis utilizando dos estrategias empíricas. Los test de quiebres estructurales [@Andrews1991;@Andrews1994;@BaiPerron1998;@BaiPerron2003;@Zeileis2010] con los cuales se pone a prueba la hipótesis nula de no existencia de un cambio estructural en la relación de vacantes y desempleo, estimando las CB para los periodos obtenidos. Por último, utilizando vectores autorregresivos con parámetros variables y volatilidad estocástica, TVP-VAR o TVP-VAR-SV [@Nakajima2011;@Benati2013;@Primiceri2005;@Lubik2016b], identificado bajo un modelo básico de búsqueda y emparejamiento, con restricciones de identificación de Cholesky y estimado de forma bayesiana mediante el muestreo de Gibbs. -->

La organización del documento es la siguiente:

* **[Capítulo 2](#cap:Fundamentos):** revisa la literatura de la curva de Beveridge en Uruguay, Estados Unidos y Europa. Expone la teoría de Búsqueda y Emparejamiento.

* **[Capítulo 3](#cap:Datos):** define conceptos y fuentes de datos.

* **[Capítulo 4](#cap:Metodología):** revisa la metodología de quiebres estructurales y TVP-VAR.

* **[Capítulo 5](#cap:Resultados):** presenta los resultados, creación del índice de vacantes, estimación de la CB, test de quiebres estructurales y la estimación de un TVP-VAR

* **[Capítulo 6](#cap:Discusion):** discute mejoras y posibles comentarios del indicador de vacantes y los resultados obtenidos de la estimación de la CB.

* **[Capítulo 7](#cap:Conclusiones):** exhibe las conclusiones de la investigación y posibles trabajos futuros producto de la tesis, en especial posibles lineas de investigación futuras.

<!--chapter:end:01-chap1.Rmd-->

# Curva de Beveridge {#cap:Fundamentos}

## Revisión de literatura

<!-- Revisión Literatura -->
<!-- @Elsby2015 muestran que la CB se utiliza a nivel macroeconómico como un marco de análisis para el entendimiento de los mercados laborales tanto a nivel agregado como desagregado, para el análisis de la volatilidad y la naturaleza de los shocks que generan las fluctuaciones del mercado laboral, la coexistencia de desempleo y vacantes y como una aproximación al proceso de matching y eficiencia del mercado laboral^[Por si sola la CB solo puede ser una aproximación a la eficiencia del mercado laboral puesto que es necesario ampliar el análisis y tomar en cuenta los flujos de trabajadores, tanto la creación como la destrucción de puestos laborales]. -->
<!-- La CB ha sido utilizada para descomponer el desempleo en diferentes tipos, clarificar el debate respecto al pleno empleo y como un punto de discordancia respecto al vaciamiento de los mercados competitivos.  -->
<!-- Mientras en los años sesenta y setenta fue ampliamente empleada en la década de los ochenta su uso cayó notablemente al punto de ser casi abandonada, entre otras causas, por su falta de microfundamentos [@Rodenburg2007]. Resurge en los noventa gracias al trabajo de @Blanchard1989 quienes remarcan el error que la CB haya ocupado un lugar secundario en la macroeconomía dado que contiene información esencial sobre el funcionamiento del mercado de trabajo y shocks que afecten al mismo. -->
<!-- Actualmente ha logrado volverse el marco de análisis central en la macroeconomía laboral [@Elsby2015] gracias a los trabajos de @Pissarides1985, @Mortensen1994 y @Diamond1982 quienes dieron microfundamentos a la relación entre vacantes y desempleo permitiendo derivar una forma analítica que surge de las decisiones de optimizaciones de los agentes del modelo. Hecho relevante al permitir que la CB sea utilizada para el análisis de la política económica al estar exenta de la crítica de Lucas^[La crítica de Lucas fue un claro ataque a los modelos de ecuaciones simultáneas y por sobre todo a la no inclusión de microfundamentos en el análisis macroeconómico lo cual invalidaba cualquier calculo sobre el efecto de una política económica, falla típica de los modelos keynesianos [@Lucas1976]. Una respuesta posterior surgió por parte de la econometría bajo el trabajo de @exogeneity1983 según el cual pese a que un modelo no tenga microfundamentos si cumple la superexogeneidad el mismo puede ser utilizado para simular política económicas.]. Permitiendo estudiar las reacciones de vacantes y desempleo ante shocks de productividad [@Benati2013], la estabilidad paramétrica en la relación [@Lubik2016] y las causas que generan traslados o movimientos sobre la CB desde una perspectiva dinámica. -->
<!-- Análisis de la OECD: Inglaterra y que otro? -->
@Beveridge es pionero en identificar las vacantes laborales como un determinante de la tasa de desempleo y, encontrar una relación negativa entre ambas variables. @Dicks-Mireaux1958 plantean por primera vez la CB gráficamente al examinar la confiabilidad del uso de vacantes y desempleo para ilustrar tendencias en la demanda de trabajo. Entienden y utilizan la CB para medir el exceso de demanda en el mercado de trabajo como un indicador del exceso de demanda en el mercado de bienes. 

A nivel europeo en los años sesenta y setenta se genera una gran cantidad de trabajos, principalmente en Inglaterra, los cuales buscaban explicar los traslados de la CB [@Rodenburg2007].^[Según @Rodenburg2007 en los años 1980 la CB tuvo un periodo de baja utilización debido en parte a la falta de microfundamentos de la CB, lo cual genera que cualquier análisis de política económica fuese plausible de la crítica de @Lucas1976. Con el establecimiento del marco DMP dicha crítica desaparece.] Por ejemplo, @Gujarati1972 considera que el traslado de la CB (1958-1972) en Inglaterra se debió a factores institucionales debido al Redundancy Payment Act. @Bewley1979 halla resultados similares y agrega que el movimiento de la CB se asoció a factores demográficos que disminuyeron la eficiencia de la búsqueda laboral en conjunto a variaciones en los flujos de destrucción de puestos laborales (renuncias y despidos). @Evans1977 lo asocia a la diferencia entre desempleo total y desempleo registrado, mostrando que las proporciones de personas registradas como desempleadas varían considerablemente entre regiones y en el tiempo.
<!-- ^[El estudio fue novedoso en la medida que utilizó la discriminación por sexo en las vacantes laborales. Lo cual fue prohibido a partir de 1976 en Inglaterra. @Chew1986 realizan un estudio similar entre 1965-1980 para Singapur]. -->

@Nickell2002 estudian los mercados laborales de países de la OCDE entre 1960 y 1990 hallando traslados de la CB para todos los países a excepción de Noruega y Suecia. El motivo principal son las modificaciones en las instituciones del mercado laboral, como la unión sindical y las protecciones en el empleo. @Bouvet2012 detecta resultados similares en Alemania, Bélgica, Holanda, España y Reino Unido entre 1975 y 2004 donde los traslados en la curva se deben a rigideces del mercado laboral, como aumentos de salarios mínimos y elevados seguros de desempleo, sumado al desempleo de largo plazo generando _histeresis_. @Hobijn2013 <!-- analizan países europeos --> <!-- y EEUU --> encuentra desplazamientos de la fuerza laboral en las recesiones que generan un cambio en la composición de vacantes disminuyendo la eficiencia del matching, lo cual se ve compensado por la disminución en la tasa de abandono (separaciones, sin incluir los despidos).  <!-- Para EEUU encuentran que la Gran Recesión generó traslados de la CB por un tiempo superior al que podría esperarse según @Mortensen1994.  -->  <!-- @Elsby2015 señala que entre 1970 y 1980 la CB de países europeos ha tenido traslados debido a cambios en la eficiencia de los procesos de contratación.  --> <!-- Dicho resultado se repite para EEUU a partir de la Gran Recesión. -->

<!-- EEUU -->
En los EEUU @Abraham1987 genera un índice ajustado del proxy de vacantes basado en el help-wanted-index que asocia los movimientos en la CB a cambios en la composición del empleo, en la dispersión de la demanda laboral entre sectores de actividad y alteraciones en los comportamientos de búsqueda. @Blanchard1989 utilizan datos de vacantes, desempleo y fuerza laboral para identificar variaciones en el desempleo y tasa de vacantes debido a shocks cíclicos (demanda agregada), sectoriales (realocación) y shocks de oferta. Determinan que en el largo plazo son los shocks sectoriales y una tendencia determinista los que generan los traslados de la CB, mientras en el corto plazo los shocks de oferta agregada generan un movimiento sobre la curva.<!-- como insumo desarrollan una función de matching Cobb-Douglas con retornos constantes para EEUU estimada mediante MCO. --> <!-- @Barnichon2010 construye un índice de vacantes compuesto por el Help-Wanted Index (HWI) en papel, que recolecta las búsquedas laborales en los 51 mayores periódicos y, el HWI en linea del Conference Board que recolecta los avisos publicados en portales laborales web desde 2005^[trabajos previos pueden encontrarse en @Barnichon2009.]. --> @Hobijn2013 consideran que la Gran Recesión generó traslados de la CB por un tiempo superior al que podría esperar según @Mortensen1994, además de dezplazamientos de la fuerza laboral que genero un cambio en la composión de vacantes disminuyendo la eficiencia del matching. 
<!-- @Elsby2015 encuentra que los traslados de la CB posterior a la gran recesión se deben a cambios en la eficiencia de los procesos de contratación. -->
@Benati2013 usando los datos generados por @Barnichon2010 y la metodología de @Stock1996 y @Stock1998 ponen a prueba la variabilidad paramétrica entre vacantes y desempleo con lo cual estiman un TVP-VAR donde la evolución de los traslados de la CB es similar a observada para el periodo recesivo de la era Volcker, identificando cambios similares desde 1960. @Lubik2013 siguiendo la misma metodología TVP-VAR afirma que el traslado de la CB luego de la Gran Recesión puede ser explicado por la interacción de una caída cíclica de la productividad y una disminución en la eficiencia del matching. @Lubik2016 plantean un modelo de Búsqueda y Emparejamiento con el cual encuentran un cambio estructural en la CB, sin embargo al plantear un TVP-VAR no observan transformaciones en los parámetros de las variables endógenas pero si modificaciones en la matriz de varianzas y covarianzas (VCV). Concluyendo que un TVP-VAR puede asociar cambios estructurales a variaciones temporales en la matriz de VCV de los shocks.

@Hobijn2013 encuentra tanto para EEUU como países europeos cambios en la composición de vacantes. @Elsby2015 resumen hallazos previos para Europa y EEUU, mencionando movimientos y traslados de la CB similares entre algunos países europeos como Holanda y los EEUU. Efecto guiado por los cambios en los procesos de contratación lo cual es un resultado interesante en la medida que las instituciones del mercado laboral difieren fuertemente al ser Europa un mercado laboral más regulado.
<!-- @Elsby2015 -->

 <!-- @Diamond2015,  -->
<!-- Resto del mundo -->

<!-- Análisis AL -->
A nivel sudamericano la cantidad de trabajos es escasa comparativamente debido a la ausencia de series de vacantes laborales, por eso, suelen compartir la necesidad de construir un indicador de vacantes. En Chile @BankChile2002 crean un índice de vacantes utilizando los puestos laborales solicitados en los avisos laborales de periódicos para las cinco mayores áreas urbanas desde 1986 hasta 2002.
En Argentina @ArgentinaBC2019 utilizan encuestas sobre posiciones abiertas recolectadas por el Ministerio de Trabajo y construyen un índice de vacantes siguiendo la metodología expuesta por @Barnichon2010.

<!-- Análisis de Uruguay -->
Los trabajos en Uruguay comienzan con @Rama1988 quien utiliza la CB con fines descriptivos para aproximarse a la descomposición de la desocupación en desempleo voluntario, de segmentación y desequilibrio. Es el primer trabajo en Uruguay en plantear gráficamente la CB (1978-1988) y construir un índice de vacantes. Su objetivo es cuantificar los componentes de la desocupación analizando la agregación de micro mercados de trabajo mediante un modelo de desequilibrio. Plantea que la CB puede utilizarse para cuantificar los componentes de segmentación y desequilibrio, no así el componente voluntario debido a las variaciones de la PEA en la década de 1980. <!-- Pero al dividirlo por la pea, eso no esta tomado en cuenta??? --> @DECON1993 estiman mediante una regresión lineal la CB entre 1980 y 1990 en la cual encuentran evidencia de una tendencia de la curva a desplazarse hacia afuera, indicando un mercado de trabajo con mayor rigidez, aunque se demarcan que la razón fundamental de ello sean salarios relativos inadecuados. @Urrestarazu1997 construye en base al trabajo y serie de @Rama1988 y extiende el periodo de análisis hasta el año 1995. Si bien su foco al igual que Rama es el estudio del desempleo de segmentación y los micro mercados mediante modelos de desequilibrio, estima por mínimos cuadrados ordinarios (MCO) una CB. Siguiendo a @Chow1960 mediante una estimación restringida y otra no restringida pone a prueba la hipótesis de que exista un cambio estructural en 1990. Sus resultados concluyen que se rechaza la hipótesis nula de no existencia de un cambio estructural entre 1980 y 1995. <!-- Sin embargo, advierte siguiendo a Rama que las conclusiones obtenidas solo son válidas en la medida que la evolución del desempleo voluntario sea estable. --> <!-- , característica común en países desarrollados, pero no así en Uruguay.  --> Finalmente, @Alma2011 obtienen datos de puestos laborales a partir de las publicaciones de prensa en las primeras dos semanas de los meses de marzo, mayo o junio y septiembre entre 2000 y 2009, con lo cual construyen una CB anual utilizada con fines ilustrativos para mostrar una relación inversa y negativa para el periodo 2000-2009. Los autores hacen referencia a un posible cambio de pendiente en la curva, sin embargo, dada la escasez de datos afirman que no es posible identificar alteraciones relevantes.

Este trabajo se siguen los últimos trabajos de @Benati2013, @Lubik2013 y @Lubik2016 al estimar un TVP-VAR para analizar la existencia de cambios en la CB que puedan estar asociados a modificaciones en la media o shocks idiosincraticos que alteren la volatilidad de la relación entre vacantes y desempleo. Adicionalmente, se utilizan test de quiebres estructurales como en @Urrestarazu1997, aunque los mismos son generalizados y no requieren especificar el punto de quiebre. Finalmente, se da un paso más al utilizar un test que permiten encontrar múltiples quiebres estructurales en la CB, tanto en media como en varianza.
<!-- Agregar trabajo del BID sobre vacantes -->
<!-- Agregar trabajos de Chile sobre vacantes, ver los citados en Alma Espino del iecon -->

## Teoría de Búsqueda y Emparejamiento

Es posible enmarcar la CB bajo la teoría de búsqueda y emparejamiento utilizando el modelo básico de @Pissarides2000 con el cual derivar una forma analítica de la curva suponiendo la existencia de una función de matching, un proceso estocástico de Poisson mediante el cual se llenan las vacantes y que en estado estacionario la tasa de variación de la tasa de desempleo debe ser nula. Conceptualmente modificaciones en la función de matching generan traslados de la CB. Así como alteraciones en la función de producción generan una economía con mayor o menor capacidad de producción, la función de matching refleja una economía donde el match entre trabajador y firma puede ser más rápido o lento, por lo tanto, el mercado laboral puede ser más o menos eficiente (deben considerarse los flujos). 

El _matching_ es un proceso estocástico sujeto a una tecnología o función de producción (denominada función de _matching_) donde interactúan firmas (demanda) y trabajadores (oferta) quienes guían sus decisiones optimizando sus funciones objetivos dada su información disponible (imperfecta) y restricciones. La contratación del trabajador (el match) resulta de una búsqueda aleatoria y negociación (a la nash y en general asimétrica) que satisface ciertas condiciones salariales negociadas por ambas partes.
En este proceso el trabajador incurre en costos de búsqueda y pierde beneficios potenciales en otras actividades, valora el ingreso salarial y condiciones laborales presentes (decisión estática) y el flujo de ingresos futuros y condiciones laborales futuras (factores monetarios y no monetarios que implican una decisión dinámica). La firma tiene costos de búsqueda los cuales internaliza en conjunto a los potenciales costos de capacitación y posible fin de la relación laboral futura, por lo cual calcula el valor presente del costo de la vacante y sus beneficios potenciales. Este proceso de decisión es inherentemente dinámico, descoordinado, con fricciones, riesgos y asimetrías de información que generan una diferencia sistemática entre demanda y oferta laboral.^[Estudios sobre el proceso de _matching_ puede verse en @Pissarides2000 y @Elsby2015 mientras inspecciones sobre la función de _matching_ en @Petrongolo2001.]

En la ecuación \eqref{eq7} ($u = \frac{\lambda}{\lambda+\theta q(\theta)}$) puede observarse como transformaciones de $\theta$, cociente entre vacantes y tasa de desempleo, genera movimientos sobre la curva asociados al ciclo económico. Modificaciones de q($\theta$), que reflejen cambios en la función de matching $m$ o alteraciones en $\lambda$, incertidumbre que sufre el trabajador de perder los beneficios de un puesto ocupado, van a generar corrimientos de la curva.

Cambios en la función de matching pueden asociarse a una diferencia entre las habilidades requeridas por las firmas y las ofrecidas por los trabajadores. Por ejemplo, si los cambios tecnológicos son sesgados hacia la utilización del capital, nuevas tecnologías y personas con alta capacitación. Si este fuese el caso, tanto las vacantes laborales como la tasa de desempleo pueden aumentar, o crecer solamente el desempleo para una misma tasa de vacantes. Los shocks sobre la función de matching pueden tener un carácter permanente o transitorio, por ejemplo, una reforma estructural, como las políticas sociales, en especial las nuevas relaciones laborales mencionadas en @Bergara2017 deberían tener un efecto permanente.

El modelo básico también permite ver como una economía en la cual los trabajadores enfrenten un riesgo mayor de perder los beneficios de un puesto ocupado, mayor $\lambda$, genera un mercado laboral menos eficiente. El caso extremo de $\lambda=0$, nos lleva a un punto de la curva que se situaría sobre el origen, un mercado sin desempleo ni vacantes. Si bien dicho caso es irrelevante en términos prácticos, muestra que sin la existencia del riesgo de perdida laboral y sin trabajadores que transiten del empleo al desempleo (ecuación \eqref{eq4} sería cero), estaríamos en una economía completamente eficiente. Es decir, el riesgo y los flujos laborales son relevantes para cuantificar la eficiencia de un mercado laboral.

<!-- Este párrafo me parece que esta mal, revisarlo y si no logro arreglarlo lo borro -->
Modificaciones en el riesgo pueden deberse a reformas que hayan cambiado las reglas en el ámbito laboral aumentando la protección de los trabajadores. Los flujos podrían verse alterados por innovaciones tecnológicas como portales laborales que faciliten el match entre trabajador y empresa. Si bien $\lambda$ es una variable exógena en el modelo, podemos pensar dichos cambios como variaciones en ella, siendo los mismos shocks transitorios o estructurales.

<!-- El _matching_ es un proceso estocástico sujeto a una tecnología o función de producción (denominada función de _matching_) donde interactúan firmas (demanda) y trabajadores (oferta) quienes guían sus decisiones optimizando sus funciones objetivos dada su información disponible (imperfecta) y restricciones. La contratación del trabajador (el match) resulta de una búsqueda aleatoria y negociación (a la nash y en general asimétrica) que satisface ciertas condiciones salariales negociadas por ambas partes. -->
<!-- En este proceso el trabajador incurre en costos de búsqueda y pierde beneficios potenciales en otras actividades, valora el ingreso salarial y condiciones laborales presentes (decisión estática) y el flujo de ingresos futuros y condiciones laborales futuras (factores monetarios y no monetarios que implican una decisión dinámica). La firma tiene costos de búsqueda los cuales internaliza en conjunto a los potenciales costos de capacitación y posible fin de la relación laboral futura, por lo cual calcula el valor presente del costo de la vacante y sus beneficios potenciales. Este proceso de decisión es inherentemente dinámico, descoordinado, con fricciones, riesgos y asimetrías de información que generan una diferencia sistemática entre demanda y oferta laboral <!-- más allá del desempleo voluntario. -->
<!-- Estudios sobre el proceso de _matching_ puede verse en @Pissarides2000 y @Elsby2015 mientras inspecciones sobre la función de _matching_ en @Petrongolo2001. -->


<!--chapter:end:02-chap2.Rmd-->

# Fuentes de datos y definiciones {#cap:Datos}

## Definiciones

<!-- % 2. Desempleo -->
En Uruguay el INE sigue las recomendaciones del Conferencia Internacional de Estadísticas del Trabajo (CIET), y define el desempleo como aquellas personas que buscan trabajo remunerado activamente pero no logran obtenerlo.[^ine] Si bien existe consenso por parte de países y autoridades en seguir las recomendaciones del CIET para su definición y medición, la misma deja abierto el período relevante a considerar para catalogar a una persona como ocupada o desocupada, generando una diferencia relevante en la intensidad de búsqueda [@Elsby2015] que puede generar subestimación en la duración del desempleo en hasta un 8% [@Poterba1986]. Asimismo la definición de PEA como personas que aportan su trabajo para producir bienes y servicios comprendidos dentro de la frontera de producción durante un periodo de referencia especificado, si bien compartida, presenta diferencias en los límites inferiores lo cual se traslada a la definición y medición de la tasa de desempleo.\footnote{El CIET sigue las recomendaciones del SCN, el cual sigue el criterio de la frontera de posibilidades de producción. Según lo anterior las actividades que quedan por fuera son exclusivamente actividades de producción de servicios, los servicios excluidos son los producidos por los miembros del hogar para el consumo final propio del hogar, producidos por el trabajo voluntario desde los hogares con destino a otros hogares. Las actividades excluidas son limpieza y pequeñas reparaciones del hogar, cocinar para los miembros del hogar, tareas de cuidado y educación de los miembros del hogar, transporte de los miembros del hogar. La PEA se utiliza como un sinónimo de fuerza de trabajo} No obstante, la diferencia entre desempleados y personas fuera de la fuerza de trabajo son significativas, ya que, las primeras es más probable que transiten al empleo [@Flinn1982].

[^ine]: Según el @INE2019, "se considera como desempleado a toda persona que durante el período de referencia considerado (última semana) no está trabajando por no tener empleo, que lo busca activamente y está disponible para comenzar a trabajar ahora mismo. Por definición, también son desocupados aquellas personas que no están buscando trabajo debido a que aguardan resultados de gestiones ya emprendidas y aquellas que comienzan a trabajar en los próximos 30 días".

<!-- %\subsubsection*{Vacantes} -->
En el caso de las vacantes laborales no existe una definición compartida a nivel conceptual. Según @Abraham1983 una vacante debe verse como una demanda insatisfecha por parte de la empresa aunque para @Elsby2015 esto presenta tres problemas. El primero es que puede resultar difícil identificar el recurso ocioso en una firma. Segundo, la dificultad para medir la producción no llevada a cabo debido a la ausencia del puesto. Por último, las empresas pueden contratar anticipándose a una posible apertura de posición y la misma puede variar por sector de actividad, por ejemplo, @Myers1966 identificando algunos problemas conceptuales y de medición de vacantes encuentra que un 10\% de los avisos laborales se llenan antes de que el empleado actual deje la firma y, que las vacantes son más numerosas en las industrias manufactureras durables.

## Fuentes

En el Cuadro \@ref(tab:fuentes), se presentan todas las fuentes de datos utilizadas en el trabajo. La primer columna refiere al tipo de información pudiendo ser avisos laborales, PEA, Encuesta Continua de Hogares (ECH), tasa de desempleo o encuesta de uso de Internet. La columna _Fuente_ establece cual es la institución que genero dichos datos, sean portales laborales o el Instituto Nacional de Estadística (INE). _Extraída_ indica el lugar físico o virtual del cual fueron obtenidos los datos. Mientras _Periodo_ menciona los años contenidos en cada fuente de información.

```{r fuentes, results='asis', fig.cap="Fuentes de datos"}
tabla <- data.table(Nombre = c(rep("Avisos laborales", 11), "PEA", "ECH", "Tasa desempleo", "Encuesta Internet"),
                    Fuente = c(rep("Gallito", 6), "Buscojobs", "Computrabajo", "Gallito", "Buscojobs", "Computrabajo", "INE", "INE", "INE", "RADAR"),
                    Extraida = c("(ref:Urrestarazu1997)", "Biblioteca Nacional", "Biblioteca Nacional", "CERES", "(ref:Alma2011)", "Diario El País", "WaybackMachine", "WaybackMachine", "Portal Gallito", "Portal Buscojobs", "Portal Computrabajo", "(ref:Urrestarazu1997)", "IECON", "INE", "RADAR"),
                    Periodo = linebreak(c("1980-1995", "1995-1998", "1999, 2000, 2009, 2010,\n 2011, 2012, 2013, 2014", "1998-2014", "2000-2009", "2013-2018", "2007-2018", "2003-2018", "2018-2019", "2019", "2018-2019", "1980-1995", "1980-2018", "1981-2019", "2015-2016")))

tabla[2:3, Nombre := paste0(Nombre, footnote_marker_number(1, "latex"))]
tabla[7:11, Nombre := paste0(Nombre, footnote_marker_number(1, "latex"))]

texto = "\\\\footnotesize Fuentes de avisos laborales utilizadas en la construcción del índice de vacantes. En \\\\textit{Nombre} se define el tipo de información que contiene la fuente de datos, por ejemplo, avisos laborales de prensa o portales laborales, datos de población económicamente activa (PEA) o encuesta continua de hogares (ECH). En \\\\textit{Fuente} se define la fuente de datos utilizada, es decir, donde se generó la información. Pudiendo ser portales laborales o prensa (Gallito, Buscojobs Computrabajo), el Instituto Nacional de Estadística (INE) o la empresa consultora RADAR. En \\\\textit{Extraída} se menciona donde fueron recabados los datos física o virtualmente, por ejemplo, el Centro de Estudios de la Realidad Económica y Social (CERES) facilito los datos del Índice Ceres de Demanda Laboral (ICDL) el cual tenia como fuente los avisos de Gallito. Por último \\\\textit{Periodo} refiere a los años que cubre cada fuente de datos."

kableExtra::kable(tabla #%>% dplyr::mutate_all(linebreak)
                  , format = "latex", align = "c",
                  caption = "Fuentes de datos",
                  booktabs = TRUE,
                  escape = FALSE,
                  row.names = FALSE,
                  col.names = c("Nombre", "Fuente", "Extraída", "Periodo")) %>%
  kableExtra::kable_styling(font_size = 12, 
                            # latex_options = "scale_down", 
                            latex_options = "hold_position"
                            # position = "center"
                            ) %>%
  # kableExtra::collapse_rows(valign = "top") %>%
  # kableExtra::column_spec(column = 4, width = "3cm") %>%
  kableExtra::footnote(general = texto,
                       number = "Datos de recolección propia. En todos los casos fue necesario imputar valores faltantes, lo cual se realizó mediante el paquete imputeTS, ver detalles en (ref:Moritz2017).",
                       general_title = "Notas:",
                       threeparttable = TRUE,
                       escape = FALSE
                       )
```

(ref:Urrestarazu1997) @Urrestarazu1997
(ref:Alma2011) @Alma2011
(ref:Moritz2017) @Moritz2017

De el INE se obtiene la tasa de desempleo para el departamento de Montevideo en dos subperiodos con los cuales se construye una serie desde 1981 hasta 2019. La ECH se utiliza desde 1980 hasta 2018, usándose una versión compatibilizada por parte del Instituto de Economía (IECON). Mientras las proyecciones poblacionales de Montevideo utilizadas son del periodo 1995-2025.^[Dichas estimaciones vienen agrupadas por tramo etario en intervalos de cinco años, por lo tanto, no es posible obtener los mayores de 14 años, se debe trabajar con mayores de 15 años. Si bien esto podría introducir un leve sesgo, vale destacar que @Urrestarazu1997 para calcular la PEA omite mencionar que utiliza las proyecciones de población de @Celade1990, las cuales están agrupadas de la misma forma.]

La serie de @Urrestarazu1997 extiende el periodo del índice de @Rama1988 quien construye una serie trimestral entre 1978 y 1987 en base a las publicaciones semanales de _Gallito_, Ministerio de Trabajo y Seguridad Social (MTSS) y BCU.^[La serie construida por @Rama1988 une tres fuentes que miden distintas poblaciones, las primeras dos son de carácter nacional mientras la tercera es departamental para Montevideo, el autor supone que no genera sesgos relevantes.] @Urrestarazu1997 obtiene por parte de El País las publicaciones semanales entre 1989 y 1995, estima la cantidad de avisos entre 1988 y 1989 y calcula un índice de vacantes laborales de frecuencia trimestral desde 1978 hasta 1995.\footnote{Para obtener la cantidad de avisos en 1987-1988 realiza un supuesto de aviso promedio. Estudian las publicaciones del primer trimestre de 1987 obtiene el tamaño del aviso promedio y supone que el mismo no varia para el total del periodo analizado. Posteriormente observando el espacio disponible con que contaba la sección de demanda de trabajo en cada edición estima la cantidad de avisos promedio que podían caber}

Los datos de CERES corresponden al ICDL desde abril de 1998 hasta julio de 2014 con frecuencia mensual, fueron facilitado por CERES. La serie esta corregida por factores estacionales, pero no se detalla el filtro aplicado ni es especificada la fuente de datos. La información metodológica se puede ver en @Ceres2012. @Alma2011 construyen una base de datos de vacantes laborales a partir de las publicaciones de prensa en las primeras dos semanas de los meses de marzo, mayo o junio y septiembre entre 2000 y 2009. La base de datos es compartida por los autores.

El diario el País entrega una base de datos confidencial con todas las publicaciones laborales contenidas en el portal _gallito_ entre 2013 y 2018. Adicionalmente se construye el año 2019 por medio de scraping web. Todos los datos de 2013-2019 referidos al gallito son publicaciones donde se han limpiado publicaciones repetidas por link.\footnote{Esta limpieza refiere a publicaciones con link idéntico, lo cual es un identificador utilizado en la base de datos. Posteriormente se realiza una limpieza adicional utilizando la similaridad del texto de los avisos.} Como se observar en la Figura \@ref(fig:ga13-18-comparacion) el efecto es un cambio de nivel que mantiene la misma forma de la serie de avisos sin filtrar.

En *Buscojobs* se recaba la cantidad de publicaciones mensuales publicadas por el portal _Buscojobs_ entre 2007 y 2019. Para el periodo 2007 a 2018 se obtienen muestras por mes y en cada mes, las cuales son promediadas mensualmente obteniendo la cantidad de avisos mensuales promedio, dichos datos son obtenidos a través de la página web _Waybackmachine_. Para el año 2019 se realiza scraping del portal _Buscojobs_. En *Computrabajo* se extraen todas las publicaciones mensuales disponibles publicadas en el portal laboral _Computrabajo_ entre 2003 y 2018 a partir de _Waybackmachine_ y avisos laborales entre 2018 y 2019 mediante scraping del portal _Computrabajo_. La particularidad de este portal es que la cantidad de avisos publicados que se observa no se corresponde con la cantidad de avisos publicados en los últimos treinta días debido a que dicho portal mantiene avisos por más de sesenta días.

<!--chapter:end:04-chap4.Rmd-->

# Estrategias Empíricas {#cap:Metodologia}

Se utilizan dos estrategias empíricas, primero se trabaja con test de quiebre estructural siguiendo a @Andrews1993, @Andrews1994, @BaiPerron2003, @BaiPerron1998, @Zeileis2002, @Zeileis2005, @Zeileis2010, ya que, que son capaces de captar quiebres discretos en los parámetros de un modelo lineal. Posteriormente se utilizan vectores autoregresivos estructurales con parámetros variables y volatilidad estocástica (TVP-VAR) siguiendo a @Primiceri2005, @Nakajima2011, @Lubik2016b y se utiliza el algoritmo corregido por @DelNegro2015.

Los TVP-VAR permiten relajar el supuesto de una relación invariante en los parámetros del modelo, mediante la modelización de parámetros que siguen un proceso autoregresivo.^[Se suele asumir que el orden del proceso es uno, por lo cual se modelan como paseos aleatorios.] El primer motivo para la utilización de los TVP-VAR con volatilidad estocástica se da porque gran parte de las series macroeconómicas muestran cierta no linealidad con comportamientos diferentes tanto en la persistencia como en la volatilidad.^[Por ejemplo, el desempleo tiende a aumentar más rápidamente al comienzo de una recesión que su caída ante una recuperación de la economía [@Lubik2016b]] Segundo, si solo se permite volatilidad en rezagos e interceptos, es posible obtener una gran variabilidad en los parámetros pese a que el verdadero PGD tenga únicamente volatilidad estocástica [@Sims2002]. Consecuentemente es preferible modelar ambos de forma conjunta y que los datos definan la fuente más importante [@Lubik2016b]. Tercero, la utilización de un TVP-VAR debería incluir volatilidad estocástica para poder representar de forma subyacente un modelo dinámico estocástico de equilibrio general, DSGE [@Lubik2016b].

Los test de quiebres estructurales para modelos de regresión lineal se pueden dividir en dos clases. Los test de fluctuación generalizada [@Kuan1995;@Ploberger1989;@Brown1975;@Society1988] y los test basados en el estadístico F [@Andrews1993;@Andrews1994;@Hansen1992]. Los primeros incluyen los test CUSUM, MOSUM y basados en estimadores. Mientras el test de Chow, test supF, aveF y expF corresponden al segundo tipo.^[@Zeileis2005 plantea un marco de referencia más general conocido como el marco de fluctuación M-generalizado que engloba a las dos clases anteriores y los scores por máxima verosimilitud (ML scores).]

Los test de tipo F permiten poner a prueba la hipótesis nula de parámetros invariables en el período, contra la hipótesis alternativa de cambio en los parámetros en un momento particular. Los mismos son generalizados (supF, aveF y expF) para poner a prueba Ho sin conocer el momento de quiebre estructural, sin embargo, se sigue planteando la existencia de dos particiones.^[La existencia de un quiebre estructural en los parámetros de la media condicional implica la existencia de dos periodos. En general si existen n quiebres se tienen n+1 periodos.] @BaiPerron1998 y @BaiPerron2003 extienden los test de tipo F para encontrar múltiples quiebres estructurales, al minimizar una función objetivo y aplicar el algoritmo de programación dinámica para encontrar un mínimo global sobre todas las posibles particiones. @Zeileis2010 lo adapta para modelos estimados por máxima verosimilitud y la inclusión de la varianza como un regresor (los casos anteriores lo tratan como un parámetro molesto).

Los test de fluctuación generalizada incluyen los test CUSUM, que contienen la suma acumulada de residuos estandarizados, mientras MOSUM refiere a suma de residuos móviles. Adicionalmente, estos procesos se repiten pero en vez de utilizar residuos, se utilizan las estimaciones de los parámetros, siendo procesos basados en los estimadores. La idea de los test de fluctuación generalizada es ajustar un modelo a los datos y derivar un proceso empírico que capture las fluctuaciones en los residuos o en las estimaciones de los coeficientes. Para esto se calculan límites del proceso (fronteras), lo que implica que el proceso límite es conocido bajo la hipótesis nula. Si el proceso  de fluctuación empírico (efp) cruza dichos límites en algún momento, la fluctuación del mismo es improbablemente elevado lo que lleva a rechazar la hipótesis nula al nivel de significación $\alpha$ [@Zeileis2002].

Como remarca @Benati2013, bien podría utilizarse únicamente test de quiebre estructural. Sin embargo, @Benati2007 muestra que los test de quiebres estructurales de @BaiPerron1998, @BaiPerron2003, @Bai1997 ofrecen poca evidencia de quiebres cuando el proceso generador de datos (PGD) evoluciona como un paseo aleatorio, en contraposición a una metodología más flexible como @Stock1996, @Stock1998 que logra captar dicha evolución, @Cogley2005 encuentran resultados similares. @Benati2013 remarcan que la utilización de TVP-VAR es robusta frente a la especificación de la variación temporal en los datos, mientras que los test de quiebres estructurales lo son solamente si el PGD tiene quiebres discretos. Sin embargo, @Lubik2016 obtiene que el TVP-VAR puede llevar a conclusiones erróneas al asociar corrimientos paralelos de la curva con shocks en la matriz de varianzas y covarianzas y no en los coeficientes rezagados de las variables endógenas.

Otra posible elección es la desarrollada por @Barnichon2012, @Hobijn2013. Como sugieren @Hobijn2013 el análisis no lineal es el método empírico más común en el análisis de la curva de Beveridge, sin embargo, no es el único, ellos utilizan una nueva forma basados en @Barnichon2012 en el cual estiman el logaritmo del ratio de contrataciones sobre el stock de vacantes usando como regresores las contrataciones, separaciones, número de desempleados, empleados y el stock de vacantes. Desafortunadamente, no todas esas variables están disponibles en Uruguay para el periodo considerado. Descartadas esta metodología, se sigue adelante con TVP-VAR y quiebres estructurales.

Para poder estimar un VAR estructural es necesario imponer restricciones de identificación sobre la matriz de varianzas y covarianzas para pasar de la forma reducida a la forma estructural. De esta forma, es posible descomponer el efecto de cada shock individual sobre las restantes variables endógenas del sistema y calcular las funciones de impulso respuesta [@Hamilton1994]. Las parametrizaciones que se deseen imponer sobre la matriz de varianzas y covarianzas puede provenir o no de la teoría económica. En el primer caso suele suceder cuando una variable es publicada con rezago respecto de otra, o bien responden de forma diferente, por ejemplo una variable financiera y otra relacionada a bienes y servicios. En cualquier caso, las restricciones pueden ser de corto plazo, de largo plazo o de signo y los shocks pueden ser tanto permanentes como transitorios. En el caso de un TVP-VAR el calculo es diferente y se obtiene una FIR para cada momento del tiempo.

La estimación de un TVP-VAR con volatilidad estocástica tiene como desafío principal como realizar la inferencia. Este trabajo utiliza un enfoque bayesiano en el cual se utiliza el muestreo de Gibbs en el cual el problema original de la estimación del vector de parámetros es intratable.^[@Primiceri2005 remarca cuatro puntos que hacen preferible la estimación con un enfoque bayesiano. El primero es que los parámetros de interés son componentes inobservables. Segundo, si la varianza de los parámetros variables es pequeña, la estimación por máxima verosimilitud (MV) de la varianza tiene un punto de masa en cero. Tercero, la estimación por MV en un modelo de alta dimensión y no lineal puede generar múltiples máximos locales implausibles o sin interés alguno. Por último, puede ser difícil maximizar la verosimilitud en un problema de alta dimensión, mientras los métodos bayesianos manejan esta problemática de forma natural, como el muestreo de Gibbs. @Lubik2016b remarca este último punto al mencionar que el enfoque bayesiano tiene desarrollados poderosos algoritmos computacionales que se adaptan particularmente bien al tratamiento de la variación temporal en los parámetros.] Por lo cual, se divide en pequeños bloques que pueden ser evaluados de forma secuencial e independiente.^[Para un detalle de la metodología ver @Nakajima2011, @Lubik2016b. Para ver esquemas de bloques desarrollados para los TVP-VAR ver @Primiceri2005, @DelNegro2015]

## TVP-VAR
@Primiceri2005 propone el siguiente modelo para un vector n-dimensional $y_t$:

\begin{equation}
y_t = c_t + B_{1,t}y_{t-1} + ...+ B_{k,t}y_{t-k} + u_t \ \ \ \ t = 1,....T.
(\#eq:var-reducido)
\end{equation}

Donde $y_t$ es un vector de variables endógenas n x 1; $c_t$ es un vector de parámetros variables n x 1 que multiplica términos constantes; $B_{i,t}$, $i = 1,....k$, son matrices n x n de coeficientes variables; $u_t$ son shocks inobservables, heterocedásticos con matriz de varianzas y covarianzas $\Omega_t$. 

Consideremos (sin perder generalidad) una reducción triangular de $\Omega_t$ definida por:

\begin{equation}
A_t\Omega_tA_t' = \Sigma_t\Sigma_t'
\end{equation}

Con $A_t$ una matriz triangular inferior con elementos ($\alpha_{ij,t}$) y unos en su diagonal. Y $\Sigma_t$ una matriz diagonal de elementos $\sigma_{i, t}$. El modelo \@ref(eq:var-reducido) pasa a ser:

\begin{align}
y_t &= c_t + B_{1,t}y_{t-1} + B_{2,t}y_{t-2} + ... + B_{p,t}y_{t-p} + A_t^{-1}\Sigma_t\epsilon_t (\#eq:svar-sv) \\
V(\epsilon_t) &= \mathbb{I}_t \notag
\end{align}

Apilando mediante el operador $vec$ todos los coeficientes del lado derecho de la ecuación \@ref(eq:svar-sv) en un vector $B_t$, se puede reescribir el modelo.

\begin{align}
y_t  &= X_t'B_t + A_t^{-1}\Sigma \epsilon_t (\#eq:svar-sv-final)
\end{align}

Donde $X_t' = I_n \otimes [1, y_{t-1}, ..., y_{t-p}]$ con $\otimes$ denotando el producto de Kronecker. Se mantiene que $y_t$ es un vector columna n-dimensional y  $B_t$ contiene los parámetros $\{B_{j,t}\}_{j=1}^p$ y $c_t$ de la ecuación \@ref(eq:svar-sv).
La estrategia de identificación consiste en modelar los coeficientes de la ecuación \@ref(eq:svar-sv-final) en lugar de \@ref(eq:var-reducido). El modelo VAR completo es:

\begin{align}
y_t  &= X_t'B_t + A_t^{-1}\Sigma\epsilon_t (\#eq:yt)\\
B_t &= B_{t-1} + \nu_t (\#eq:Bt)\\
\alpha_t &= \alpha_{t-1} + \zeta_t (\#eq:alphat)\\
log \ \sigma_t &= log \ \sigma_{t-1} + \eta_t (\#eq:sigmat)
\end{align}

La dinámica del modelo se resume en las ecuaciones \@ref(eq:Bt) a \@ref(eq:sigmat), donde $\alpha_t$ es el vector de elementos no negativos y no unos de la matriz $A_t$ los cuales están apilados por filas mediante el operador $vec$ mientras $\sigma_t = diag(\Sigma)$. Los elementos de $B_t$ se modelan como paseos aleatorios, supuestos que puede ser relajado, al igual que los elementos de la matriz $A_t$. Se supone que los desvíos estándar $\sigma_t$ evolucionan como un paseo aleatorio geométrico, lo cual lo hace pertenecer a la clase de modelos con volatilidad estocástica.^[La diferencia más importante respecto a los modelos ARCH es que las varianzas son componentes inobservables.]

La matriz de varianzas y covarianzas (VCV) de los residuos varía en el tiempo debido al término de error compuesto $A_t^{-1}\Sigma_t\epsilon_t$. Con $\epsilon_t$ siguiendo una distribución normal n-dimensional y $\{\nu, \zeta_t, \eta_t\}$ vectores normales homocedásticos, de media cero y mutuamente independientes.

Las innovaciones en el modelo se asume tienen una distribución normal conjunta con la siguiente matriz de varianzas y covarianzas:

\begin{equation}
V = Var 
\begin{pmatrix} 
\epsilon_t \\\nu_t \\ \zeta_t \\ \eta_t 
\end{pmatrix} =
\begin{pmatrix}
\mathbb{I}_n \ 0 \ 0 \ 0\\
0 \ Q \ 0 \ 0 \\
0 \ 0 \ S \ 0 \\
0\ 0\ 0\ W
\end{pmatrix}
\end{equation}

Donde $\mathbb{I}_n$ es una matriz identidad n-dimensional, Q, S, W son matrices semidefinidas positivas.^[@Primiceri2005 sustenta la elección de la la matriz V en que ya existe una gran cantidad de parámetros en el modelo y que permitir una estructura completa de autocorrelación entre las diferentes fuentes de incertidumbre inhibe cualquier interpretación estructural de los shocks. Adicionalmente se supone que S es diagonal por bloques, donde cada bloque corresponde a cada ecuación por lo cual los coeficientes de las relaciones contemporáneas se asumen evolucionan de forma independiente, esto se realiza para aumentar la eficiencia del algoritmo]

Para poder estudiar los efectos de un shock de productividad desde el producto al desempleo y vacantes, se computan las funciones de impulso respuesta (FIR). En el caso de un VAR de parámetros fijos, el computo de las FIR son independientes al proceso de estimación del modelo, mientras en un TVP-VAR ambas etapas son dependientes y se obtiene una FIR para momento del tiempo.
<!-- Especificación del test de Stock y Watson si soy capaz de armarlo -->


<!-- ESPECIFICACIÓN DE LOS MÉTODOS DE IMPUTACIÓN -->
<!-- Agrego la metodología de quiebres estructurales siguiendo a Zeilis. -->
## Quiebres estructurales

El modelo básico de regresión lineal es:

\begin{equation}
y_i = x_i^T\beta_i + u_i\space\space\space\space (i= 1...n)
\end{equation}

Para cada momento i, $y_i$ es la variable dependiente, $x_i = (1, x_{i2}, ..., x_{ik}^T)$ es un vector $k \times 1$ con $k-1$ observaciones de regresores o variables independientes. Los $u_i$ son $iid(o, \sigma^2)$ y $\beta_i$ es un vector k variado de parámetros. Los test de cambio estructural plantean la hipótesis nula de que los $\beta_i$ son invariantes en el tiempo, versus la hipótesis alternativa de que existe variabilidad en algún parámetro:

\begin{align}
H_0:\space\space \beta_i &= \beta_0 \space\space\space\space (i= 1...n) \\
H_1:\space\space \beta_i &\not= \beta_0 \space\space\space\space (i= 1...n)
\end{align}

<!-- Aca fue que corte y pase toda la parte explicativa de MOSUM y CUSUM al anexo -->
Se trabaja con dos estadísticos para poner a prueba la hipótesis nula. El estadístico $S_r$ se utilizada para los procesos basados en residuos, mientras el estadístico $S_e$ se utiliza para los procesos basados en estimaciones:

\begin{align}
S_r &= \max_t\frac{efp(t)}{f(t)}, \\
S_e &= \max \Vert efp(t) \Vert
\end{align}

Con f(t) dependiendo de la forma del límite, $b(t) = \lambda f(t)$. ^[Para los distintos cálculos de los p-valores para cada test consultar sección A en @Zeileis2002.]

Los estadísticos F difieren de los test anteriores en que se especifica la hipótesis nula a contrastar, se define una hipótesis alternativa de un quiebre en un momento particular.

\begin{equation}
\beta_i = \begin{cases} 
\beta_A &(1 \leq i \leq i_0) \\
\beta_B &(i_0 < i \leq n)
\end{cases}
\end{equation}

Con $i_0$ es algún punto en el intervalo $(k, n-k)$. Posteriormente son extendidos para no especificar el momento de quiebre, sino un intervalo de tiempo. Se utiliza una secuencia de estadísticos F para un quiebre en un momento i, se calculan los residuos de ese segmento $\hat{u}_i$ (una regresión para cada submuestra) y se comparan con los residuos de un modelo sin quiebres.

\begin{equation}
F_{i} = \frac{\hat{u}^T\hat{u}-\hat{e}(i)^T\hat{e}(i)} {\hat{e}(i)^T\hat{e}(i)/(n-2k)}
\end{equation}

<!-- El test original de Chow @Chow1960 necesita que se especifique el momento del quiebre en particular en la hipótesis alternativa. Su planteamiento es realizar dos regresiones, una restringida y otra sin restringir.^[Para testar la igualdad entre los conjuntos de coeficientes en las dos regresiones, se obtienen la suma cuadrado de los residuos (RSS) asumiendo igualdad (bajo $H_0$) y RSS sin asumir igualdad. El ratio de la diferencia entre las dos sumas y la última suma, ajustado por los correspondientes grados de libertad se distribuye como el estadístico F bajo $H_0$. Los resultados de @Chow1960 se pueden resumir en sus ecuaciones 50 y 51 y, son generalizables al caso de más de dos regresiones. Específicamente, $F_{i_0} \sim \chi^2_k$ y $F_{i_0}/k \sim \chi^2_{k, n-2k}$.] -->

<!-- La desventaja del test de Chow, es que el punto de quiebre debe ser conocido. Sin embargo, es posible plantear un test F generalizado para todos los potenciales puntos de quiebre en un intervalo de la muestra, cumpliendo que $k < \underline i \leq \overline i \leq n-k$^[@Andrews1993 recomienda 15-85% de la muestra al realizar los test de inestabilidad paramétrica cuando el punto de quiebre no es conocido.]. Rechazando $H_0$ si cualquier estadístico, $F_i$ sobrepasa los valores de tabla.  -->

Al igual que los efp, es posible plantear límites para el estadístico F. Asumiendo $H_0$, los límites se pueden calcular de tal forma que la probabilidad asintótica de alguna forma de agregación de los distintos estadísticos F calculados sobre los intervalos considerados $\underline i \leq i \leq \bar{i}$, superen dicho umbral con una significación $\alpha$. @Andrews1993, @Andrews1994 plantean utilizar los funcionales: supremo, media o exponencial, con lo cual plantean tres estadísticos para poner a prueba la hipótesis nula:

\begin{align}
supF &= \sup_{\underline i \leq i \leq \bar{i}} F_i \\
aveF &= \frac{1}{\bar{i} - \underline i + 1}\sum_{i = \underline i}^{\bar{i}}F_i \\
expF &= log \left( \frac{1}{\bar{i}-\underline i + 1}\sum_{i = \underline i}^{\bar{i}}exp(0.5\times F_i) \right)
\end{align}

Si sucede que el funcional supera dicho umbral, existe evidencia de un cambio estructural con una significación $\alpha$. 

<!-- @Liu1997 y @BaiPerron1998 plantean paralelamente este problema, su solución teórica y propiedades.  -->
@BaiPerron1998 plantean un marco general que engloba a un modelo estructural completo y parcial y @BaiPerron2003 implementan dicha solución utilizando un algoritmo de programación dinámica (ecuación de Bellman) y la función objetivo de MCO.\footnote{La diferencia entre un modelo estructural completo es que todos los parámetros pueden tener quiebres en la muestra, mientras un modelo estructural parcial permite a un subconjunto de los parámetros tener quiebres, mientras el resto son estimados con la muestra completa asumiéndolos invariables}

Sea $RSS(\{T_{r,n}\})$ la suma de cuadrados de residuos asociada con la partición optima conteniendo $r$ quiebres usando las primeras n observaciones. La partición óptima resuelve el siguiente problema recursivo[^estimation]:

$$
RSS(\{T_{m,T}\}) = \min_{mh\leq j\leq T-h}[RSS(\{T_{m-1,j}\}) + RSS(\{j+1, T\})]
$$

[^estimation]: Se evalúa primero, el primer quiebre óptimo para todas las submuestras desde $h$ hasta $T-mh$. Se guardan un conjunto de $T-(m+1)h+1$ particiones óptimas y sus RSS, donde cada partición corresponde a una submuestra terminando en $2h$ hasta $T-(m-1)h$.
Segundo, se buscan las particiones óptimas con dos quiebres, las cuales terminan en el periodo $3h$ hasta $T-(m-2)h$. Para cada uno de estas posibles fechas de término, se busca en que partición de un quiebre guardada previamente puede ser insertada para obtener un RSS mínimo. Se devuelve un conjunto de $T-(m+1)h+1$ con dos quiebres óptimos. El algoritmo continua de forma secuencial hasta que el conjunto $T-(m+1)h+1$ de $(m-1)$ particiones óptimas se obtiene con finalización desde $(m-1)h$ hasta $T-2h$. Se busca cual de esas $(m-1)$ particiones óptimas genera un mínimo global en RSS cuando se combina con un segmento adicional.

@Zeileis2010 extiende el trabajo de @BaiPerron2003, para trabajar con modelos de tipo de cambio, como @Frankel1994 en los cuales la varianza del error $\sigma^2$ es de crucial interés. Esto lleva a la inclusión del error de la varianza como un regresor adicional en vez de un parámetro molesto y, la estimación del modelo por máxima verosimilitud o cuasi-máxima verosimilitud. La inclusión de $\sigma^2$, como nota @BaiPerron2003, puede mejorar la estimación de los quiebres estructurales.

El modelo planteado es cuasi-normal y tiene densidad:
$$
f(y|x,\beta, \sigma^2) = \phi((y-x^T\beta)/\sigma)/\sigma
$$
Donde $\phi(.)$ es la función de densidad de una normal estándar. Con $\theta = (\beta^T, \sigma^2)^T$ de largo $k = c +2$, siendo c la cantidad de regresores, más intercepto y varianza.

El algoritmo para encontrar quiebres es exactamente el mismo que @BaiPerron2003, con la diferencia que en vez de usar estimaciones MCO se usan estimaciones QML y la función objetivo $RSS$ se cambia por la log-verosimilitud negativa $-logf(y_i|x_i, \theta)$.

<!--chapter:end:03-chap3.Rmd-->

# Resultados {#cap:Resultados}

```{r FuncionesTVPVAR}
impulse_response <- function (fit, impulse.variable = 1, response.variable = 2, 
    t = NULL, nhor = 20, scenario = 2, draw.plot = TRUE, ..main = .main) 
{
    Beta.draws <- fit$Beta.draws
    nd <- dim(Beta.draws)[3]
    if (is.null(t)) 
        t <- dim(Beta.draws)[2]
    out <- matrix(0, nd, nhor + 1)
    M <- fit$M
    p <- fit$p
    H.sel <- fit$H.draws[, ((t - 1) * M + 1):(t * M), ]
    A.sel <- fit$A.draws[, ((t - 1) * M + 1):(t * M), ]
    if (scenario == 3) {
        sig <- apply(exp(0.5 * fit$logs2.draws), 1, mean)
        sig <- diag(sig)
    }
    else {
        sig <- NULL
    }
    for (j in 1:nd) {
        if (scenario == 1) {
            H.chol <- NULL
        }
        else if (scenario == 2) {
            H.chol <- t(chol(H.sel[, , j]))
        }
        else if (scenario == 3) {
            H.chol <- t(solve(A.sel[, , j])) %*% sig
        }
        aux <- bvarsv:::IRFmats(A = bvarsv:::beta.reshape(Beta.draws[, t, j], 
            M, p)[, -1], H.chol = H.chol, nhor = nhor)
        aux <- aux[response.variable, seq(from = impulse.variable, 
            by = M, length = nhor + 1)]
        out[j, ] <- aux
    }
    if (draw.plot) {
        pdat <- t(apply(out[, -1], 2, function(z) quantile(z, 
            c(0.05, 0.25, 0.5, 0.75, 0.95))))
        xax <- 1:nhor
        matplot(x = xax, y = pdat, type = "n", ylab = "", xlab = "Horizonte", bty = "n", xlim = c(1, nhor))
        polygon(c(xax, rev(xax)), c(pdat[, 5], rev(pdat[, 4])), 
            col = "grey60", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 4], rev(pdat[, 3])), 
            col = "grey30", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 3], rev(pdat[, 2])), 
            col = "grey30", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 2], rev(pdat[, 1])), 
            col = "grey60", border = NA)
        lines(x = xax, y = pdat[, 3], type = "l", col = 1, lwd = 2.5)
        abline(h = 0, lty = 2)
        title(main = ..main, cex.main = 0.6, adj = 0, line = 0)
    }
    list(contemporaneous = out[, 1], irf = out[, -1])
}
# Carga del modelo svar-sv
fit <- readRDS(here::here("Datos", "Finales", "modelo.rds"))

# Función para generar los gráficos de parámetros y varianzas
matplot2 <- function(...) matplot(..., type = "l", lty = 1, lwd = 2, bty = "n", ylab = "")
stat.helper <- function(z) c(mean(z), quantile(z, c(0.16, 0.84)))[c(2, 1, 3)]
gp <- seq(1985, 2020, 5) # marks for vertical lines
# colors, taken from http://www.cookbook-r.com
cols <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
cols1 <- cols[c(2, 4, 2)]
make_plot <- function(.fit = fit, .type = "vcv", .var = 1, .title = "") {
    gp <- seq(1990, 2020, 5) # marks for vertical lines
    if(.type == "vcv") {
        # SD of unemployment residual
        # Get posterior draws
        sd_inf <- parameter.draws(.fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(sqrt(sd_inf), 2, stat.helper))    
    } else if (grepl(x = .type, pattern = "lag")) {
        beta <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1   <- t(apply(beta, 2, stat.helper))    
    } else {
        beta_0 <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(beta_0, 2, stat.helper))       
    }
    xax <- seq(1990, 2018, length.out = NROW(x1)) # x axis
    # Plot
    if(.type == "vcv") {
        var <- sd.residuals.ols[.var]
    } else {
        var <- NULL
    }
    matplot2(x = xax, y = x1, ylim = c(min(x1), max(x1)), col = cols1, main = .title , xlab = "")
    # abline(h = seq(min(x1), max(x1), length.out = 10), v = gp, lty = 4, lwd = 0.3)
    if(.type == "vcv") {
        abline(h = var, col = cols[1], lwd = 1.4, lty = 5)
    }
}

# Función para generar los IRF
plot_irf <- function(.fit = fit, impulse, response, scenario = 2, .main = "") {
    ira <- impulse_response(fit, impulse.variable = impulse, response.variable = response, scenario = scenario, ..main = .main)
    # OLS impulse responses for comparison
    ira.ols <- irf(fit.ols, n.ahead = 20)[[impulse]][[response]][-1, 1]
    # Add to plot
    lines(x = 1:20, y = ira.ols, lwd = 1, lty = 5, col = "red")
}

# carga del modelo var
library(vars)
fit.ols <- readRDS(here::here("Datos", "Finales", "modelo-var-comun.rds"))
sd.residuals.ols <- apply(residuals(fit.ols), 2, sd)
```
<!-- Este capítulo presenta los resultados principales del trabajo. Primero se grafica la curva de Beveridge entre 1981 y 2018, se hace una partición por décadas para visualizar potenciales etapas. Realizamos una exploración de la relación entre vacantes laborales y el IVF del PIB y tasa de desempleo con producto. -->
<!-- Posteriormente analizamos las series utilizadas y sus propiedades estadísticas, tales como raíces unitarias, raíces estacionales y cointegración utilizando distintos test estadísticos, resultados que pueden consultarse en el anexo. -->
<!-- A continuación planteamos una búsqueda de quiebres estructurales mediante distintos tipos de test estadísticos, como procesos de fluctuación, test F y dating. Con los test de tipo dating logramos encontrar la cantidad de quiebes estructurales óptimos dada una función objetivo con lo cual obtenemos diferentes periodos de análisis. -->
<!-- Posteriormente estimamos un TVP-VAR con volatilidad estocástica, donde analizamos la evolución de los rezagos de las variables endógenas y los desvíos estándar de la matriz de varianzas y covarianzas. De esta forma analizamos si el PGD contiene modificaciones (suaves) en los parámetros bajo un modelo multivariado.  -->
<!-- Finalmente computamos las FIR para analizar el efecto que tienen shocks estructurales desde el producto hacia la tasa de desempleo y el índice de vacantes laborales. -->
<!-- Todo el proceso de quiebres estructurales y estimación de TVP-VAR ha sido realizado en el lenguaje _R_ utilizando los paquetes strucchange [@Zeileis2002], fxregime [@Zeileis2010] y bvarsv [@Kruger2015]. -->
<!-- \newpage -->
## Índice de vacantes

```{r include_data, include = FALSE}
# Base de datos final
dt <- readRDS(here::here("Datos", "Finales", "series_version_final.rds"))

# Serie iecon. Se usa en FUENTES y CRÍTICAS
iecon <- haven::read_dta(here::here("Datos", "Originales", "Gallito-2000-2009.dta"))
iecon$fecha <- as.Date(paste(iecon$aniog,iecon$mesg, iecon$diag, sep = "-"))
iecon_ts <- iecon %>% 
    dplyr::group_by(aniog, mesg) %>% 
    dplyr::summarise(puestos = sum(puestos, na.rm = TRUE),
                            avisos = dplyr::n())
iecon_ts$fecha <- as.Date(paste(iecon_ts$aniog, iecon_ts$mesg,"1", sep = "-"))
iecon_ano <- iecon %>% 
                dplyr::group_by(fecha = lubridate::make_date(aniog)) %>%                          dplyr::summarise(puestos = sum(puestos, na.rm = TRUE),
                                 avisos = n())

# Serie de CERES. Utilizada en FUENTES:Ceres
ceres <- readxl::read_xls(here::here("Datos", "Originales", "ICDL-1998-2014.xls"),
                          col_names = TRUE, sheet = "serie", 
                          col_types =c("date","numeric"),
                          readxl::cell_cols("A:B"))
ceres_ts <- ts(data = ceres[,2], start = c(1998,3), frequency = 12)
ceres_ano <- 
    ceres %>% 
        dplyr::group_by(fecha = lubridate::make_date(lubridate::year(fecha))) %>%
        dplyr::summarise(ind_vacantes = mean(vacantes, na.rm = TRUE))

```
<!-- ## Construcción indicador de vacantes  -->
Hasta los años 2000 la única fuente relevante de avisos laborales en papel fue _Gallito_, por lo cual basta con recabar dicha información para tener una muestra representativa de los avisos laborales.^[En Uruguay es posible visitar la Biblioteca Nacional donde por ley deben guardarse copias de todas las publicaciones de prensa. Al consultar a los encargados no se encuentran otras fuentes de publicaciones laborales para el departamento de Montevideo. Esta preponderancia de _Gallito_ sumado a la ausencia de encuestas es una de las causas de no seguir la metodología utilizada en @Barnichon2010] Sin embargo, con la revolución de Internet y los portales web comienza a perder representatividad. A partir de allí el problema se divide en tres: 1) Cuánta representatividad pierde 2) A partir de año comienza a perderla 3) Para responder 1 y 2, es necesario definir que otras fuente relevantes aparecen y la población de avisos a utilizar, la cual debe ser representativa de todos los portales laborales.

Las nuevas fuentes laborales consideradas en Uruguay son _Buscojobs_ portal laboral que nace a partir del año 2007 y rápidamente logra obtener un cuota relevante del mercado.^[Los datos de _Uruguay Concursa_ se obtienen indirectamente a partir de _Buscojobs_ debido a que dicha página los incluye entre sus publicaciones] _Computrabajo_ que opera en Uruguay a partir del año 2003 y, _Uruguay Concursa_ portal laboral que centraliza todos los llamados de empleos públicos.^[La página omitida más relevante es _LinkedIn_, sin embargo, su participación comienza en los últimos cuatro o cinco años y según la encuesta uso de Internet no más del 15\% de personas la consultan.] La preponderancia de _Gallito_ se mantiene hasta 2008-2009, luego su participación decrece sistemáticamente hasta estabilizarse en 40\% del total de publicaciones.
En la Figura \@ref(fig:series-conjuntas) se observa las series trimestrales con las que se construye el índice de vacantes para el periodo 1980-2019. El proceso se detalla en la sección siguiente.

```{r series-conjuntas, fig.cap = "Series trimestrales sin corregir"}
notas = "Series individuales de avisos laborales sin corregir utilizadas en la creación del índice de vacantes laborales. La serie \\textbf{um} corresponde a la serie combinada de (ref:Urrestarazu1997) de los años 1980-1995 y una serie de elaboración propia desde 1995 a 2001-Q1. La serie \\textbf{Ceres} son los avisos obtenidos a partir del Índice Ceres de Demanda Laboral (ICDL), facilitada por el Centro de Estudios de la Realidad Económica y Social (CERES). \\textbf{Gallito} refiere a los avisos de \\textit{Gallito} entre 2013 y 2018. \\textbf{Buscojobs} y \\textbf{Computrabajo} son los avisos obtenidos de los portales laborales \\textit{Buscojobs} y \\textit{Computrabajo} respectivamente. \\textbf{Gallito-BN} (linea punteada gris) corresponde a los avisos trimestrales de \\textit{Gallito} de recolección propia. Todas las series graficadas son de construcción propia. Se observa una caída desde 1980 hasta 1983 debido a la crisis de la \\textit{tablita} y luego un aumento permanente hasta los 2000 (con una caída importante 1995 por la \\textit{crisis del tequila}) de la cantidad de avisos laborales. Luego hay una disminución abrupta debido a la crisis de 2002, seguido de un aumento hasta 2012 y posteriormente una caída permanente hasta 2019, a excepción de las las series de Buscojobs y Computrabajo muestran un crecimiento casi constante. Las series \\textbf{um}, \\textbf{Computrabajo} y \\textbf{Buscojobs} tienen imputaciones de datos faltantes."
fuentes = "Se utilizan datos de (ref:Urrestarazu1997) obtenidos de \\textit{Gallito}, datos de elaboración propia (1995-1998, 1999Q1, 2000, 2001Q1, 2009, 2010, 2011, 2012, 2013, recolectados de \\textit{Gallito} en biblioteca nacional. Base de datos de \\textit{Gallito} (2013-2018) facilitada por diario El País y datos de scraping web de los portales laborales \\textit{Buscojobs}, \\textit{Computrabajo} y \\textit{Gallito}."

col_um           = "black"
col_ceres        = "darkorange"
col_ga           = "darkgreen"
col_ct           = "darkblue"
col_bj           = "red"
col_muestreos    = "gray"
colores <-  c(col_um, col_ceres, col_ga, col_ct, col_bj, col_muestreos)
long_dt <- melt.data.table(data = dt, id.vars = "fecha", 
                measure.vars = c("av_urr_mol", "av_ceres", "av_ga_s_dup", 
                                 "av_ct_s_dup", "av_bj_s_dup", "muestreos"),
                variable.name = "series", value.name = "avisos")
ggplot(long_dt, aes(x = fecha, y = avisos, color = series)) +
    geom_line(linetype = "dashed") +
    geom_point(data = long_dt[series == "muestreos",], aes(x = fecha, y = avisos)) +
    scale_color_manual(values = colores, 
                       labels = c("um", "Ceres", 
                                  "Gallito", 
                                  "Computrabajo", "Buscojobs", 
                                  "Gallito-BN"),
                       name = "") +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "5 year", date_labels = "%Y") +
    labs(x = "Fecha", y = "Avisos") +
    theme_Publication(position_legend = "bottom")
```

### Gallito

Para compatibilizar *Gallito* primero se obtienen los datos de @Urrestarazu1997. Segundo, se recaba de forma exhaustiva los avisos laborales existentes en las publicaciones semanales de _Gallito_ entre octubre de 1995 y agosto de 1998. Tercero, se utilizan los datos de @Ceres2012. Dada la falta de especificaciones, se contrasta con la información generada por @Alma2011 con la cual se construye un índice de frecuencia anual. La correlación lineal entre ambos índices es de 97\%, mientras sus tasas de crecimiento correlación lineal de 90\%. El resultado puede observarse en la Figura \@ref(fig:iecon-ceres). Cuarto, se recolectan muestras trimestrales de avisos entre 1999 y 2013 (denotados _gallito-BN_), de forma tal de corroborar la hipótesis que el ICDL debe estar generado a partir de publicaciones de _Gallito_.^[El término _gallito-BN_ refiere a recolecciones realizadas en la biblioteca nacional en los trimestres de 1999, 2000, 2009, 2010, 2011, 2012, 2013 y 2014 los cuales pueden verse en la Figura \@ref(fig:series-conjuntas) con color gris] En la Figura \@ref(fig:series-conjuntas) se ve como los _gallito-BN_ siguen el índice hasta el año 2012, a partir de allí se genera una diferencia en torno a los 1500-2000 avisos. Se asume que a partir de dicho año o bien cambio la metodología o se sumaron nuevas fuentes de datos. Por último se utiliza una base de datos confidencial entregada por el diario El País entre 2013 y 2018 y se realiza scraping de la página web _Gallito_ entre 2018 y 2019 de forma mensual. A continuación se analiza con detalle.

Para despejar los avisos totales entre 1980 y 1995 de la serie de @Urrestarazu1997 ($v_t=\frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}$) se debe utilizar la *PEA* calculada por @Urrestarazu1997, posteriormente se debe calcular dicha variable desde 1995 hasta 2019 y finalmente unir ambas estimaciones.

Se calcula la PET desde 1997 con frecuencia trimestral a partir de la ECH, se estiman los valores ausentes en 1996 y 1997 y se obtiene $pet_{96-18}$. La estimación es un promedio no ponderado de una interpolación mediante splines y un modelo básico estructural estimado mediante un filtro de Kalman suavizado [@Durbin2013;@Moritz2017].
Segundo, se trimestraliza la serie anual de PET en Montevideo, usando el método de denthon-chollete [@Sax2013], con promedios, usando como serie guía la PET trimestral en el paso previo. Tercero la PEA, se obtiene usando la TA publicada por el INE, por definición $TA = PEA/PET$.
Cuarto se repondera la PEA de 1981-1995 y se unen las series. Finalmente se estiman los valores de 1980-1981 mediante un promedio simple de una estimación mediante un modelo estructural básico estimado por filtro de Kalman suavizado y un modelo Arima en representación de espacio estado.

<!-- ### 1980 -->
El objetivo es obtener una aproximación de la cantidad de avisos totales, por ello primero es necesario obtener $a_t$ desde el índice de vacantes @Urrestarazu1997. Sin embargo, la base de dicho índice es 1980 y los datos disponibles en el trabajo son desde 1981 en adelante. El índice esta expresado en función de la PEA:
\begin{equation}
v_t = \frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}
\end{equation}

No se conoce $a_0$ ni $PEA_0$, ni se tienen datos de 1980. Por lo cual, se siguen los siguientes pasos:

1. Obtener $a_t$ en algún punto: $a_{1995_q4}$\footnote{La cantidad de avisos laborales del cuarto trimestre de 1995, fueron obtenidos a partir de las publicaciones del diario El País, sección Gallito}.
2. Imputar la PEA hacia atrás, obteniendo $pea_{t=0}$.
3. Despejar $a_{t=0}$ de 
$v_t$ = $(a_t/pea_t)/(a_{t=0}/pea_{t=0})100$.
4. Obtener la base $\alpha$ = $a_{t=0}$/$pea_{t=0}$.
5. Imputar las vacantes hacía atrás.
6. Calcular la serie de avisos

Se opta por realizar una predicción hacia atrás, planteando un modelo básico estructural mediante un filtro de Kalman suavizado [@Durbin2013].^[Se plantearon formulaciones alternativas, como imputación mediante filtro de Kalman sin suavizar, con 3 tipos de modelos estructurales: nivel-local, tendencia y básico estructural [@Harvey1989], además de modelos Arima.] A partir de ahora se trabaja con una serie trimestral de avisos.

<!-- ### 95-98 -->
A continuación se elige unir la serie de avisos $a_t^{80-95}$ con las publicaciones recolectados entre 1995 y 1998, dado que se utiliza $a_{1995}^{q4}$ para poder despejar la cantidad de avisos $a_t^{80-95}$ las series coinciden por construcción en el cuarto trimestre de 1995, por lo cual pueden ser unidas sin realizar ningún calculo adicional, la serie obtenida se denota $a_{um}^{80-98}$. Como se puede observar en la Figura \@ref(fig:series-conjuntas) las series combinadas mantienen el mismo nivel, aunque llama la atención la caída que se da entre 1995 y 1996. Sin embargo, dicha disminución no es causada por la combinación de las series, la misma comienza en el índice de vacantes de @Urrestarazu1997 donde los últimos cuatro trimestres tienen respectivamente una caída interanual de -17.3\%, -29.4\%, -25\% y -18.4\% debido a la crisis del tequila. A partir de 1996 se revierte iniciandose un período de crecimiento que se mantiene por 3 años.

<!-- ### 98-14 -->
Se utiliza el ICDL, $a_{ceres}^{98-14}$, de frecuencia mensual y sin factores estacionales. El índice tiene la forma $v_t = a_t/a_0$, el problema es que observamos $\tilde{v_t}$ debido a la desestacionalización y no conocemos $a_0$. Aún conociendo $a_0$, no es posible recobrar el verdadero $a_t$, pero es posible encontrar una aproximación que mantenga el mismo movimiento y tendencia.

La serie es trimestralizada y se analiza si es necesario realizar alguna corrección. Previamente valido la hipótesis que el ICDL corresponde, al menos hasta 2012, a publicaciones de _Gallito_. Suponemos que el proceso generador de datos es el mismo. La diferencia radica en que ha dicha serie le ha sido extraido (al menos) el componente estacional. 

Si suponemos que nuestro proceso generador de datos es:^[Basandonos en @Harvey1989]
\begin{equation}
Y_t = S_t + T_t + C_t + I_t
\end{equation}
Y observamos que $a_t^{ceres}$ es $T_t + C_t$ el hecho de que ajustemos el nivel de la serie mediante una corrección, por ejemplo, una reponderación, quiere decir que estaríamos reponderando la tendencia-ciclo de la serie. En la Figura \@ref(fig:series-conjuntas), tanto los _Gallito-BN_ como la serie $a_{um}^{80-98}$ acompañan el nivel de $a_t^{ceres}$, a tal punto que en algunos trimestres prácticamente coinciden. Suponemos que dichas diferencias se asocian al filtro aplicado, por lo cual se opta por unir las series a partir de 1998-III sin realizar ninguna corrección.^[Adicionalmente, se genera una serie de estacionalidad entre 1995 y 2018 utilizando los avisos recolectados entre 1995-1998 y los datos entregados por El País. Se imputa la estacionalidad para el periodo 1998-2013 y se agrega dicho componente a los avisos de _Ceres_, los resultados no varían.]
Se trimestraliza, se realiza la unión y se obtiene $a_{umc}^{80-14}$.

<!-- ### 13-19 Ga -->
El paso siguiente es la creación de una serie mensual de publicaciones sin avisos repetidos, utilizando los datos proporcionados por el diario El País, entre 2013 y 2018. Dicha serie se une con los avisos obtenidos para el año 2019 mediante el scraping web de la página web _Gallito_ y posteriormente se trimestralizan los datos..^[La información de los últimos tres meses de 2018 se obtuvo desde @MTSS2018]

```{r comparacion-gallito, fig.cap="Comparación avisos laborales"}
comparacion_gallito <- data.table(
    avisos_bd_s_dup = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), av_ga_s_dup],
    avisos_bd_c_dup = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), av_ga_c_dup],
    avisos_papel    = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), muestreos],
    fecha = seq.Date(as.Date("2013-10-01"), as.Date("2014-10-01"), "quarter")
)
comparacion_gallito[, `:=`(ratio_s_dup = avisos_papel/avisos_bd_s_dup,
                           ratio_c_dup = avisos_papel/avisos_bd_c_dup,
                           diferencia_s_dup = as.integer(avisos_papel - avisos_bd_s_dup)#,
                           # pct_s_dup = avisos_bd_s_dup/avisos_papel,
                           # pct_c_dup = avisos_bd_c_dup/avisos_papel
                           )]
data.table::setnames(comparacion_gallito, old = names(comparacion_gallito), new = c("Avisos sin\n duplicados", "Avisos con\n duplicados", "Avisos\n papel", "Fecha", "Ratio sin\n duplicados", "Ratio con\n duplicados", "Diferencia sin\n duplicados"))
setkey(comparacion_gallito, "Fecha")
notas <- "\\\\footnotesize Comparación de avisos laborales \\\\textbf{Gallito Base de datos} y \\\\textbf{Gallito papel}. La primera columna \\\\textbf{Avisos sin duplicados} son los avisos del portal \\\\textit{gallito} facilitados por el diario El País, filtrados de avisos duplicados. \\\\textbf{Avisos con duplicados} son los mismos datos pero sin filtrar los avisos duplicados. \\\\textbf{Avisos papel} refiere a los avisos recolectados manualmente de \\\\textit{gallito} en la Biblioteca Nacional. \\\\textbf{Fecha} hace referencia al periodo de tiempo correspondiente. Las columnas \\\\textbf{Ratio sin duplicados} y \\\\textbf{Ratio con duplicados} indican los ratios con duplicados y sin duplicados (cociente) y corresponden a la división de la tercera columna con la primera o segunda respectivamente. La última columna, refiere a la diferencia entre la primera y segunda columna con respecto a la tercera. No hay grandes diferencias en los ratios con o sin duplicados. El rango del ratio sin duplicados se mueve entre 1.26 y 1.51 indicando que los avisos en papel sobreestiman la cantidad de publicaciones reales en la base de datos de diario El País.

\\\\textit{Fuentes}: Avisos recolectados en Biblioteca Nacional de \\\\textit{gallito} y base de datos de publicaciones de \\\\textit{gallito} facilitada por el diario El País."

kableExtra::kable(comparacion_gallito, 
                  format = "latex", 
                  align = "c", 
                  booktabs = TRUE, 
                  digits = 2, 
                  caption = "Comparación avisos laborales",
                  col.names = linebreak(colnames(comparacion_gallito)),
                  escape = FALSE) %>%
kableExtra::kable_styling(latex_options = "hold_position",
                          # latex_options = "scale_down", 
                          position = "center",
                          font_size = 10,
  ) %>%
  kableExtra::footnote(general = notas, #number = notas,
                       # number_title = "Fuentes", 
                       general_title = "Notas:", 
                       # footnote_as_chunk = F, 
                       threeparttable = TRUE, 
                       escape = FALSE,
                       footnote_order = c("general"))
```

Se utilizan los avisos laborales facilitados por el diario El País para el período 2013-2018, con ellos se construye una serie trimestral de la cantidad de publicaciones.
Dichos datos es la variable no observable que deseamos cuantificar en los períodos previos pero que observamos con errores de medición, por ello lo denotamos $\tilde{a_t}$. Para obtener cuanto difieren los avisos en papel de los avisos de la base de datos, se recolectan los siguientes trimestres: $a_{q4}^{13}$, $a_{q1}^{14}$, $a_{q2}^{14}$, $a_{q3}^{14}$, $a_{q4}^{14}$.\footnote{En todos los casos existen semanas, donde la publicación semanal no estaba disponible. Por ello, se modelizaron serie de frecuencia semanal las cuales fueron imputadas mediante la modelización de un modelo básico estructural estimado por un filtro de Kalman suavizado} Como se puede observar en el Cuadro \@ref(tab:comparacion-gallito) la diferencia entre $\tilde{a_t}$ y ${a_t}$ es significativa.

Se elige corregir la serie $a_{umc}^{80-14}$ de forma que la serie quede expresada en los mismos términos que los avisos actuales, $\tilde{a_t}$, por lo cual, es posible seguir extendiendo el periodo de análisis simplemente agregando futuras observaciones obtenidas mediante scraping. Se repondera la serie $a_{umc}^{80-14}$ en base al promedio de los avisos $a_{q2}^{14}$, $a_{q3}^{14}$, $\tilde{a}_{q2}^{14}$, $\tilde{a}_{q3}^{14}$, obteniendo $\tilde{a}_{umc}^{80-14}$.

Se utilizan los _gallito-BN_ para corroborar que la fuente del ICDL sea _Gallito_.
Dada la diferencia que se observa entre los _gallito-BN_ y $a_{ceres}^{98-14}$ a partir del año 2012, se considera dicha serie hasta el primer trimestre de 2012. El fundamento es que el ICDL coincide con los _gallito-BN_ realizados cuya diferencia puede ser atribuida a la desestacionalización del mismo, sin embargo, en 2013 y 2014 las diferencias son de un orden de magnitud tal que dicha hipótesis no puede ser mantenida. 
Como aproximación en base a las series disponibles se genera una serie de estacionalidad imputada desde 1995 hasta 2018 de frecuencia mensual plateando los siguientes modelos:

$$
\begin{aligned}
Y_t &= T_t + C_t + S_t + I_t \\
Y_t &= T_t C_t S_t I_t
\end{aligned}
$$
Se recupera el componente $S_t$ y $S_t + I_t$, se generan dos series, las cuales son imputadas entre 2001 y 2012. En ningún caso los valores absolutos de la estacionalidad superan los 600 avisos, si eso se trimestraliza la diferencia máxima que se observa es inferior a 1000 avisos.

Finalmente, se unen las series imputando los valores trimestrales $a_{q2}^{12}$, $a_{q3}^{12}$, $a_{q4}^{12}$, $a_{q2}^{13}$. Se divide la serie en estaciones (trimestres) y se realizan imputaciones en cada serie por separado, utilizando el filtro de Kalman suavizado. De esta forma se obtiene la serie final correspondiente a _Gallito_ denotada como $a_{umcg}$, la misma se puede observar en la Figura \@ref(fig:serie-final-gallito) con color azul (umcg).

```{r serie-final-gallito, fig.cap = "Serie de avisos final"}
notas = "Serie trimestral de avisos laborales. La serie \\textbf{umcg} refiere a la serie final de gallito obtenida de la combinación de (ref:Urrestarazu1997), datos de recolección propia entre 1995-2001, avisos obtenidos del Índice Ceres de Demanda Laboral (ICDL) y avisos de \\textit{gallito}. \\textbf{Ceres} son los avisos obtenidos del ICDL. \\textbf{Gallito} es la serie generada a partir de \\textit{gallito} entre 2013 y 2018. Y \\textbf{um} es la serie combinada de Urrestarazu y de recolección propia. La diferencia de nivel se explica porque se reescalaron las cantidades de avisos desde 2013 hacia atrás para que correspondan a los avisos de la base de datos de el diario El País (\\textbf{Gallito})."
fuentes = "1980-1995 datos extraídos de (ref:Urrestarazu1997), entre 1995 y 2001 recolección propia, 1998-2012 se utilizan los datos del Centro de Estudios de la Realidad Económica y Social (CERES), entre 2013 y 2018 datos provistos por diario El País referidos a publicaciones de \\textit{gallito}."
# Generar otra serie urr_mol pero sin la predicción, agregarla acá.
long_dt <- melt.data.table(data = dt, id.vars = "fecha", 
                measure.vars = c("av_umcg", "av_ga_s_dup", 
                                 "av_urr_mol", "av_ceres"),
                variable.name = "series", value.name = "avisos")
ggplot(long_dt, aes(x = fecha, y = avisos, color = series)) +
    geom_line(linetype = "dashed") +
    scale_color_manual(values = c("darkblue", col_ga, col_um, col_ceres),
                       labels = c("umcg", "Gallito", "um", "Ceres"),
                       name = "") +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "5 year", date_labels = "%Y") +
    labs(y = "Avisos", x = "Fecha") +
    theme_Publication(position_legend = "bottom")
```

### Buscojobs

Los publicaciones laborales del portal *Buscojobs* se recaban desde 2007/06 hasta 2018/12 a través de la página web _Waybackmachine_. Se contabilizaron todas las publicaciones mensuales encontradas que hacen referencia al total de avisos publicados, los cuales son promediados mensualmente. Es decir, cada punto refiere a un año-mes y el total de avisos de publicados en dicho momento.

En el año 2019 se realiza scraping sobre el portal _Buscojobs_ extrayendo de forma mensual todos los avisos laborales disponibles y la información completa de cada aviso. La serie se modela con frecuencia mensual y los valores faltantes son imputados removiendo el componente estacional de la serie, imputando la serie desestacionalizada y luego agregando el componente estacional. Por último la serie es trimestralizada, la misma puede obvervarse en la Figura \@ref(fig:serie-buscojobs).

### Computrabajo

Las publicaciones laborales del portal *Computrabajo* referidas al total de avisos publicados, se promedian los meses en que existen múltiples observaciones y se obtiene una serie de frecuencia mensual. Se agregan los datos de scraping desde octubre de 2018 hasta diciembre de 2019. Dado que _Computrabajo_, muestra una cantidad de avisos totales diferente de la cantidad de avisos publicados en los últimos treinta días es necesario realizar una corrección, en caso contrario se estará sobrestimando los avisos publicados. De los datos obtenidos, se observa que los avisos publicados en los últimos treinta días representan un 40-43\% del número de avisos totales publicados que muestra el portal. A partir de julio de 2011, los avisos de _Computrabajo_ son reponderados por 0.42 y en las fechas previas por 0.63.^[Esta decisión ad-hoc se toma puesto que a partir de julio de 2011 la serie de _Computrabajo_ comienza a tener un crecimiento exponencial, además en las fechas previas las cantidades de avisos son bajas comparativamente, por lo cual es menos probable que difieran la cantidad de avisos mostrados de los efectivamente publicados en los últimos treinta días.] La serie final se observa en la Figura \@ref(fig:serie-computrabajo).

### Unión

Las tres páginas web de las cuales se extrajeron los datos difieren en la estructura con la cual presentan los avisos laborales. El mismo aviso se presenta de forma diferente y se pueden encontrar variaciones en cuanto a la información brindada. Por ejemplo, puede que el nombre del puesto y la empresa difieran levemente, que no se brinde información respecto a la empresa, o se haga referencia a "importante empresa".

Para poder identificar empresas compartidas por portales, primero se limpia el texto, se borran _stopwords_ [@Jurasky2019], se buscan patrones similares y finalmente se hace un filtro manual del cual se mapean 452 valores contenidos en una matriz de dimensión $\Re^{207*6}$ hacia un vector $x$ de dimensión $\Re^{207}$.

Finalmente, las tres series son combinadas realizando un análisis del texto de los avisos laborales de forma de obtener el porcentaje de avisos compartidos entre páginas. Siguiendo a @Jurasky2019 y utilizando el paquete text2vec [@text2vec] se construye un document-term matrix (DTM) con los avisos laborales, se vectoriza el texto mapeando palabras (1-gram) hacia un espacio vectorial, para esto es necesario crear un vocabulario común. Posteriormente, se calcula la similaridad de coseno:

$$
similaridad(doc1, doc2) = \cos(\theta) = \frac{doc1doc2}{|doc1||doc2|}
$$
entre los 3 portales web, obteniendo que los portales que más comparten avisos son _Computrabajo_ y _Buscojobs_, en torno a un 9\%, mientras con _Gallito_ es torno a 3\%. Se combinan primero los avisos entre _Computrabajo_ y _Buscojobs_ ajustados por el \% de avisos compartidos, luego se combina con _Gallito_ en base al \% compartido. El indicador de cantidad de avisos final y su estimación en tendencia-ciclo entre 1980 y 2019 se puede ver en la Figura \@ref(fig:IndiceAvisosTC). Por último se calcula el índice de vacantes entre 1980 y 2018 como $v_t=\frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}$ tomando como base el año 2010. El resultado se visualiza en la Figura \@ref(fig:IndiceVacantesAvisos).^[El índice de vacantes, contiene un año menos que el indicador de avisos en la medida que se utiliza la ECH compatibilizada por el IECON la cual finaliza en 2018, por lo cual no se tiene el valor de la PEA para el año 2019. Obtenido dicho valor, se seguirá expandiendo el índice.] 

```{r IndiceVacantesAvisos, fig.cap = "Índice de vacantes laborales 1980-2018"}
notas = "Serie trimestral \\textbf{índice de vacantes laborales} e \\textbf{índice de avisos laborales}. El índice de vacantes laborales corresponde a la linea solida, tiene base 2010 y esta normalizado por un índice de  población económicamente activa (PEA) con base 2010. La linea punteada es el índice de avisos con base 2010. Las series tienen frecuencia trimestral. Las caídas observadas entorno a 1982 y 2002 corresponden a crisis económicas. La disminución en 1995 se debe a la crisis del tequila. Las series tienen la misma dinámica y leves diferencias. La serie de avisos termina en 2019 mientras la de vacantes en 2018. Esto se debe a que se utilizó la encuesta continua de hogares (ECH) compatibilizada por el Instituto de Economía (IECON) para calcular la PEA, la cual finaliza en 2018. Ambas series son de construcción propia."

fuentes = "1980-1995 datos extraídos de (ref:Urrestarazu1997), entre 1995 y 2001 recolección propia, 1998-2012 se utilizan los datos del Centro de Estudios de la Realidad Económica y Social (CERES), entre 2013 y 2018 datos provistos por diario El País referidos a publicaciones de \\textit{gallito}. Además se utilizan los datos de \\textit{Buscojobs} y \\textit{Computrabajo} bases de datos generadas propiamente. Los datos de \\textit{Buscojobs} contienen la información del portal laboral \\textit{Uruguay Concursa}. También se utiliza la ECH compatibilizada por el IECON desde 1980 a 2018."


dt[, ggplot(.SD, aes(x = fecha)) + 
       geom_line(aes(y = ind_vac), linetype = "solid", color = "black") + 
       geom_line(aes(y = ind_av), color = "black", alpha = 1, size = .4, linetype = "dashed") +
       scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
       scale_x_date(date_breaks = "10 year", date_labels = "%Y") +
       # geom_point(aes(y = ind_av), color = "black", alpha = 1, size = .7) +
       geom_text(aes(label = "- - - Índice avisos", y = 0.35, x = as.Date("2015-06-01")), color = "black") +
       geom_text(aes(label = "--- Índice vacantes", y = 0.25, x = as.Date("2016-01-01")), color = "black") +
       labs(y = "Avisos", x = "Fecha") +
       theme_Publication()
   ]
```

## Curva de Beveridge

```{r beveridge-curve, fig.cap="Curva de Beveridge 1981-2018", fig.ncol = 2, fig.height=5}
#out.width='.49\\linewidth'
# fig.fullwidth = TRUE
# fig.subcap=c('Curva de Beveridge trimestral', 'Curva de Beveridge anual')
notas = "Curva de Beveridge 1981-2018, Montevideo. El Panel A es la Curva de Beveridge (CB) con frecuencia trimestral, mientras el Panel B es la CB con frecuencia anual. Se observa una curva con pendiente negativa y traslados paralelos en ambas direcciones. La década de 1980 tiene un movimiento de U por la crisis de la \\textit{tablita} de 1982. El periodo de 1990 muestra una transición hacia un punto más alejado del origen. El movimiento de U se vuelve a repetir entre 1999 y 2005 por la crisis económica de 2002. El periodo de 2010 hasta 2008 son movimientos sobre la curva. Luego parece haber un traslado hacia el origen y desde 2011 en adelante hay una caída de las vacantes pero con una tasa de desempleo estable (Panel B)."
fuentes = "Compatibilización de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE). Índice de vacantes de elaboración propia."
# Reordenar década
dt$decada <- factor(dt$decada, levels = c(80, 90, 2000, 2010))

ochenta        = "gray"
noventa        = "orange"
dos_mil        = "darkgreen"
dos_mil_diez   = "skyblue"
color_decada <-  c(ochenta, noventa, dos_mil, dos_mil_diez)

# y_lim <- c(min(dt$ind_vac)+1, max(dt$ind_vac)+1)
y_lim <- c(0.1, 1.2)
# x_lim <- c(min(dt$td)+1, max(dt$td)+1)
x_lim <- c(5, 20)
p1 <- ggplot(dt[data.table::between(fecha, "1981-01-01", "2018-10-01"),], aes(y = ind_vac, x = td)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = y_lim) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10), limits = x_lim) +
    geom_path() +
    # scale_color_manual(name = "", values = color_decada) +
    # coord_fixed(ratio = 15) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = "Panel A. Datos trimestrales") +
    theme_Publication()

p3 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
         .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), 
           ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), 
         keyby = .(ano)] %>%
    ggplot(., aes(y = ind_vac, x = td, label = ano2)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = y_lim) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10), limits = x_lim) +
    geom_path() +
    # position=position_jitter(width=.2,height=.02)
    ggrepel::geom_text_repel(fontface = "bold", size = 2, position=position_jitter(width=.1,height=.02), color = "black") +
    # scale_color_manual(name = "", values = color_decada) +
    # coord_fixed(ratio = 15) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = "Panel B. Datos Anuales") +
    theme_Publication()
library(gridExtra)
library(grid)
# grid.arrange(p1,                             # First row with one plot spaning over 2 columns
#              arrangeGrob(p2, p3, ncol = 2), # Second row with 2 plots in 2 different columns
#              nrow = 2
#              )
# plot_grid_split(p1, p3)
library(patchwork)
# grid.arrange(p1, p3, nrow = 1)
(p1 + p3)
  
# lista = list(p1, p3)
# lista[[1]]
# cat('\n\n') 
# lista[[2]]
```

El primer hallazo del trabajo se observa en la Figura \@ref(fig:beveridge-curve) Panel A, es una relación negativa entre vacantes y desempleo en linea con la literatura económica. Se observa que la curva ha tenido traslados tanto hacia afuera como hacia el origen lo que debería relacionararse a modificaciones estructurales de distinta indole o bien shocks. También existen movimiento diagonales relacionados al ciclo económico.

En la Figura \@ref(fig:beveridge-curve) Panel B observamos los promedio anuales del índice de vacantes y tasa de desempleo. Aquí la década de 1980, esta en un cuadrante inferior hacia la izquierda en contraposición a la década del 2000 que es la más alejada del origen. Las observaciones de la década de 1990, se alejan del origen de forma oscilante, mientras las observaciones de los años 2010 se mueven de forma descendente y con una elevada pendiente moviendose entorno al 6-8% de desempleo. Siendo la década con menor variabilidad en la tasa de desempleo.

```{r td-vac-pib, fig.cap="Producto-Vacantes y Producto-Desempleo (1981-2018)", fig.ncol = 2, fig.height=4}
# , out.width='.49\\linewidth', fig.asp=1, fig.ncol = 2, fig.fullwidth = TRUE
notas = "Relación entre producto-vacantes y producto-desempleo, datos desde 1981 hasta 2018 con frecuencia trimestral. En el panel A se observa el producto y el índice de vacantes, hasta el año 2012 la correlación es positiva, luego negativa. En el panel B observamos el producto y la tasa de desempleo. Las décadas de 1980 y 2000 muestran un comportamiento similar, una forma de U que hace referencia las crisis de 1982 y 2002."
fuentes = "Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE). La serie de producto es elaborada por el Banco Central del Uruguay y fue facilitada por el Centro de Investigación Económica (CINVE). Índice de vacantes de elaboración propia."
p1 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), keyby = .(ano)
   ] %>% 
ggplot(., aes(x = ind_vac, y = pib, label = ano2)) +
    geom_point() +
    geom_path() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(0.1, 1.2)) +
    # scale_color_manual(name = "", values = color_decada)+
    coord_fixed(ratio = 1/70) +
    labs(x = "Índice de vacantes", y = "IVF PIB", title = "Panel A. PIB-Vacantes") +
    ggrepel::geom_text_repel(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme_Publication()

p2 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
         .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), 
         keyby = .(ano)
   ] %>% 
    ggplot(., aes(x = td, y = pib, label = ano2)) +
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(5, 18)) +
    geom_path() +
    # scale_color_manual(name = "", values = color_decada)+
    coord_fixed(ratio = 1/6) +
    labs(x = "Tasa de desempleo", y = "IVF PIB", title = "Panel B. PIB-Desempleo") +
    ggrepel::geom_text_repel(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme_Publication()
(p1 + p2)
```

En la Figura \@ref(fig:td-vac-pib) Panel A se observa la relación entre vacantes y PIB, en la cual existe una correlación positiva hasta el año 2012 momento en que se vuelve negativa. La tasa de variación negativa en que caen las vacantes laborales entre 2011 y 2018, es prácticamente la misma entre 1998-2002 y 1981-1983, sin embargo el nivel de actividad no cae en ningún momento por lo que no se observa el movimiento de U típico de 80-82 y 98-2002. 
<!-- Esta observación en conjunto con el análisis de la CB podría estar indicando que el aumento del desempleo de los últimos años no se debe a mayores fricciones del mercado laboral, sino al desempleo estructural asociado a la diferencia sistemática entre las habilidades requeridas por el mercado y las ofrecidas por los trabajadores. Además debería haber un componente de cambio tecnológico e inversión en capital lo cual genera una menor demanda de trabajadores. -->

En la Figura \@ref(fig:td-vac-pib) panel B observamos transición desde una década de crecimiento con alto desempleo, hacia otra con crecimiento y caída de la desocupación (2000). Además, los años 1980 y 2000 vuelven a compartir la forma de U, solo que esta vez es en sentido contrario. La década de 2010 muestra a diferencia del gráfico anterior un comportamiento similar a los años 1990, observandose crecimiento económico con aumento del desempleo.

## Quiebres estructurales

A continuación, ponemos a prueba la hipótesis de existencia de algún quiebre estructural en la relación vacantes y desempleo y buscamos, en caso de existir, la fecha de los quiebres.

Planteamos:
\begin{equation}
log(ind\_vac_i) = \beta_i + \beta_i\log(td_i) + \epsilon_i
\end{equation}

Y sometemos a prueba:
\begin{align}
H_0: \beta_i &= \beta_0 \ \ \ (i = 1, ..., n) \\
H_1: \beta_i &\not= \beta_0 \ \ \ (i = 1, ..., n)
\end{align}

Los resultados se pueden ver en el Cuadro \@ref(tab:quiebres), donde se han llevado a cabo los test de fluctuación generalizada. En la columna _Test_, se identifican los test de quiebres estructural llevados a cabo siguiendo la nomenclatura usada por @Zeileis2002. La diferencia entre Rec-CUSUM y Rec-CUSUM(d) es que se permite la existencia de un rezago, ya que, @Society1988 muestran que los test CUSUM no pierden sus propiedades al relajar algunos supuestos, como trabajar con modelos dinámicos. En todos los casos que no se utilizan rezagos de la tasa de vacantes, se rechaza la hipótesis nula de invariabilidad en los parámetros, por tanto, no se rechaza la existencia de algún quiebre estructural en la relación entre vacantes y tasa de desempleo. Los únicos test que no rechazan $H_0$ son Score-CUSUM y OLS-CUSUM(d) modelos que incluyen un parámetro autoregresivo de vacantes. En todos los casos, siguiendo a @Zeileis2004 se estimo la matriz de varianzas y covarianzas robusta ante la heteroscedasticidad y autocorrelación usando un estimador de kernel cuadrático HAC [@Andrews1991] con un filtrado VAR(1) y una elección automática del ancho de banda basado en una aproximación AR(1).^[El kernel génerico es $\omega_l = K(\frac{l}{B})$ con K la función de kernel y B el ancho de banda. El kernel espectral tiene la siguiente forma $\omega_l = \frac{3}{z^2}(\frac{\sin(z)}{z} - \cos(z))$ siendo $l$ el rezago y $z = \frac{6\pi}{5}\frac{l}{B}$ [@Andrews1991]]

En las Figuras de la sección \@ref(efpAnexo) en el apéndice podemos observar las fluctuaciones del proceso empírico y su comparación con la fluctuación del proceso límite. Esto muestra en que periodo debería estar el o los quiebres en los parámetros, básicamente todos los test utilizados en el Cuadro \@ref(tab:quiebres) comparten que la hipótesis nula de la no existencia de cambio estructural debería ser rechazada cuando el proceso empírico se vuelve improbablemente superior a las fluctuaciones del proceso límite [@Zeileis2002]. 
<!-- DESCRIBIR! -->

Los test Score permiten observar variabilidad en la varianza, al sobrepasar el umbral esta es estadísticamente signifiticativa al 5\%. Tanto en el test Score-CUSUM como Score-MOSUM la varianza muestra fluctuaciones entre 1990 y 1995, y en torno a 2010-2011, sobrepasando el umbral. Por otra parte, el test Score-CUSUM con rezagos con p-valor 0.5, muestra una varianza al límite del umbral, pero sin sobrepasarlo. Es un indicio de que es posible plantear un modelo que no solo tome en cuenta los quiebres en la media condicional sino también en la varianza de los errores lo cual puede mejorar la estímación de los quiebres [@BaiPerron2003].<!-- , por ello se estiman modelos de quiebres tanto de parámetros como varianza siguiendo a @Zeileis2010. -->

Planteamos los test de tipo F generalizados definiendo el tamaño mínimo del intervalo a considerar, en base a un parámetro de ancha de banda h fijado en 0.15.

Utilizamos las tres variaciones propuestas por @Andrews1993 y @Andrews1994, supF, aveF y expF. En el Cuadro \@ref(tab:ftest) podemos observar el test, el valor del estadístico y el p-valor asociado. Se usa la misma matriz de variazas y covarianzas robusta igual que en el caso anterior. En los tres test, encontramos un quiebre estructural en torno a 1990-I, resultado en linea con @Urrestarazu1997.



```{r coefTestEstructural, fig.cap='Coeficientes de diferentes períodos', results="asis", fig.align='center', eval = TRUE, include=FALSE}

library(fxregime)
# FXREGIME 
dt_ts <- ts(data = dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
                      .(pib, ind_vac, td)],
            start = c(1981, 1), frequency = 4)
reg <- log(ind_vac) ~ log(td) + 1
# Buscamos los quiebres cada 5 años
mod_reg <- fxregimes(formula = reg, data = zoo(dt_ts, frequency = 4), h = 20, 
                     breaks = 5)
# confint(mod_reg, level = 0.95, vcov = kernHAC)
# Resúmen completo, primero re-estimar el modelo en los subperiodos y luego aplicando summary
mod_rf <- refit(mod_reg)
# print(xtable(round(coef(mod_reg), 4)), comment = FALSE)
texto = "\\\\footnotesize Explicación de los coeficientes del modelo"
kableExtra::kable(coef(mod_reg), digits = 2, row.names = T, align = "c", caption = "Coeficientes de cada periodo. Resúmen", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = c("$\\beta_0$", "$\\beta_1$", "$\\sigma^2$")
                  ) %>% 
  kableExtra::kable_styling(full_width = F, latex_options = "hold_position") %>% 
  kableExtra::column_spec(column = 2:4, width = "2cm") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::footnote(general = texto, general_title = "Notas:", 
                       threeparttable = TRUE, fixed_small_size	= TRUE)
```

(ref:Zeileis2010) @Zeileis2010
(ref:BaiPerron2003) @BaiPerron2003

```{r vcovHAC-andrews, fig.cap='Coeficientes del período', results='asis', fig.align='center'}

get_model <- function(.mod, .mat = sandwich::vcovHAC) {
    a = data.table::rbindlist(
        lapply(.mod, function(x) {
        broom::tidy(
            lmtest::coeftest(x, .mat)
            )
    }), 
    use.names = TRUE, idcol = "modelo")
    setnames(a, old = names(a), 
            new = c("Modelo", "coeficiente", "Estimación", "Estándar error", 
                    "Estadístico", "p-valor"))
    a[`p-valor` > 0.1, sigf := ""]
    a[between(`p-valor`, lower = 0.05, 0.1),   sigf := "."]
    a[between(`p-valor`, lower = 0.01, 0.05),  sigf := "*"]
    a[between(`p-valor`, lower = 0.001, 0.01), sigf := "**"]
    a[between(`p-valor`, lower = 0, 0.001),    sigf := "***"]
    a[coeficiente == "(Intercept)", coeficiente := "$\\hat{\\beta}_0$"]
    a[coeficiente == "log(td)", coeficiente := "$\\hat{\\beta}_1$"]
    a[coeficiente == "(Variance)", coeficiente := "$\\hat{\\sigma}^2$"]
    a
}

texto = "\\\\footnotesize Estimación para los cuatro periodos detectados mediante los test de quiebres estructural siguiendo a (ref:BaiPerron2003) y (ref:Zeileis2010). Se muestran los valores de los coeficientes estimados $\\\\beta_0$, $\\\\beta_1$ y $\\\\sigma^2$ en la columna \\\\textit{Estimación}, sus errores estándar en \\\\textit{Estándar error} y el valor del estadístico en \\\\textit{Estadístico}. \\\\textit{P-valor} muestra los respectivos p-valores de cada prueba. \\\\textit{Significación} muestra los niveles de significación donde . quiere decir no significativo al 5\\\\% pero si al 10\\\\%. Un * es estadísticamente significativo al 5\\\\%, ** es significativo al 1\\\\% mientras *** es significativo al 0.1\\\\%. En todos los casos los coeficientes son estadísticamente significativos al 5\\\\% a excepción de $\\\\beta_0$ entre 1990 y 1996. Los signos de $\\\\beta_1$ son negativos para todos los períodos. En el período 2013 el valos de $\\\\sigma^2$ es diferente de 0, pero ha sido redondeado. Se usaron variables desestacionalizadas, aunque los resultados se mantienen si se utilizan variables sin desestacionalizar.

\\\\textit{Fuentes}: Datos de vacantes laborales de elaboración propia. Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE)"

kableExtra::kable(get_model(.mod = mod_rf, .mat = sandwich::vcovHAC), digits = 2, row.names = F, align = "c", caption = "Coeficientes de cada periodo", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = linebreak(c("Periodo", "Coeficiente", "Estimación", "Estándar\nerror", "Estadístico", "p-valor", "Significación")
                  )) %>% 
  kableExtra::kable_styling(latex_options = "hold_position",
                            font_size = 10,
    # latex_options = "scale_down"
    ) %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)
```

Finalmente, calculamos todos los posibles periodos de quiebres estructurales de forma generalizada. Para ello seguimos los trabajos de @BaiPerron2003, @BaiPerron1998 y aplicaciones de @Zeileis2003, en especial a @Zeileis2010. Obtenemos tres quiebres estructurales en los años 1990-II, 1996-II y 2013-II usando como función objetivo la log-verosimilitud negativa. ^[Los quiebres son exactamente iguales si las variables se modelan en niveles o logaritmos. Es posible modelar otro cambio estructural eligiendo otra función objetivo como el BIC o RSS, sin embargo dicho intervalo es extremadamente amplio al computarlo con una matriz HAC, al elegir un máximo de 3 puntos de quiebres se obtienen los mismos resultados, si se fija un máximo de 4 se obtiene un quiebre adicional en 2004-I y se mantienen los restantes.] Por lo tanto, los 4 periodos de análisis son 1981-I.1990-II, 1990-III.1996-II, 1996-III.2013-I y 2013-II.2018.IV. En el Cuadro \@ref(tab:vcovHAC-andrews) observamos que en todos los periodos los parámetros muestran el signo correcto según la literatura (negativo). En todos los casos son estadísticamente significativos al 5\%, usando una matriz HAC con ponderadores de Andrews. Sin embargo, usando ponderadores de Kernel en el segundo periodo el p-valor es 0.12. Dada la existencia de autocorrelación y heteroscedasticidad, la elección de la matriz de ponderadores es relevante y puede generar variaciones en la significatividad estadística de algunos parámetros. Por ello los Cuadros \@ref(tab:vcovHAC-lumley), \@ref(tab:kernHAC) y \@ref(tab:NeweyWest) muestran diferentes ponderaciones para los distintos periodos. Es de notar que el único momento que podría ser discutible es entre 1990-III.1996-II, en el cual dependiendo la elección de la matriz el parámetro de la tasa de desempleo puede resultar no significativo, sin embargo, si se estima para el mismo periodo un modelo lineal pero sin varianza de los errores, la tasa de desempleo es estadísticamente significativa, con el signo del coeficiente negativo. En el resto de los casos los coeficientes son estadísticamente significativos al nivel $\alpha = 0.05$, al igual que las varianzas de cada periodo.

Las CB estimadas para los periodos obtenidos se pueden ver en la Figura \@ref(fig:BCtest). En todos los casos se mantiene una relación negativa entre vacantes y desempleo. A la vez que se observan cambios de nivel y pendiente. El primer periodo desde 1981-I hasta 1990-II muestra una curva comparativamente más cercana al origen que 1990-III a 1996-II donde se traslada de forma paralela, lo mismo vuelve a suceder en 1996-III a 2013-II (incluyendo un leve cambio de pendiente). <!-- denotando lo que podría ser un mercado laboral menos eficiente.  --> El comportamiento se modifica a partir de 2013-III en donde la curva tiene un cambio tanto paralelo como de pendiente hacia el origen.
<!-- Una interpretación es que las reformas estructurales llevadas a cabo a partir del año 2005, generaron un efecto negativo en el mercado laboral lo cual se revierte a partir de 2013. Aunque, es poco verosimil en la medida que los gobiernos entre 2005 y 2020 llevaron adelante medidas de protección hacia los trabajadores. Otra lectura sería que dichas reformas tuvieron un efecto impensado y no solo no generaron mayores fricciones en el mercado laborales, sino que las disminuyeron tornandolo más eficiente al aumentar el matching entre trabajadores y firmas^[Mantenemos siempre la condicionalidad sobre la mejora o no de la eficiencia en la medida que no tomamos en cuenta los flujos laborales desde el empleo al desempleo y del desempleo al empleo.]. Otro factor relevante son los portales laborales de Internet y el avance tecnológico que permiten una búsqueda y proceso de contratación a una velocidad comparativamente mayor que los procesos iniciados mediante prensa en papel. -->

```{r BCtest, fig.cap="Curva de Beveridge por periodo", fig.align="center"}

# Hacer una función, esto es muy manual.
dt[fecha <= "1990-04-01", decada_test := "1-periodo"
   ][between(fecha, "1990-07-01", "1996-04-01"), decada_test := "2-periodo"
     ][between(fecha, "1996-07-01", "2013-04-01"), decada_test := "3-periodo"
       ][between(fecha, "2013-07-01", "2019-10-01"), decada_test := "4-periodo"]
dt$decada_test <- factor(dt$decada_test,labels = c("81-90:Q4", "90:Q3-96:Q2", "96:Q3-2013:Q2", "2013:Q3-2018:Q4"))
notas = "Curva de Beveridge (CB) para los periodos obtenidos mediante los test de quiebre estructural siguiendo a (ref:Zeileis2010) y (ref:BaiPerron2003). Se utiliza el paquete fxregime aunque resultados similares se obtienen con el paquete strucchange. En todas las etapas, la relación entre vacantes y desempleo es negativa. Se observan traslados de la CB y movimientos de pendiente. El segundo periodo entre los años 1990 y 1996 muestra un traslado paralelo hacia fuera y sucede entre los años 1996 y 2013, lo que podría estar indicando la posibilidad de un mercado laboral menos eficiente. En 2013-2018 se da un corrimiento al origen, indicios de un mercado laboral que podría ser más eficiente. En ningún caso puede afirmarse categóricamente que el mercado laboral sea más o menos eficiente por un traslado de la CB hacia fuera o hacia el origen, sin incluir como mínimo análisis de los flujos de creación y destrucción de puestos de trabajo."
fuentes = "Datos de vacantes laborales de elaboración propia. Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE)."

ggplot(dt, aes(y = ind_vac, x = td, color = decada_test)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = " \n ") +
    scale_color_manual(name = "", values = color_decada) +
    geom_smooth(method = "lm", formula = y ~ x) +
    theme_Publication()
# Agregar el caso de 4 quiebres y por tanto 5 particiones usando breakpoint.
```

## TVP-VAR

Por último, realizamos una estimación multivariada de un vector autoregresivo con parámetros variables y volatilidad estocástica siguiendo a @Primiceri2005, @Lubik2016 y @Lubik2016b, usando el algoritmo corregido por @DelNegro2015. Nos interesa analizar la variabilidad en los parámetros beta y la matriz de varianzas y covarianzas, sumado al efecto de un shock por parte del producto sobre vacantes y desempleo. <!-- \footnote{Es posible utilizar el test desarrollado por @Stock1998.}. -->

Usamos las primeras 36 observaciones, nueve años, desde 1981-I hasta 1989-IV para calibrar las distribuciones a priori, quedando un periodo efectivo desde 1990 hasta 2018. Simulamos 50 mil veces y elegimos un orden de rezagos igual a uno, es decir, un TVP-VAR(1).^[Se corrieron 50 mil simulaciones con un orden de rezagos igual a dos y los resultados fueron los mismos. Por ello, se muestra un TVP-VAR(1) en la medida que se facilita la visualización.] Las series utilizadas de producto, tasa de desempleo e índice de vacantes son todas de frecuencia trimestral, lo cual (a excepción del PIB que se publica trimestralmente) va a generar series con menor variabilidad.

La restricción de identificación es que son los shocks desde el producto (shocks de productividad) los cuales afectan al desempleo y las vacantes laborales, con un rezago. Por lo tanto, el orden de exogeneidad de las variables es que el pib es la primer variable, seguido del índice de vacantes y la tasa de desempleo. El orden de la segunda y tercer variable, no es una restricción de identificación sino una normalización necesaria que puede modificar los resultados [@Primiceri2005], sin embargo, en este caso el orden no genera diferencias. La estructura de identificación elegida para las innovaciones es básicamente una especificación de Cholesky. En la Figura \@ref(fig:svar-parameters) observamos los desvíos estandar variables a lo largo del tiempo de los residuos del modelo, se gráfica la media (posterior) y los percentiles 16 y 84.\footnote{Bajo normalidad los percentiles 16-84 corresponden a las cotas de una desviación estándar en los intervalos de confianza}. Adicionalmente se estima un VAR de parámetros fijos y sin volatilidad estocástica el cual se puede ver en la Figura \@ref(fig:svar-parameters) fila tres columna tres. En el resto de los casos no se observa debido a que no se encuentra en el rango de valores mostrados. 

El gráfico muestra dos resultados, el primero es que las vacantes laborales son estables para el periodo. El segundo es que si bien la magnitud es leve, parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante con una transición suave, donde el primero presenta una mayor varianza tanto para el pib como la tasa desempleo, mientras en el caso de las vacantes no se observan diferencias.^[En los test de quiebre estructural si se definen cuatro posibles quiebres estructurales, el cuarto se genera en torno a 2004.] Las modificaciones en las varianzas comienzan previo a 2005, lo cual se relaciona con la crisis de la economía en 2002 y el posterior crecimiento ininterrumpido a partir del tercer trimestre de 2003. 

```{r svar-parameters, fig.cap="Rezagos, coeficientes y volatilidades modelo TVP-VAR", fig.height=6, fig.width=8, fig.align="center"}
notas = "Se grafican las matrices $\\hat{c}_{t}$, $\\hat{B}_{jt}$ y $\\hat{\\Sigma}_{jt}$ desde 1990 hasta 2018 para la tasa de crecimiento del producto, índice de vacantes y tasa de desempleo. Observamos los desvíos estandar de los residuos del modelo, la media (posterior) y los cuantiles 16 y 84. Parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante, donde el primero presenta mayor varianza tanto para el pib como la tasa desempleo. En las vacantes, no se observan diferencias en coeficientes y rezagos ni en varianzas. Producto y tasa de desempleo muestran variaciones en la matriz de varianzas-covarianzas. La linea punteada en la tercera fila-columna corresponde a la estimación de un Vector Autorregresivo (VAR) de parámetros fijos y sin volatilidad estocástica. En el resto de las figuras la linea punteada no se observa por el rango de valores mostrados."
fuentes = "Serie de producto trimestral calculada por el Banco Central del Uruguay (BCU) y facilitada por el Centro de Investigaciones Económicas (CINVE). Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE). Índice de vacantes de construcción propia."
par(mfrow = c(3, 3))
for(i in c("intercept", "lag1", "vcv")) {
    for(j in 1:3) {
      if(j == 1) k <- "pib" else if (j == 2) k <- "vacantes" else k <- "desempleo"
      if (i == "intercept") {
        k <- eval(bquote(expression(.(k) ~ c[j][t])))
      } else if (i == "lag1") {
        k <- eval(bquote(expression(.(k) ~ B[j][t])))
      } else {
        k <- eval(bquote(expression(.(k) ~ Sigma[j][t])))
      }
      make_plot(.fit = fit, .type = i, .var = j, .title = k)
    }
}
```

En la Figura \@ref(fig:svar-parameters) se observa que los parámetros de las variables rezagados $B_{j,t}$ y los interceptos $c_{t}$ tienen poca variabilidad a lo largo del periodo de análisis.^[se muestra la estimación de un TVP-VAR(1) en vez de un TVP-VAR(2), dado que los resultados no se modifican y se facilita su visualización] Este resultado es común en la literatura de TVP-VAR [@Lubik2016b], sin embargo, como se notó anteriormente si existe variabilidad en las innovaciones del producto y desempleo.

El resultado es robusto frente a diferentes especificaciones, porque con variables en niveles o en logaritmos los resultados no cambian. Adicionalmente se estima un modelo bivariado con desempleo y vacantes, tanto en niveles como en logaritmos, obteniendo las mismas conclusiones. Se podría pensar en el uso de un modelo de parámetros fijos y volatilidad estocástica, sin embargo, en la medida que el TVP-VAR no impone la restricción de parámetros fijos pero se obtiene dicho resultado no parece necesario.

Por último analizamos las FIR mediante la estimación de la mediana. Su visualización no es trivial, en la medida que en cada momento del tiempo existe una FIR. Una opción es visualizar distintos momentos y observar si existen diferencia (la que se elige), otra es mostrar una visualización en tres dimensiones. En nuestro caso, las FIR en los distintos momentos del tiempo se mantienen prácticamente iguales. 

En la Figura \@ref(fig:FIR) observamos un shock en el último momento de la serie (es decir 2018-Q4). En el Panel A tenemos un shock desde el producto hacia las vacantes laborales y se identifica un efecto positivo en todo momento. Esto tiene sentido en la medida que una innovación de productividad, debería generar que la demanda laboral de las empresas aumente, en la medida que crezca el nivel de producción de la economía. Al contrario en la Figura \@ref(fig:FIR) Panel B observamos como el efecto del shock del producto genera un efecto negativo en todo momento sobre la tasa de desempleo. Al mejorar la productividad de la economía, el desempleo debería disminuir en la medida que la economía es capaz de aumentar su producción, lo cual lleva a que las empresas aumenten su contratación (más o menos dependiendo de cuan sesgado sea hacia el uso de tecnología y capital). Los efectos de entrada o salida de personas a la PEA esta presente en ambos indicadores.

```{r FIR, fig.cap="FIR", out.width='1\\linewidth'}

notas = "FIR desde producto hacia el índice de vacantes (A) y tasa de desempleo (B). La linea negra es la mediana, mientras las áreas grises refieren a los intervalos de confianza al 5-95\\% y 25-75\\%. Con color rojo la FIR de un VAR de parámetros fijos. Los signos de la mediana del TVP-VAR se ajusta a lo que se espera de un shocks desde el producto a vacantes y desempleo. En el primer caso un efecto positivo y en el segundo un efecto negativo. En el caso de un VAR los resultados son menos claro con valores en torno a cero."
# Tengo que modificar plot_irf para que sea en español y que los label de eje y estén rotados en 90 grados. O sino pasarlo a ggplot.
par(mfrow = c(1, 2))
plot_irf(impulse = 1, response = 2, .main = "Panel A. Vacantes")
plot_irf(impulse = 1, response = 3, .main = "Panel B. Desempleo")
```

Para finalizar, corroboramos si los shocks desde producto a desempleo y producto a vacantes tienen signos y valores similares para distintos momentos del tiempo. En la Figura \@ref(fig:FIRs) observamos shocks desde el producto hacia vacantes y desempleo en trece momentos temporales distintos (desde el periodo inicial hasta el momento 97, pasos de a ocho) en un horizonte de hasta veinte pasos (cinco años). Es notorio que el efecto sigue la misma dinámica y los valores son similares tanto para las vacantes Panel A, como para el desempleo Panel B. 

```{r FIRs, fig.cap="FIR diferentes años", out.width='1\\linewidth'}

notas = "FIR desde producto hacia el índice de vacantes (A) y tasa de desempleo (B). Se grafican las medianas de la FIR para 8 periodos diferentes y se observa que siguen la misma dinámica con leves diferencias. Los signos de la mediana del TVP-VAR se ajusta a lo que se espera de un shocks desde el producto a vacantes y desempleo. En el primer caso un efecto positivo y en el segundo un efecto negativo."
abline2 <- function(...){ 
    abline(..., lty = 4, lwd = 0.3)
}
fir_periodos <- function(fit, respuesta, impulso = 1, titulo) {
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
cols1 <- cbPalette#[c(2, 4, 2)]
cols2 <- cbPalette#[c(2, 4, 6)]

fir <- list()
j = 0
for(i in seq(1, 100, 8)) {
    j = j + 1
    fir[[j]] <- impulse.responses(fit, 
                      impulse.variable = impulso, 
                      response.variable = respuesta, 
                      t = i, scenario = 2, 
                      draw.plot = FALSE)$irf
}

gdat <- rbind(0, sapply(fir, function(z) apply(as.matrix(z), 2, median))) 

yb <- c(min(gdat), max(gdat))
matplot2(x = 0:20, y = gdat, ylim = yb, xlab = "Horizonte",
         col = cols2) 
title(main = titulo, cex.main = 0.6, adj = 0, line = 0)
abline2(v = seq(5, 20, 5), h = seq(yb[1], yb[2], (yb[2]-yb[1])/10)) # 0.0001
}
par(mfrow = c(1, 2))
fir_periodos(fit = fit, respuesta = 2, impulso = 1, titulo = "Panel A. Vacantes")
fir_periodos(fit = fit, respuesta = 3, impulso = 1, titulo = "Panel B. Desempleo")

```


<!--chapter:end:05-resultados.Rmd-->

# Discusión {#cap:Discusion}

## Indicador
<!-- %%%%%%%%%%% Respuesta a posibles críticas -->
<!-- % 1. Serie es de avisos y no de puestos, usar la serie de avisos y puestos en 1996 y 1997 para mostrar que las series siguen el mismo patrón.  -->
<!-- % 2. No se conoce serie CERES original, Test realizados sobre la serie de ceres -->
<!-- % 3. Serie contiene datos que no son de Montevideo -->
<!-- % 4. Serie esta sesgada hacia puestos de baja calificación? -->
<!-- % 5. Por qué no usar las tendencia-ciclo de cada serie y luego unir? -->
<!-- % 6. Duplicados? Reponderaciones? gallito -->
<!-- % 7. Duplicados, entre las != series gallit-ct-bj? Text Mining. -->

El índice de vacantes construido esta sujeto a comentarios y futuras mejoras, tanto en lo que refiere a las fuentes de datos, los métodos de imputación utilizados así como la metodología empleada para calcular y combinar las diferentes series individuales. Por ejemplo, en lo referido a la información utilizada la serie construida es de avisos laborales no de puestos de trabajo. En la medida que los puestos sigan una dinámica diferente que los avisos estaríamos representando incorrectamente la demanda laboral por parte de las empresas. Para corroborar este punto, se llevan a cabo dos pruebas: a) Entre 1996 y 1998 se contabilizan todos los puestos publicados en la sección de avisos destacados.\footnote{No se contabilizan los avisos publicados en otras secciones por un tema de tiempo. Por lo cual, se asume que los avisos en dichas secciones corresponden a un puesto} En la Figura \@ref(fig:molina96-98) se observa que puestos y avisos se mueven de forma conjunta, manteniendo una diferencia de nivel. b) Entre los años 2000 y 2009 se cuenta con la base de datos creada por @Alma2011, en la cual contabilizan todos los avisos y puestos laborales en las primeras 2 semanas de los meses 3, 5 o 6 y 9 de los respectivos años. En la Figura \@ref(fig:iecon-avisos-puestos) se puede ver que ambas tienen una diferencia de nivel.

Además, para el periodo 1998-2014 se usan datos del ICDL al cual se le extrajeron los componentes estacionales, no estando disponible la serie original y, no se tiene detalles precisos de la metodología. Sin embargo, se pudo corroborar que el ICDL corresponde, al menos hasta 2012, con los avisos laborales en papel recogidos en _Gallito_. El ICDL tiene base agosto 1998, dicho mes fue contabilizado completamente lo cual permite generar una aproximación a la cantidad de puestos. A futuro es posible ampliar las muestras entre 1998 y 2014, combinar dichos datos con los obtenidos por @Alma2011, generar una subserie para el periodo 1998-2014, comparar con el ICDL y potencialmente dejar de utilizarlo. Esto permitiría recobrar el componente estacional, pero para este trabajo dicho componente no es relevante y tampoco lo es en la medida que la CB se suele analizar de forma desestacionalizada. Si lo es para tener una medida más precisa de la cantidad de publicaciones laborales y lograr una mejor desestacionalización para la serie completa entre 1980 y 2019.

Otro comentario es que la serie de avisos es de Montevideo aunque contiene información de otros departamentos. Observando la Figura \@ref(fig:avisos-dpto) que corresponde a todos los avisos obtenidos mediante scraping en el año 2019, se puede ver que para Buscojobs, Computrabajo y Gallito las publicaciones correspondientes a Montevideo son de 82\%, 84\% y 95% respectivamente. Por lo tanto, se puede mantener que es representativa de la capital. Es posible suponer que el porcentaje de publicaciones no de Montevideo se mantiene estable a lo largo del periodo y reponderar la serie de avisos. Sin embargo, se opta por trabajar con todos los datos.

A los posibles sesgos de información de otros departamentos se añade que la serie de _Gallito_ presenta un sesgo hacia puesto de baja calificación. En la Figura \@ref(fig:ga13-18-comparacion) se observa que la mayor cantidad de avisos corresponden a puestos de auxiliar, técnico/especialista, ejecutivo comercial y peón. Si bien, esto no es una muestra del sesgo, puesto que los puestos de mayor jerarquía son por definición menor a los puestos de menor calificación, por lo cual una simple comparación porcentual no nos dice nada. @Alma2011 utilizando la información de puestos laborales para el periodo 2000-2009, utilizando la misma fuente de datos realizan un análisis pormenorizado de la información publicada en cada aviso laboral y lo comparan con la ECH, concluyendo que _Gallito_ tiene sesgo hacia puestos de baja calificación. Es posible extender el análisis de @Alma2011 y replicarlo para el periodo 2013-2018. Lo mismo puede realizarse para el año 2019 con la información recolectada en _Buscojobs_ y _Computrabajo_.

Un punto marcado por @Rama1988 es que la existencia de informalidad es una característica propia de los países subdesarrollados a la cual Uruguay no escapa. Ello se agrava para los años entre 1980 y 2005, debido a las graves crisis económicas de 1982 y 2001-2002, aunque disminuye de forma importante en los últimos quince años desde 30% a menos de 20% en Montevideo [@Amarante2015]. Las vacantes laborales publicadas en la prensa no captan los puestos generados en la informalidad. Una posible solución sería estimar la informalidad para el periodo considerado y ajustar una nueva serie de vacantes que tome en cuenta dicho factor, la dificultad radica en que recién a partir del año 2001 en la ECH se pregunta a las personas si cotizan a la seguridad social.

En la medida que existe informalidad y la información de _gallito_ tiene un sesgo hacia puesto de baja calificación ¿Cuán representativo es un índice de vacantes laborales respecto de la demanda laboral? Se puede argumentar que tomar las vacantes laborales no es representativo de la demanda laboral. Sin embargo, en el caso de EEUU @Barnichon2010 construye un índice de vacantes utilizando tanto fuentes de avisos laborales en la prensa, como encuestas a empresas realizadas por el Bureu of Labor Statistics mediante el Job Openings and Labor Turnover Survey (JOLTS). Encuentra que las series siguen la misma dinámica, aunque la serie de JOLTS necesita ser reescalado.

Por último, si bien la serie de vacantes tiene una correlación positiva con el PIB para el periodo 1980-2019, se correlaciona negativamente si se toma desde 2013 en adelante. Esto genera dudas importantes sobre su construcción en los últimos cinco años. Sin embargo, aún si se sumaran de forma bruta todos los avisos recolectados de _Gallito_, _Computrabajo_ y _Buscojobs_ la correlación negativa se mantiene. Vale recordar que _Buscojobs_ contiene la información publicada en el portal _Uruguay Concursa_, por lo cual son cuatro fuentes de datos relevantes. Esto puede verificarse en base a las encuestas del uso de Internet realizadas por RADAR, en base a los datos recolectados las páginas más utilizadas por las personas que buscan trabajo son las utilizadas en este trabajo, a excepción de _LinkedIn_. Es inverosímil pensar que agregando _LinkedIn_ cambie de forma tan relevante la dinámica de los avisos, en especial porque dicha página levanta publicaciones de los cuatro portales utilizados en este trabajo. Una posible explicación es que la economía este pasando por un cambio importante en el cual pese al crecimiento las firmas demandan cada vez menos puestos laborales, en la medida que sustituyen trabajo por capital y utilizan nuevos procesos de producción. Si se añade que la serie principal de este trabajo, _Gallito_, tiene sesgo hacia puestos de baja calificación [@Alma2011] y estos son los más afectados por los nuevos procesos de producción y prestación de servicios, el efecto de crecimiento con menos solicitudes laborales se potencia. 
<!-- Agregar el tema de los shares, porque usar las 3 series y no solamente 1 y ajustar los share? Creo que es la crítica más relevante a la serie -->
<!-- (CITAR ALGÚN TRABAJO SOBRE INFORMALIDAD EN URUGUAY) -->
<!-- % 5. Por qué no usar las tendencia-ciclo de cada serie y luego unir? -->
<!-- % 6. Duplicados? Reponderaciones? gallito -->
<!-- % 7. Duplicados, entre las != series gallit-ct-bj? Text Mining. -->

## Estimaciones

<!-- Y lo de los prior sensibles -->
El resultado obtenido en la estimación multivariada de la CB se encuentra en la literatura TVP-VAR, ya que es común encontrar poca variabilidad en interceptos y rezagos y quiebres en la matriz de covarianzas, pese a que el proceso generador de datos debería mostrar quiebres en los $B_t$ [@Lubik2016b]. Esto se agrava al trimestralizar la serie de desempleo y vacantes laborales, ya que se pierde variabilidad. <!-- Es posible calcular a partir de 1996 las vacantes de forma mensual, por lo cual sería interesante realizar un análisis bivariado desde 1996 en adelante entre vacantes y desempleo.  --> También podría realizarse un análisis similar al de @Lubik2016b y ver cuan sensible es la estimación a la elección de priors. Por último, debe notarse una leve disminución en la matriz de varianzas y covarianzas de producto y desempleo en torno a 2005 lo cual parece indicar dos etapas que coinciden con la salida de la crisis económica de 2002 y un periodo de alto crecimiento económico y bajo desempleo, cambios asociados al ciclo económico y un auge de commodities. Adicionalmente se computan las funciones de impulso respuesta (FIR) y se comparan con un VAR de parámetros fijos y sin volatilidad, obteniendo diferentes relevantes en las estimaciones, tanto en magnitud como signo. También se calculan las FIR para distintos momentos del tiempo y se observa que las medianas son muy similares. Por último en la medida que un TVP-VAR con volatilidad estocástica es una forma reducida de un DSGE, esto podría dar indicio de dos períodos diferentes a evaluar bajo dicha metodología. Puede ser posible que las series no tengan quiebres continuos y si discretos, como se obtuvo, aunque esto es un tanto dudoso en la medida que no se rechaza la raíz unitaria de la serie de vacantes y desempleo como puede observarse en el Cuadro \@ref(tab:test-ru-trim).

Los resultados en los test de quiebres estructurales son cuatro o cinco periodos de tiempo dependiendo si los quiebres se eligen utilizando el BIC o BIC corregido y se varía la cantidad máxima de quiebres permitidos. Aunque los quiebres previos no se modifican si se permite un quiebre adicional. Los resultados indican distintas fases de la CB que pueden deberse a cambios por el ciclo económico (dadas las crisis y periodos de alto y bajo crecimiento o alteraciones de la PEA) como a factores estructurales de distinta índole como institucional, tecnológico o un _mismatch_ de habilidades.

<!-- El factor institucional debería jugar un factor relevante en los traslados de la CB resultado en linea con @Nickell2002, @Gujarati1972 y @Bouvet2012, en especial en las décadas de 1990 y 2000. Mientras los factores cíclicos deberían estar presentes en prácticamente todas las décadas. Por último las mejoras tecnológicas no pueden estar ausentes en los años 2010, dada la transformación en el mercado de publicaciones y búsqueda laboral. -->
Por ejemplo, la década de 1980 se caracteriza inicialmente por la crisis de la _tablita_ de 1982, efecto asociado al ciclo económico (incluyendo shocks) que genera la forma de U observada en la CB entre 1981 y 1988, una elevada negociación por rama empresarial, vuelta de consejos de salarios y una mayor presencia de los sindicatos [@Filgueira2003;@Quinones2001]. Por el contrario en los años 1990 cae la negociación a nivel de rama, la cantidad de afiliados y se deja de convocar los consejos de salarios, en los tres casos los cambios se concentran en el primer lustro de la década [@Filgueira2003;@Antia2001]. También hay una importante ganancia de productividad, caída de las horas trabajadas [@Quinones2001], desempleo elevado (8-10%), aumento de la informalidad al aumentar la proporción de ocupados en el sector servicios en conjunto a la caída de trabajadores en el sector manufacturero [@Amarante2007;@Amarante2015] y un alto crecimiento del producto hasta 1998. Aunque dicha variación en parte fue fomentada por la apertura de la economía uruguaya tanto de cuenta corriente como de capitales, la cual generó como contrapartida que las empresas locales no fuesen capaces de competir con los productos importados y se vieran obligadas a cerrar [@Antia2001], lo cual puede verse como shocks tanto sectoriales o de demanda agrega al igual que los analizados por @Blanchard1989 o shocks comunes como en @Benati2013. De hecho, en la CB se observa como entre 1990 y 1995 las vacantes crecen de forma permanente mientras el desempleo se estabiliza en torno al 9%, hasta la crisis del tequila que genera una importante caída en las vacantes y un aumento en el desempleo que termina entre 12-13% en 1996 (momento del segundo quiebre estructural). Luego siguen tres años de crecimiento hasta una nueva crisis económica (2002) que repite la forma de U en la CB. A partir de allí, parece haber un efecto típico de ciclo económico con movimientos sobre la curva por aumento de vacantes, caída de desempleo y aumento del producto (tasas de 5-6%). Una posible causa son los shocks de oferta y demanda agrega a nivel internacional con elevados precios de commodities y fuerte demanda en especial de China y EEUU. 

También podemos pensar en los desplazamientos de la fuerza laboral que pueden reducir la eficiencia del matching [@Hobijn2013], siendo el más notorio la caída de la PEA en torno a 2004 (momento del posible y opcional tercer quiebre estructural), movimiento demográfico originado por la crisis de 2002 el cual podría alterar la CB [@Bewley1979]. Por otro lado tenemos las reformas estructurales iniciadas a partir de 2005 entre las que se incluye la reforma tributaria del año 2007, el importante aumento del salario mínimo en reiteradas ocasiones [@Amarante2015], convocatoria de los consejos de salarios, aumento de la negociación por rama de actividad, regulación del trabajo de servicio doméstico, ley de responsabilidad penal empresarial y reforma de la salud entre otras, reformas que pudieron fomentar que la central obrera PIT-CNT cuadruplicara sus afiliados entre 2005 y 2020 [@Bergara2017]. Estos factores institucionales son similares a los identificados por @Bouvet2012 y @Nickell2002 y, pueden fundamentar el quiebre estructural de 1996-2013 (o el posible quiebre de 2004) y también el de 2013. 

Aunque en este último quiebre de 2013 (movimiento hacia el origen de la CB), es difícil argumentar si las reformas institucionales mejoraron o empeoraron el matching. Por un lado es posible plantear que pudieron haber favorecido a los trabajadores y deberían tener un efecto negativo en la medida que aumentan las fricciones del mercado laboral, como densidad sindical y salario mínimo [@Hall2005], y comienzan a ser operativas a partir del fin de periodo de _bonanza_ de 2005-2012 donde las tasas de crecimiento se reducen de 5-6% a 2-3%, y en los últimos años 0-2%. De hecho, @Merlo2019 encuentra que tanto la destrucción como la creación de puestos de trabajo tiene una tendencia decreciente a partir de 2005, indicando un mercado laboral menos dinámico con un crecimiento neto de puestos decreciente.
Sin embargo, podría suceder que las empresas alteren su comportamiento en respuesta a estas reformas buscando mejores match y generen procesos de contratación más eficientes al aumentar su _intensidad de búsqueda_ [@Haltiwanger2012].

Por último, el traslado hacia el origen de la CB en 2013 podría deberse a cambios en los procesos de contratación, resultado observado por @Elsby2015. Recordemos que a partir de 2008 los portales laborales comienzan a tener un flujo de avisos relevante, a la vez que aumenta sistemáticamente la penetración de Internet en los hogares. Dada la enorme caída en el índice de vacantes y el aumento leve del desempleo relativo a sus valores históricos, no debería ser extraño que la eficiencia del matching haya aumentado considerablemente guiado en parte por la disminución en el costo y tiempo de búsqueda tanto del empleador como trabajador y también por un aumento en la _intensidad de búsqueda_ [@Haltiwanger2012].

<!--chapter:end:06-discusion.Rmd-->

# Conclusión {- #cap:Conclusiones}
\markboth{}{Conclusiones}

Este trabajo ha creado una base de datos de cantidad de avisos laborales, con la cual se calcula un índice de vacantes laborales que busca reflejar la demanda laboral por parte de las empresas en Montevideo, en la medida que la capital concentra cerca del 50% de la población y una parte importante de la producción la misma debería ser representativa de los movimientos de vacantes a nivel Nacional. Se ha logrado generar una serie desde 1980-I hasta 2019-IV siendo la única serie de vacantes laborales actualizada en Uruguay. Se ha escrito el código en lenguaje _R_, código público y reproducible, para poder obtener la información de avisos laborales futuros, lo cual da pie a que el periodo de análisis se pueda seguir extendiendo de forma ininterrumpida, aumentado las fuentes de datos y dotando de una herramienta clave como la Curva de Beveridge a la política económica.

Es de interés que el índice de vacantes construido sea perfeccionado en trabajos futuros, se agreguen nuevas fuentes de información (como Linkedin) y se mejoren datos previos. En especial durante el periodo de 2000 a 2012 en el cual se utiliza una serie filtrada sin conocimiento detallado de la transformación aplicada. Adicionalmente, es posible reducir la frecuencia de las series de trimestral a mensual. Además de mejorar el indicador se podría calcular tanto la tendencia como el ciclo de la serie de vacantes mediante la aplicación del filtro de Hamilton y hacer un análisis conjunto con PIB y desempleo en el dominio de frecuencias, para poder analizar en que etapa del ciclo se encuentra la demanda laboral.

Utilizando metodología TVP-VAR no observamos modificación en los parámetros asociados a la media condicional y las modificaciones en las matrices de varianzas y covarianzas son leves o nulas. Sin embargo, las FIR por un shock del producto a vacantes y desempleo tiene efectos significativos. La lectura sería que los traslados paralelos observados de la CB se deben a shocks con efectos significativos de signo opuesto. Si bien la economía ha tenido fuertes shocks externos, dada la cantidad de reformas que pueden catalogarse de estructurales, sería llamativo que la variabilidad en vacantes y desempleo pueda deberse solamente a innovaciones. Es posible profundizar sobre los modelos TVP-VAR, haciendo un análisis de priors y sus efectos, a la vez que probar y/o combinar diferentes restricciones de identificación y realizar el análisis con las series mensuales.

<!-- Se han buscado cambios estructurales en el mercado laboral mediante diferentes metodologías de quiebres estructurales. En todos los casos se ha llegado a la conclusión de rechazar la hipótesis de no existencia de quiebre en el proceso generador de datos. Se analiza en que momento la economía ha transitado esos cambios, encontrando tres o cuatro puntos de quiebre, lo que indica cuatro o cinco periodos de tiempo. Siendo reservados en cuanto a los quiebres podemos mantener que el mercado laboral ha transitado por cuatro periodos. Desde 1980 hasta 1990, siendo este un periodo de mayor eficiencia laboral (entendiendo a la misma como una CB más cercana al origen), de 1990 hasta 1996, lo que puede verse como un deterioro del mercado laboral con menos vacantes y mayor desempleo. Son años de transición, que se enmarcan dentro de los cambios que tuvo la economía uruguaya a principio de 1990, modificaciones importantes que abrieron fuertemente la economía siendo un periodo de crecimiento moderado y alto desempleo.  -->

Las CB estimadas mediante los test de quiebre estructural, tienen sustento en los procesos transcurridos en la economía uruguaya. Es de hacer notar que las caídas observadas en las vacantes laborales son de orden similar a las observadas en 1982 y 2002, sin embargo la tasa de desempleo en ningún momento ha llegado a niveles similares. La única forma en que la economía soporte tal nivel de caída en la demanda laboral, es que el matching en el mercado laboral haya aumentado de forma considerable. El factor institucional es imposible no este presente dada la cantidad de reformas de caracter estructural y modificaciones en las instituciones del mercado laboral. Pero no es el único, ademas de los factores cíclicos, un factor posiblemente relevante, son los portales laborales de Internet y el avance tecnológico que permiten una búsqueda y proceso de contratación a una velocidad comparativamente mayor que los procesos iniciados mediante la prensa en papel. Las agencias de contratación (y los mismos portales) pueden manejar enormes volúmenes de información sobre posibles candidatos que permiten una búsqueda sobre un universo mayor e incluso de forma selecta. En conclusión, si bien las factores institucionales deberían estar jugando un papel relevante en los traslados de la CB, no es menos cierto que han habido modificaciones importantes asociadas a factores tecnológicos y factores de ciclo económico, especialmente en los últimos veinte años. Por lo cual, no es posible responder cual debería ser la causa detras de los movimientos de la CB. 

Finalmente es de interés para la política económica identificar cuales son las causas que están detrás de los traslados de la CB en la medida que se puedan tomar acciones para mejorar la eficiencia del mercado laboral. Por lo que es necesario que trabajos futuros investiguen al respecto y agreguen análisis sobre los flujos laborales.

<!-- Finalmente, es llamativo que periodos con políticas laborales tan diferentes como los 90 y 2010 tengan un comportamiento similar en cuanto al crecimiento de la actividad y aumento del desempleo. -->


<!--chapter:end:06-conclusion.Rmd-->

<!-- <div id="refs"></div> -->
<!--
The bib chunk below must go last in this document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->
<!-- `r if (knitr:::is_latex_output()){ ' -->
<!-- # Referencias {-} -->
<!-- <div id="refs"></div> -->
<!-- '}` -->

<!-- `r if (knitr:::is_latex_output()){ ' -->
<!-- # Referencias {-} -->
<!-- <div id="refs"></div> -->
<!-- '}` -->

<!-- \backmatter -->

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

<!-- # Referencias {-} -->

<!--
This manually sets the header for this unnumbered chapter.
-->
<!-- \markboth{References}{Referencias} -->
<!--
To remove the indentation of the first entry.
-->
<!-- \noindent -->

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

<!-- \setlength{\parindent}{-0.20in} -->
<!-- \setlength{\leftskip}{0.20in} -->
<!-- \setlength{\parskip}{8pt} -->

<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->



<!-- `r if (knitr:::is_latex_output()){ ' -->
<!-- # Referencias {-} -->
<!-- <div id="refs"></div> -->
<!-- '}` -->


<!-- \backmatter -->
\markboth{References}{Referencias}
\noindent
\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

`r if (knitr:::is_latex_output()){ '
# Referencias {-}
<div id="refs"></div>
'}`
<!-- \markboth{References}{Referencias} -->

<!--chapter:end:99-references.Rmd-->

<!-- # --- -->
<!-- # output: html_document -->
<!-- # editor_options: -->
<!-- #   chunk_output_type: console -->
<!-- # --- -->
`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'`

<!-- \mainmatter -->
<!-- \noindent -->
\setlength{\parindent}{0.20in}
\setlength{\leftskip}{-0.20in}
\setlength{\parskip}{8pt}

<!--
If you feel it necessary to include an appendix, it goes here.
-->

# Apéndice

## Modelo básico de búsqueda y emparejamiento

En esta sección se describe el modelo básico de búsqueda y emparejamiento desarrollado en @Pissarides2000. 

\begin{equation} \label{eq1}
mL = m(uL, vL)
\end{equation}
La ecuación \eqref{eq1} muestra el número de match durante una unidad de tiempo
\begin{equation} \label{eq2}
q(\theta) = m(\frac{u}{v}, 1)
\end{equation}
Ecuación \eqref{eq2} es la tasa a la cual una vacante se completa.
\begin{equation} \label{eq3}
\frac{1}{q(\theta)}
\end{equation}
Ecuación 3 es la duración media de las vacantes.
\begin{equation} \label{eq4}
\lambda(1-u)L\delta t
\end{equation}
Ecuación \eqref{eq4} es el número promedio de trabajadores que pasan al desempleo durante un intervalo de tiempo
\begin{equation} \label{eq5}
mL\delta t = u\theta q(\theta)\delta t
\end{equation}
Ecuación \eqref{eq5} es el número promedio de trabajadores que pasan al empleo durante un intervalo de tiempo. Siendo $\theta q(\theta)\delta t$ la probabilidad de transición del desempleo.

Al restar los dos flujos correspondiente a la ecuación 4 y 5, tenemos la evolución.
\begin{equation} \label{eq6}
\frac{\delta u}{\delta t} = \lambda(1-u)-\theta q(\theta)u
\end{equation}
Usando que en estado estacionario (EE) la variación debe ser cero, y despejando se obtiene la ecuación \eqref{eq7} que representa la Curva de Beveridge.
\begin{equation} \label{eq7}
u = \frac{\lambda}{\lambda+\theta q(\theta)}
\end{equation}
En el caso del modelo básico esta es la primera ecuación clave.

\subsection{Creación de trabajo}
J es el valor presente descontado del beneficio esperado de un puesto ocupado. V es el valor presente descontado del beneficio esperado de una vacante. Bajo un mercado de capitales perfectos, usando horizonte infinito y sin cambios dinámicos esperados en los parámetros V satisface la ecuación de Bellman.
\begin{equation} \label{eq8}
rV = - pc + q(\theta)(J-V)
\end{equation}
Bajo el supuesto de mercado de capitales perfecto, el puesto es un activo que pertenece a la firma y su valor es tal que el costo de capital rV es igual a la tasa de retorno esperado del activo. El costo de la vacante por unidad de tiempo es pc, la misma cambia de estado de acuerdo a un proceso de Poisson con tasa $q(\theta)$, dicho cambio de estado genera un retorno neto $J-v$, el cual es constante por estar en EE, ya que, V o J no varían.

Al aplicar la condición de cero beneficio (ZPC), las rentas de las vacantes laborales son cero, por lo tanto, $V=0$. Despejando se obtiene:
\begin{equation} \label{eq9}
J = \frac{pc}{q(\theta)}
\end{equation}
La ecuación \eqref{eq9} es la segunda más importante para resolver el equilibrio del modelo. Establece que en equilibrio, la estrechez del mercado es tal, que el beneficio esperado de un nuevo puesto laboral es igual al costo esperado de contratar a un trabajador.

rJ es el flujo del costo de capital de un puesto ocupado.
\begin{equation} \label{eq10}
rJ = p - w - \lambda J
\end{equation}
El puesto ocupado genera un retorno neto de p- w, siendo p el producto real y w el costo del trabajo. El trabajador enfrenta un riesgo $\lambda$ (shock negativo) que conlleva perder J.

Usando las ecuaciones \eqref{eq10} y \eqref{eq9} llegamos a la condición marginal para la demanda laboral. Es decir, la curva de creación laboral (job creation, JC).
\begin{equation} \label{eq11}
p - w  - \frac{(r+\lambda)pc}{q(\theta)} = 0
\end{equation}

\subsection{Trabajadores}

La ecuación \eqref{eq12} representa el activo dado por el capital humano y su valuación llevada a cabo por el mercado U.
\begin{equation} \label{eq12}
rU = z + \theta q(\theta)(W-U)
\end{equation}

\begin{equation} \label{eq13}
rW = w + \lambda(U-W)
\end{equation}

\subsection{Negociación}
\begin{equation} \label{eq14}
w_i = argmax (W_{i} - U)^{\beta} (I_{i}-V)^{1-\beta}
\end{equation}

A partir de la negociación a la Nash surge la última ecuación clave del modelo, la ecuación \eqref{eq15} del salario agregado en equilibrio. Esta remplaza la curva de oferta laboral de los modelos walrasianos. Y vale remarcar que este modelo es fija (linea vertical), ya que, la fuerza laboral es constante, es decir, los trabajadores buscan vacantes con una intensidad constante, y trabajan un número de horas fijas cuando están ocupados. En el plano ($\theta$, w) la curva tiene pendiente positiva. 

\begin{equation} \label{eq15}
w = (1 - \beta)z + \beta p(1 + c\theta)
\end{equation}

\subsection{Definición del Equilibrio}
El equilibrio del modelo es una asignación de (u, $\theta$, v) que satisface la condición de equilibrio de los flujos representando por la BC \eqref{eq7}, la condición de creación de trabajo (JC) ecuación \eqref{eq11} y la ecuación de salario \eqref{eq15}. El equilibrio se puede ver en la Figura \@ref(fig:curvasEQ)


<!-- \begin{figure}[h!] -->
<!-- 	\centering -->
<!-- 	{% -->
<!-- 		\includegraphics[width=0.4\textwidth]{JC_BC_(v,u).png}% -->
<!-- 		\label{fig:a}% -->
<!-- 	}% -->
<!-- 	\hfill% -->
<!-- 	{% -->
<!-- 		\includegraphics[width=0.4\textwidth]{WC_JC_(w,theta).png}% -->
<!-- 		\label{fig:b}% -->
<!-- 	}% -->
<!-- 	\caption{Equilibrio del mercado laboral} -->
<!-- \end{figure} -->

```{r curvasEQ, fig.cap="Equilibrio del mercado laboral", fig.height=3, fig.width=8}
knit_hooks$set(plot = function(x, options, .notas = notas, .fuentes = fuentes, .addFuentes = FALSE) {
  if(.addFuentes) {
  paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\textsc{Fuentes} -- ", .fuentes,
        "\n\\end{figure}\n",
        sep = '')
  } else {
      paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        # "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\end{figure}\n",
        sep = '')
  }
})
plot_notes <- knit_hooks$get("plot")
curvas <- data.table(v = seq(1, 25, 1),
                     u = seq(1, 25, 1))

create_curves <- function(title_curva1, title_curva2, theta, x_axis, y_axis) {
ggplot(curvas, aes(x = u, y = v)) +
    geom_line() +
    geom_line(aes(x = (1:25) + 1, y = 20/(u))) +
    scale_x_continuous(limits = c(0,20)) +
    scale_y_continuous(limits = c(0,20)) + 
    geom_text(aes(label = title_curva1, y = .5, x = 12)) +
    geom_text(aes(label = title_curva2, y = 20, x = 12)) +
    {if(theta) {
        annotate('text', x = 2, y = 1, 
                 label = "theta",parse = TRUE,size=5)  
    }} +
    labs(x = x_axis, y = y_axis) +
    # geom_segment(aes(x=0, xend = 20 , y=0, yend = 0), size=1.5,
    #              arrow = arrow(length = unit(0.6,"cm"))) +
    theme(
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour="black"), 
        axis.ticks = element_blank())
}
p1 <- create_curves(title_curva1 = "Curva de Beveridge", title_curva2 = "Creación de trabajo", 
              theta = TRUE, x_axis = "u", y_axis = "v")
p2 <- create_curves(title_curva1 = "Creación de trabajo", title_curva2 = "Curva salarial", 
              theta = FALSE, y_axis = "w", x_axis = expression(theta))
p1 + p2
```


```{r}
knit_hooks$set(plot = function(x, options, .notas = notas, .fuentes = fuentes, .addFuentes = TRUE) {
  if(.addFuentes) {
  paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\textsc{Fuentes} -- ", .fuentes,
        "\n\\end{figure}\n",
        sep = '')
  } else {
      paste("\n\n\\begin{figure}\n",
        "\\includegraphics[width=\\maxwidth]{",
        opts_knit$get("base.url"), paste(x, collapse = "."),
        "}\n",
        "\\caption{",options$fig.cap,"}","\\label{fig:",opts_current$get("label"),"}","\\textsc{}\n",
        "\n\\footnotesize\\textsc{Notas} -- ",.notas,"\n",
        "\n\\end{figure}\n",
        sep = '')
  }
})
```

<!-- , eje tasa de desempleo (u) y tasa de vacantes (v). Pendiente JC es estrechez del mercado laboral $\theta$. Equilibrio del modelo en la intersección de la curva de salario y curva de creación de puestos. -->

<!-- # ```{r fig.cap="Equilibrio del mercado laboral", out.width='30%', fig.col = 2} -->
<!-- # knit_hooks$set(plot = plot_notes) -->
<!-- # par(mfrow = c(1,2)) -->
<!-- # notas = "Curva de Beveridge y Curva de Creación laboral, eje tasa de desempleo (u) y tasa de vacantes (v). Pendiente JC es estrechez del mercado laboral $\theta$. Equilibrio del modelo en la intersección de la curva de salario y curva de creación de puestos." -->
<!-- # img <- list.files(path = here::here("tesis"), pattern = ".png", full.names = TRUE) -->
<!-- # include_graphics(img) -->
<!-- # # knitr::include_graphics(here::here("tesis", "JC_BC_(v,u).png")) -->
<!-- # # knitr::include_graphics(here::here("tesis", "WC_JC_(w,theta).png")) -->
<!-- # ``` -->




<!-- **Capítulo \@ref(ref-Metodologia):** -->
\newpage
## TVP-VAR

Para un detalle de la metodología TVP-VAR con volatilidad estocástica ver @Primiceri2005, @Cogley2005, @Nakajima2011, @Primiceri2015 y @Lubik2016b.

A continuación se específica el algoritmo utilizado por @Primiceri2015 e implementado por @Kruger2015 en el paquete bvarsv en R.

- Algoritmo, resumen en base a @Kruger2015:

Usando $B^T= \{B_t\}_{t=1}^T$; $A^T= \{A_t\}_{t=1}^T$; $\Sigma^T= \{\Sigma_t\}_{t=1}^T$. Sea $\theta = [B^T,  A^T, V]$ y sea $V = [Q, S, W]$ una colección de las matrices de varianzas y covarianzas (VCV) de los shocks iid $\{\nu_t,\zeta_t, \eta_t\}$.

1. Inicializar $A^T, \Sigma^T, s^T, V$
2. Muestrear $B^T$ de $p(B^T|\theta^{-B^T}, \Sigma^T)$, usando el algoritmo de @KarterKohn1994 (CK)
3. Muestrear Q de $p(Q|B^T)$, que se distribuye $\mathcal{IW}$.
4. Muestrear $A^T$ de $p(A^T|\theta^{-A^T}, \Sigma^T)$, usando CK
5. Muestrear S de $p(S|\theta^S, \Sigma^T)$
6. Muestrear las variables discretas auxiliares $s^T$ de $p(s^T|\Sigma^T, \theta)$ usando el algoritmo de @Kim1998.
7. Extraer $\Sigma^T$ de $p(\Sigma^T|\theta, s^T)$ usando CK
8. Muestrear W desde $p(W|\Sigma^T)$
9. Volver a la etapa 2.

(ref:Primiceri2005) @Primiceri2005
(ref:Kruger2015) @Kruger2015

Por último se muestra la especificación de los priors utilizados en la estimación del modelo. Se utilizan los valores por defecto generados por @Primiceri2005.

```{r priorsTVP}
priors <- data.table(params  = c("$B_0$", "$A_0$", "$log \\space \\sigma_0$", "$Q$", "$W$", "$S_j,\\space j=1,...n-1$"),
                     desc    = c("Betas iniciales", "Covarianza inicial", "log volatilidad inivial", "$VCV$ de shocks en $B_t$", "$VCV$ de shocks en $log\\space \\sigma_t$", "VCV de shocks en $A_t$"),
                     f_prior = c("$\\mathcal{N}(\\hat{B}_{MCO}, k_B \\times\\hat{V}(\\hat{B}_{MCO}))$", "$\\mathcal{N}(\\hat{A}_{MCO}, k_A \\times \\hat{V}(\\hat{A}_{MCO}))$", "$\\mathcal{N}(log \\space \\sigma_{MCO}, k_{\\sigma} \\times \\mathbb{I}_n$", "$\\mathcal{IW}(k^2_Q \\times pQ \\times \\hat{V}(\\hat{B}_{MCO}, \\space pQ)$", "$\\mathcal{IW}(k^2_W \\times pW \\times \\mathbb{I}_n, \\space pW)$", "$\\mathcal{IW}(k^2_S\\times pS_j\\times\\hat{V}(\\hat{A}_{j,MCO}), pS_j)$"),
                     coef    = c("$k_B = 4$", "$k_A = 4$", "$k_{\\sigma} = 1$", "$k_Q = 0.01, \\ pQ = 40$", "$k_W=0.01, pW=n+1$", "$k_S = 0.01\\space,pS_j=j+1$"))

texto = "\\\\footnotesize Resúmen con las priors utilizadas en el TVP-VAR por (ref:Primiceri2005) siguiendo a (ref:Kruger2015). $\\\\mathcal{IW}$ y $\\\\mathcal{N}$ refieren a las distribuciones inversa de Wishart y Normal. $\\\\hat{A}_{MCO}\\\\space, \\\\hat{V}(\\\\hat{A}_{MCO})\\\\space,\\\\hat{B}_{MCO}\\\\space, \\\\hat{V}(\\\\hat{B}_{MCO})$ se obtienen entrenando una muestra via mínimos cuadrados ordinarios (MCO)."
kableExtra::kable(priors, digits = 2, row.names = F, align = "c", caption = "Distribuciones a priori", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = c("Parámetros", "Descripción", "Familia de priors", "Coeficientes")
                  ) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)
```

\newpage

## Quiebres estructurales

Esta sección toma como base el trabajo de @Zeileis2002.

Se asume regresores no estocásticos que convergen a una matriz finita (Q), y $\Vert x_i\Vert = O(1)$:
\begin{equation}
\frac{1}{n}\sum_{i=1}^nx_ix_i^T \to Q
\end{equation}
^[Si bien son condiciones estrictas que no permiten trabajar con procesos con tendencia o que sean dinámicos, pueden ser levantadas. Por ejemplo @Hansen1992, plantea trabajar con series I(1), mientras @Society1988 plantean que los test CUSUM mantienen sus niveles de significancia asintótica en modelos dinámicos.]

Los residuos MCO son $\hat{u_i} = y_i - x_i^T\hat{\beta}$, la varianza estimada $\hat{\sigma}^2 = \frac{1}{n-k}\sum_i^n\hat{u}_i^2$.
Los residuos recursivos son:

\begin{equation}
\tilde{u}_i = \frac{y_i-x_i^T\hat{\beta}^{(i-1)}}{\sqrt{1+x_i^T(X^{i-1^T}X^{i-1})^{-1}x_i}} \space\space\space\space\space\space i = k+1, ... n
\end{equation}

Donde $\hat{\beta}^{i-1}$ denota todas las observaciones hasta la observación $i-1$, lo mismo para $X^i$. La varianza estimada es $\tilde{\sigma}^2 = \frac{1}{n-k}\sum_{i=k+1}^n(\tilde{u}_i-\bar{\tilde{u_i}})^2$.
A continuación detallamos los procesos de fluctuación utilizados:
Los procesos CUSUM contienen la suma acumulada de residuos estandarizados, @Brown1975 escribieron el programa TIMVAR donde recomienda la utilización de los residuos recursivos por sobre la suma acumulada de residuos o residuos al cuadrado:

\begin{equation}
W_n(t) = \frac{1}{\tilde\sigma\sqrt \eta}\sum_{i=k+1}^{k+\lfloor t\eta\rfloor}\tilde u_i \space\space\space\space\space 0 \leq t \leq1
\end{equation}

Con $\eta = n-k$ es el número de residuos recursivos y $\lfloor t\eta\rfloor$ la parte entera de $t\eta$. Bajo la hipótesis nula, el proceso límite, sobre el cual se calculan los límites, del proceso empírico $W_n(t)$ is un proceso de Wiener o un proceso de movimiento Browniano estándar\footnote{Se mantiene el teorema del límite central funcional $W_n \Rightarrow W$ con $n \to \infty$ donde $\Rightarrow$ refiere a convergencia débil de las medidas de probabilidad asociadas.}. Bajo la hipótesis alternativa si existe un único cambio estructural en $t_0$ los residuos recursivos van a tener media cero hasta $t_0$. Y para un $t \geq t_0$ moverse alejados de su media^[Los test CUSUM mantienen sus propiedades en modelos dinámicos @Brown1975 prueban modelos dinámicos y @Society1988 prueban (Teorema I) que pese a que los residuos recursivos no son normales ni independientes @Ploberger1989 en un modelo dinámico, esto no importa asintóticamente puesto que las propiedades se mantienen].

@Ploberger1989 recomienda la utilización de la suma acumulada de residuos MCO:
\begin{equation}
W_n^0(t) = \frac{1}{\hat{\sigma}\sqrt n}\sum_{i=1}^{\lfloor nt\rfloor}\hat{u_i}\space\space\space\space\space 0 \leq t \leq 1
\end{equation}
Donde el proceso límite es un proceso de puente Browniano estándar $W^0(t) = W(t) - tW(1)$, en $t_0$ vale 0 y retorna a 0 en $t = 1$. Si existiera un cambio estructural (único) el trayecto debería tener un salto en torno a $t_0$.

Los procesos MOSUM, refieren a la suma móvil de residuos, por tanto, el proceso empírico contiene la suma de un número fijo de residuos en una ventana temporal que se mueve a lo largo de todo el periodo y cuyo largo queda determinado por un parámetro de ancho de banda, $h \in(0,1)$. Al igual que en el caso anterior tenemos dos casos, recursivo y MCO. Los residuos recursivos MOSUM son:

\begin{align}
M_t(t|h) &= \frac{1}{\tilde\sigma\sqrt n}\sum_{i=k+\lfloor N_{\eta}t\rfloor + 1}^{k + \lfloor N_{\eta}t\rfloor + \lfloor \eta h\rfloor}\tilde{u_i} \space\space\space\space\space 0\leq t\leq1-h \\
&= W_n\frac{\lfloor N_{\eta}t\rfloor+\lfloor\eta h\rfloor}{\eta}-W_n\frac{\lfloor N_{\eta}t\rfloor}{\eta}
\end{align}

Con $N_{\eta} = (\eta - \lfloor\eta h\rfloor/(1-h)$. Mientras los residuos MCO del proceso MOSUM son:

\begin{align}
M_n^0(t|h) &= \frac{1}{\hat{\sigma}\sqrt n}\sum_{i=\lfloor N_{n}t\rfloor + 1}^{\lfloor N_{n}t \rfloor + \lfloor nh\rfloor}\hat{u_i} \space\space\space\space\space\space 0 \leq t \leq 1-h \\
&= W_n^0\frac{\lfloor N_nt\rfloor + \lfloor nh \rfloor}{n} - \frac{\lfloor N_nt\rfloor}{n}
\end{align}
Donde $N_n = (n - \lfloor nh\rfloor)/(1-h)$. Si, bajo la hipótesis nula, asumimos sucede un único quiebre estructural en $t_0$, los trayectos tanto de MOSUM-MCO como MOSUM-recursivo deberían tener un quiebre en torno a $t_0$.

Los límites o frontera de las fluctuaciones de los procesos empíricos (efp) unidimensionales basados en residuos se hacen con respecto a un límite $b(t)$ y su contraparte $-b(t)$, el cual el proceso límite cruza con probabilidad $\alpha$. Sea cruzando $b(t)$ o $-b(t)$ para cualquier momento t se concluye que las fluctuaciones es improbablemente grande y la hipótesis nula puede ser rechaza a un nivel de significación $\alpha$.

Los límites de los procesos MOSUM son constantes $b(t) = \lambda$, en el caso del proceso recursivo CUSUM son $b(t) = \lambda(1 + 2t)$, y para CUSUM MCO es $b(t) = \lambda$\footnote{En el caso del proceso MOSUM al ser estacionario el proceso límite, tiene sentido que $b(t) = \lambda$. En el caso de los procesos CUSUM los procesos límite no son estacionarios, el movimiento Browniano y de puente Browniano. La elección de los límites se debe a su solución cerrada para las probabilidades de exceder el límite.}.

Por último, dentro de los procesos de fluctuación empíricos tenemos procesos basados en estimadores. En lugar de definir los procesos de fluctuación de acuerdo a los residuos, se definen en base a los parámetros estimados de los regresores, parámetros poblaciones. Los dos procesos siguientes son k-variados.
La estimación recursiva sigue a @Ploberger1989:

\begin{equation}
Y_n(t) = \frac{\sqrt i}{\hat{\sigma}\sqrt n}(X^{(i)^T} X^{i})^{\frac{1}{2}}(\hat{\beta}^{(i)} - \hat{\beta}^{(n)})
\end{equation}

Con $i = \lfloor k + t(n-k)\rfloor$ y $t \in [0,1]$. Por último la estimación MCO, denotado procesos de estimaciones móviles (ME) es:

\begin{equation}
Z_n(t|h) = \frac{\sqrt{\lfloor nh\rfloor} }{\hat{\sigma}\sqrt{n}}(X^{(\lfloor nt \rfloor , \lfloor nh\rfloor)^T}X^{(\lfloor nt \rfloor , \lfloor nh\rfloor)})^{\frac{1}{2}}(\hat{\beta}^{(\lfloor nt \rfloor , \lfloor nh\rfloor)}-\hat{\beta}^{(n)}) \space\space\space 0 \leq t \leq 1-h
\end{equation}

En ambos casos el proceso límite es un proceso de puente Browniano k-dimensional.
Bajo la hipótesis alternativa de único quiebre, el estimador recursivo debería tener un pico, mientras el estimador de movimiento debería tener un quiebre en torno al punto $t_0$.

El límite de los efp en este caso, esta dado por $\Vert efp_i(t) \Vert$, donde $\Vert . \Vert$ denota un funcional que es aplicado componente a componente. Se trabaja con los funcionales 'máximo' y 'rango'. Por tanto, la hipótesis nula es rechazada si $\Vert efp_i \Vert$ es mayor que una constante $\lambda$ la cual depende del nivel de confianza escogido, $\alpha$, para cualquier $i = 1,...k$.

Por último, se trabaja con dos estadísticos para poner a prueba la hipótesis nula. El estadístico $S_r$ se utilizada para los procesos basados en residuos, mientras el estadístico $S_e$ se utiliza para los procesos basados en estimaciones:

\begin{align}
S_r &= \max_t\frac{efp(t)}{f(t)}, \\
S_e &= \max \Vert efp(t) \Vert
\end{align}

Con f(t) dependiendo de la forma del límite, $b(t) = \lambda f(t)$. De donde provienen los distintos cálculos de los p-valores para cada test puede consultarse la sección A en @Zeileis2002.

Los estadísticos F difieren de los test anteriores en que se especifica la hipótesis nula a contrastar, se define una hipótesis alternativa de un quiebre en un momento particular.

\begin{equation}
\beta_i = \begin{cases} 
\beta_A &(1 \leq i \leq i_0) \\
\beta_B &(i_0 < i \leq n)
\end{cases}
\end{equation}

Con $i_0$ es algún punto en el intervalo $(k, n-k)$. El test original de @Chow1960 necesita que se especifique el momento del quiebre en particular en la hipótesis alternativa, o sea, debe ser conocido. Su planteamiento es realizar dos regresiones, una restringida y otra sin restringir.Se ajustan dos regresiones para cada submuestra definida por $i_0$ y se rechaza $H_0$ cuando:

$$
F_{i_0} = \frac{\hat{u}^T\hat{u}-\hat{e}^T\hat{e}} {\hat{e}^T\hat{e}/(n-2k)}
$$
el estadístico sobrepasa cierto nivel de tabla. Donde $\hat{e}=(\hat{u}_A,\hat{u}_B)$ son los residuos del modelo completo sin restringir. Para examinar la igualdad entre los conjuntos de coeficientes in dos regresiones lineal, se obtienen la suma cuadrado de los residuos asumiendo igualdad (bajo $H_0$) y la suma de los cuadrados sin asumir igualdad. El ratio de la diferencia entre las dos sumas y la última suma, ajustado por los correspondientes grados de libertad se distribuye como el estadístico F bajo $H_0$ @Chow1960^[Los resultados de @Chow1960 se pueden resumir en sus ecuaciones 50 y 51 y, son generalizables al caso de más de dos regresiones.]. Específicamente, $F_{i_0} \sim \chi^2_k$ y $F_{i_0}/k \sim \chi^2_{k, n-2k}$. La desventaja del planteamiento de Chow, es que el punto de quiebre debe ser conocido a priori, sin embargo, dicha limitación puede ser levantada. Es posible plantear un test F para todos los potenciales puntos de quiebre en casi toda la muestra o en un intervalo de la misma, cumpliendo que $k < \underline i \leq \overline i \leq n-k$. Rechazando $H_0$ si cualquier estadístico, $F_i$ sobrepasa los valores de tabla. Por ejemplo, si pensamos que existe una cambio estructural entre 1990 y 2010 que genero una alteración en los parámetros del modelo, podemos definir dicho intervalo y correr test F de forma iterativa, buscando algún quiebre en cualquiera de dichos años. El beneficio es mayúsculo, no necesitamos asumir un punto y obtenemos donde se genera el quiebre. Sin embargo, seguimos obteniendo solamente un quiebre, pero dicho problema se puede resolver utilizando un algoritmo que minimice una función objetivo y basado en el principio de optimalidad de Bellman encuentre una partición óptima @BaiPerron1998, @BaiPerron2003, @Zeileis2010.

Al igual que los efp, es posible plantear límites para el estadístico F. Asumiendo $H_0$, los límites se pueden calcular de tal forma que la probabilidad asintótica de alguna forma de agregación de los distintos estadísticos F calculados sobre los intervalos considerados $\underline i \leq i \leq \bar{i}$, superen dicho umbral con una significación $\alpha$. @Andrews1993, @Andrews1994 plantean tres funcionales a utilizar: el supremo, la media o exponencial, con lo cual plantean tres opciones para poner a prueba la hipótesis nula:

\begin{align}
supF &= \sup_{\underline i \leq i \leq \bar{i}} F_i \\
aveF &= \frac{1}{\bar{i} - \underline i + 1}\sum_{i = \underline i}^{\bar{i}}F_i \\
expF &= log \left( \frac{1}{\bar{i}-\underline i + 1}\sum_{i = \underline i}^{\bar{i}}exp(0.5\times F_i) \right)
\end{align}

Si sucede que el funcional de los estadísticos F cruza dicho umbral, entonces existe evidencia de un cambio estructural con una significación $\alpha$. Sin embargo, el problema con los test F, es que nos dan información de un solo quiebre en los parámetros, cuando podrían existir varios. @BaiPerron1998 plantean el problema, solución teórica y propiedades dentro de un marco general que engloba a un modelo estructural completo y parcial\footnote{La diferencia entre un modelo estructural completo es que todos los parámetros pueden tener quiebres en la muestra, mientras un modelo estructural parcial permite a un subconjunto de los parámetros tener quiebres, mientras el resto son estimados con la muestra completa asumiéndolos invariables}, @BaiPerron2003 implementan dicha solución utilizando la ecuación de Bellman de programación dinámica y la función objetivo de MCO, mientras @Zeileis2010 amplía la solución para cuasi-máxima verosimilitud y una función objetivo de minimización de la log-verosimilitud negativa.

Siguiendo a @BaiPerron2003, se considera el siguiente modelo matricial con m quiebres y m+1 regímenes^[Si $p = 0$ estamos frente a un modelo estructural puro, donde todos los coeficientes pueden variar. La varianza de $u_i$ no es necesario que sea constante, de hecho puede cambiar en el mismo momento en que cambian los parámetros y mejorar la precisión de los quiebres en los estimadores, sin embargo, @BaiPerron2003 lo tratan como un parámetro molesto 'nuisance parameter'.]

\begin{equation}
Y = X\beta + \bar{Z}\delta + U
\end{equation}
donde $Y = (y_1, ...y_T)'$, $X = (x1, ... x_T)'$, $U = (u_1, ...u_T)'$, $\delta = (\delta_1',...\delta_{m+1}')'$ y $\bar{Z}$ es la matriz diagonal con particiones de $Z$ en $(T_1, ...T_m)$, es decir, $\bar{Z} = diag(Z_1, ... Z_{m+1})$ con $Z_i = (z_{T_{i-1}}, ..., z_{T_{i}})'$. Los puntos de quiebre $(T_1, ...T_m)$, son tratados como desconocidos. Se busca estimar los coeficientes conjuntamente con los quiebres.

Los valores de los parámetros verdaderos se denotan con 0. Es decir, $\delta^0 = (\delta_{1}'^0,...\delta_{m+1}'^0)'$ y $(T_1^0, ...T_m^0)$ denotan los verdaderos valores de los parámetros y de los quiebres. La matriz $\bar{Z}^0$ es la que particiona diagonalmente $Z$ en $(T_1^0, ...T_m^0)$. Por lo cual, el proceso generador de datos se asume:

\begin{equation}
Y = X\beta^0 + \bar{Z}^0\delta^0 + U
\end{equation}

Usando el método de estimación es MCO. Para cada m-partición $(T_1, ...T_m)$ los estimadores MCO asociados de $\beta$ y $\delta_j$ son obtenidos mediante la minimización de la suma cuadrado de los residuos (RSS):

$$
(Y - X\beta - \bar{Z}\delta)'(Y - X\beta - \bar{Z}\delta) = \sum_{i=1}^{m+1}\sum_{t = T_{i-1}+1}^{T_i}[y_t-x_t'\beta-z_t'\delta_i]^2
$$
Siendo $\hat{\beta}(\{T_j\})$ y $\hat{\delta}(\{T_j\})$ las estimaciones en cada m-partición $(T_1, .. T_m)$ denotada $\{T_j\}$. Substituyendo estos últimos en la función objetivo y denotando RSS como $S_T(T_1, ... T_m)$ los puntos de quiebre estimados $(\hat{T}_1, ..., \hat{T}_m)$ son tal que:

$$
(\hat{T}_1, ..., \hat{T}_m) = arg\min_{T_1,...T_m}S_T(T_1, ... T_m)
$$
La minimización se realiza sobre todas las particiones $(T_1, ...T_m)$ de forma tal que $T_i - T_{i-1} \geq q$. Por lo tanto, los estimadores de punto de quiebre son minimizadores globales de la función objetivo\footnote{Notar que se puede elegir cualquier función objetivo a minimizar}. Las estimaciones de los parámetros de regresión, son las estimaciones asociadas con cada m-partición $\{{\hat{T}_j}\}$, es decir, $\hat{\beta} = \hat{\beta}({\hat{\{T_j\}}})$, $\hat{\delta} = \hat{\delta}({\hat{\{T_j\}}})$. Ya que los puntos de quiebre son parámetros discretos y pueden tomar únicamente un número finito de valores, se pueden estimar mediante una grilla de valores (grid search), sin embargo dicho algoritmo tiene complejidad $O(T^m)$.

Para poder computar dichos estimadores, @BaiPerron2003 utilizan el principio de optimalidad de Bellman o principio de programación dinámica cuya complejidad es $O(T^2)$ independientemente de la cantidad particiones que se realicen. Una vez que los RSS de los segmentos relevantes^[Los segmentos relevantes refieren a los segmentos plausibles de ser estimados, ver sección 3.1 @BaiPerron2003] han sido calculados, se utiliza el enfoque de programación dinámica para evaluar que partición logra una minimización global sobre RSS. 

Sea $RSS(\{T_{r,n}\})$ la suma de cuadrados de residuos asociada con la partición optima conteniendo $r$ quiebres usando las primeras n observaciones. La partición óptima resuelve el siguiente problema recursivo:

$$
RSS(\{T_{m,T}\}) = \min_{mh\leq j\leq T-h}[RSS(\{T_{m-1,j}\}) + RSS(\{j+1, T\})]
$$

Se evalúa primero, el primer quiebre óptimo para todas las submuestras desde $h$ hasta $T-mh$. Se guardan un conjunto de $T-(m+1)h+1$ particiones óptimas y sus RSS, donde cada partición corresponde a una submuestra terminando en $2h$ hasta $T-(m-1)h$.
Segundo, se buscan las particiones óptimas con dos quiebres, las cuales terminan en el periodo $3h$ hasta $T-(m-2)h$. Para cada uno de estas posibles fechas de término, se busca en que partición de un quiebre guardada previamente puede ser insertada para obtener un RSS mínimo. Se devuelve un conjunto de $T-(m+1)h+1$ con dos quiebres óptimos. El algoritmo continua de forma secuencial hasta que el conjunto $T-(m+1)h+1$ de $(m-1)$ particiones óptimas se obtiene con finalización desde $(m-1)h$ hasta $T-2h$. Por último, se busca cual de esas $(m-1)$ particiones óptimas genera un mínimo global en RSS cuando se combina con un segmento adicional.

Por último, @Zeileis2010 extiende el trabajo de @BaiPerron2003, para trabajar con modelos de tipo de cambio, como @Frankel1994 en los cuales la varianza del error $\sigma^2$es de crucial interés. Esto lleva a la inclusión del error de la varianza como un regresor adicional en vez de un parámetro molesto y, la estimación del modelo por máxima verosimilitud o cuasi-máxima verosimilitud, en lugar de MCO. La inclusión de $\sigma^2$ no debe ser visto como relevante solo para modelos de tipo cambio, como nota @BaiPerron2003 su inclusión puede mejorar la estimación de los quiebres estructurales.

El modelo planteado es cuasi-normal y tiene densidad:
$$
f(y|x,\beta, \sigma^2) = \phi((y-x^T\beta)/\sigma)/\sigma
$$
Donde $\phi(.)$ es la función de densidad de una normal estándar. Con $\theta = (\beta^T, \sigma^2)^T$ de largo $k = c +2$, siendo c la cantidad de regresores, más intercepto y varianza.

El algoritmo para encontrar quiebres es exactamente el mismo que @BaiPerron2003, con la diferencia que en vez de usar estimaciones MCO se usan estimaciones QML y la función objetivo $RSS$ se cambia por la log-verosimilitud negativa $-logf(y_i|x_i, \theta)$

\newpage
## Datos 
<!-- **Capítulo \@ref(ref-Datos):** -->

```{r ga13-18-comparacion, fig.align="center", fig.cap="Avisos diario El País"}
notas = "Series de avisos laborales de \\textit{gallito} entre 2013 y 2018 con frecuencia trimestral. Las series refieren a los avisos publicados filtrados por avisos repetidos y a la cantidad de publicaciones sin filtrar. Se observa una diferencia de nivel relativamente estable y la misma dinámica."
fuentes = "Avisos publicados en el portal web \\textit{Gallito}. Datos confidenciales facilitados por diario El País. Procesamiento propio."
dt[data.table::between(fecha, "2013-07-01", "2018-10-01"), 
   ggplot(.SD) +
     geom_line(aes(x = fecha, y = av_ga_c_dup), linetype = "dashed") +
     geom_line(aes(x = fecha, y = av_ga_s_dup)) +
     geom_point(aes(x = fecha, y = av_ga_c_dup)) +
     geom_point(aes(x = fecha, y = av_ga_s_dup)) +
     scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
     scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
     labs(y = "Avisos", x = "Fecha") +
     theme_Publication()
  ]
```

(ref:iecon) @Alma2011
(ref:ceres) @Ceres2012

```{r iecon-ceres, fig.cap="Comparación tasas de crecimiento", fig.align="center"}
notas = "Tasas de crecimiento de series de vacantes laborales anualizadas entre los años 2000 a 2009 correspondientes a (ref:iecon) y (ref:ceres). La línea punteada corresponde a los avisos recolectados en (ref:iecon) mientras la línea cortada corresponde a los puestos laborales. Los puntos corresponden a los datos transformados a partir de (ref:ceres). Se observa una elevada correlación lineal entre las tasas de crecimiento de avisos en el orden del 90\\% que corrobora que el Índice Ceres de Demanda Laboral(ICDL) esta construido a partir de publicaciones laborales de \\textit{Gallito}."
fuentes = "Los datos de (ref:iecon) han sido facilitado por los autores y la información de (ref:ceres) ha sido facilitada por el Centro de Estudios de la Realidad Económica y Social (CERES). Las series son de elaboración propia."
d <- data.table(fecha = seq.Date(from = as.Date("2001-01-01"), to = as.Date("2009-01-01"), by = "years"))
d[, ceres := ceres_ano$ind_vacantes[3:12] %>% log(.) %>% diff(.)
  ][, iecon_avisos  := iecon_ano$avisos %>% log(.) %>%  diff(.)
    ][, iecon_puestos := iecon_ano$puestos %>% log(.) %>%  diff(.)]
ggplot(d, aes(x = fecha)) +
  geom_point(aes(y = ceres)) +
  geom_line(aes(y = iecon_avisos), linetype = "dotted", color = "black") +
  geom_line(aes(y = iecon_puestos), linetype = "dashed", color = "black") +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  labs(x = "Fecha", y = "Tasas de crecimiento") +
  theme_Publication()
```

```{r avisos-dpto, fig.cap="Avisos recolectados mediante scraping", fig.align='center'}
# Hacer un facet con histogramas de la cantidad de avisos correspondientes a Montevideo.
av_comp <- readRDS(here::here("Datos", "Finales", "AvisosCompatibilizados.rds"))

notas = "Proporción de avisos laborales publicados en cada portal web laboral para cada departamento de Uruguay. Entre el 80\\%-95\\% de los avisos laborales corresponde al departamento de Montevideo, evidenciando que existe un sesgo pequeño de avisos laborales de otros departamentos."
fuentes = "Datos recolectados mediante scraping web de los portales laborales \\textit{Buscojobs}, \\textit{Computrabajo}, \\textit{Gallito} durante el último trimestre de 2018 y todo 2019."

dd <- av_comp[ano == 2019, .N, by = .(pagina, dpto)]
dd[, ord  := data.table::frank(.SD, N, ties.method = "first")]
dd[, prop := N/sum(N), by = pagina]
dd[, dpto := dplyr::case_when(
  dpto == "montevideo" ~ "Montevideo",
  dpto == "canelones" ~ "Canelones",
  dpto == "cerrolargo" ~ "Cerro Largo",
  dpto == "paysandu" ~ "Paysandú",
  dpto == "maldonado" ~ "Maldonado",
  dpto == "durazno" ~ "Durazno",
  dpto == "soriano" ~ "Soriano",
  dpto == "sanjose" ~ "San José",
  dpto == "colonia" ~ "Colonia",
  dpto == "rocha" ~ "Rocha",
  dpto == "lavalleja" ~ "Lavalleja",
  dpto == "florida" ~ "Florida",
  dpto == "rivera" ~ "Rivera",
  dpto == "salto" ~ "Salto",
  dpto == "otros" ~ "Otros",
  dpto == "rionegro" ~ "Río negro",
  dpto == "tacuarembo" ~ "Tacuarembó",
  dpto == "treintaytres" ~ "Treinta y Tres",
  dpto == "missing" ~ "Missing",
  dpto == "artigas" ~ "Artigas",
  dpto == "flores" ~ "Flores"
)]
dd[, pagina :=  dplyr::case_when(
  pagina == "buscojobs" ~ "Buscojobs",
  pagina == "gallito"   ~ "Gallito",
  pagina == "computrabajo" ~ "Computrabajo"
)]
ggplot(dd, aes(x = reorder(dpto, ord), y = prop)) +
           geom_bar(stat = "identity", fill = "black") +
           facet_wrap(~ pagina, scales = "free_x", drop = TRUE) +
           scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
           labs(y = "Avisos", x = "Departamento", title = " \n ") +
           # xlab(NULL) +
           # theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=.5), panel.background = element_blank())
           theme_Publication(angulo_y = 0, angulo_x = 90)
```

```{r gallito-13-18, fig.cap="Avisos laborales \\textit{Gallito} por nivel jerárquico"}
notas = "Proporción de avisos laborales en \\textit{Gallito} entre 2013-2018 por nivel jerárquico. Cerca de un 40\\% de los avisos corresponden a auxiliares, un 15\\% a técnico o especialista, un 10\\% a peón y cerca de un 2\\% a puestos de gerente. Procesamiento propio de los datos."
fuentes = "Datos del portal laboral \\textit{Gallito}, facilitados por el diario El País. Procesamiento propio."
ga <- readxl::read_excel(here::here("Datos", "Originales", "Gallito-2013-2018.xlsx"))
data.table::setDT(ga)
prop.table(table(ga[, "Nivel jerarquico"], deparse.level = 2, dnn = "Area")) %>% 
  as.data.frame(., responseName = "Avisos") %>%
  ggplot(., aes( x = reorder(Area, Avisos), y = Avisos)) +
  geom_bar(stat = "identity", fill= "black") +
  coord_flip() +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme(axis.text.y = element_text(size = 7), panel.background = element_blank()) +
  labs(x = "Nivel jerárquico", y = "Proporción de avisos", title = " ")
```

\newpage

## Resultados

### Índice de vacantes, Buscojobs y Computrabajo

```{r serie-buscojobs, fig.cap="Serie trimestral \\textit{Buscojobs}"}
notas = "Serie trimestral de avisos laborales publicados en el portal \\textit{Buscojobs}, construcción y elaboración propia"
fuentes = "Los datos fueron obtenidos a través del portal \\textit{Waybackmachine}, scraping de \\textit{Buscojobs} y finalmente imputando los valores faltantes."
bj_ct <- readRDS(here::here("Datos", "Finales", "serie_trim_bj_ct.rds"))
bj_ct[fecha >= "2007-04-01", 
      ggplot(.SD, aes(x = fecha, y = av_bj_s_dup)) +
        geom_line() +
        geom_point() +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
        scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
        labs(y = "Avisos laborales", x = "Fecha") +
        theme_Publication(position_legend = "bottom")
]
```


```{r serie-computrabajo, fig.cap= "Serie trimestral \\textit{Computrabajo}"}
notas = "Serie trimestral de avisos laborales publicados en el portal \\textit{Computrabajo}, construcción y elaboración propia. Los datos de \\textit{Computrabajo} fueron corregidos en la medida que el portal laboral mantiene avisos por periodos mayores a dos meses. Si se compara la cantidad de avisos totales publicados la misma difiere fuertemente de la cantidad de avisos publicados en los últimos treinta dias, de los datos recabados estos últimos representaban un 0.40 de la cantidad de avisos totales. Adicionalmente la serie fue imputada."
fuentes = "Los datos fueron obtenidos a través del portal \\textit{Waybackmachine}, scraping de \\textit{Computrabajo} y finalmente imputando los valores faltantes."
bj_ct[, 
      ggplot(.SD, aes(x = fecha, y = av_ct_s_dup)) +
        geom_line() +
        geom_point() +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
        scale_x_date(date_breaks = "1 year", date_labels = "%y") +
        labs(y = "Avisos laborales", x = "Fecha") +
        theme_Publication()
]
```


```{r IndiceAvisosTC, fig.cap = "Índice de avisos en Tendencia-Ciclo"}

notas = "Serie trimestral del indicador de cantidad de avisos laborales y su tendencia-ciclo (TC). Serie de construcción propia a partir de los datos recabados de los portales laborales \\textit{Gallito}, \\textit{Computrabajo} y \\textit{Buscojobs}. La tendencia-ciclo fue obtenida con el método de X-11 mediante el paquete seasonal."

fuentes = "Los fuentes de información son \\textit{Gallito}, \\textit{Computrabajo} y \\textit{Buscojobs}. Las tres fuentes de datos son construidas en este trabajo."

dt[, ggplot(.SD, aes(x = fecha)) + 
       geom_line(aes(y = av_final)) + 
       geom_line(aes(y = av_final_tc), color = "red", alpha = 1, size = .4) +
       scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
       scale_x_date(date_breaks = "10 year", date_labels = "%Y") +
       # geom_point(aes(y = ind_av), color = "black", alpha = 1, size = .7) +
       geom_text(aes(label = "Indicador avisos", y = 4500, x = as.Date("2015-01-01")), color = "black") +
       geom_text(aes(label = "Indicador avisos TC", y = 3500, x = as.Date("2015-01-01")), color = "red") +
       labs(y = "Avisos", x = "Fecha") +
       theme_Publication()
   ]
```


### Caracterización de las series

Se han realizado distintos test de raíz unitaria regular (RU) y raíz unitaria estacional sobre la serie de vacantes, tasa de desempleo y sobre las series que componen el índice de vacantes. En el Cuadro \@ref(tab:test-ru-trim) del anexo, todas las series trimestrales utilizadas en el análisis son PGD que requieren una diferencia regular para ser estacionarios, a excepción del portal web _Buscojobs_^[Cuando se realizan los mismos test con _Buscojobs_ con frecuencia mensual, el test ADF y PP siguen planteando que el PGD no necesita diferencias regulares para ser estacionario. El test KPSS da como resultado la necesidad de una diferencia para que el proceso sea estacionario, ver Cuadro \@ref(tab:test-ru-mensual)].

Posteriormente analizamos si la tasa de desempleo y el índice de vacantes están cointegrados. Dado que las series se correlacionan negativamente se plantea la relación entre las vacantes y la inversa de la tasa de desempleo. En todos los casos no se rechaza la hipótesis nula de no cointegración (ver anexo).

```{r TestRU, include=FALSE, eval=FALSE}
# Series trimestrales
# readRDS(here::here("Datos", "Finales", "serie_trimestral_ga_13-19.rds"))
ga_ts <- ts(dt[, av_umcg], start = c(1980, 1), frequency = 4)
um_ts <- ts(dt[!is.na(av_urr_mol), av_urr_mol], start = c(1980, 1), frequency = 4)
ct_ts <- ts(dt[!is.na(av_ct_s_dup), av_ct_s_dup], start = c(2003, 3), frequency = 4)
bj_ts <- ts(dt[av_bj_s_dup > 0, av_bj_s_dup], start = c(2007, 2), frequency = 4)
final <- ts(dt[, av_final], start = c(1980, 1), frequency = 4)
final_tc <- ts(dt[, av_final_tc], start = c(1980, 1), frequency = 4)
pib   <- ts(dt[!is.na(pib), pib], start = c(1981, 1), frequency = 4) 
ind_vac   <- ts(dt[!is.na(ind_vac), ind_vac], start = c(1980, 1), frequency = 4)
desempleo <- ts(dt[!is.na(td), td], start = c(1981, 1), frequency = 4)
pea <- ts(dt[!is.na(pea), pea], start = c(1980, 1), frequency = 4)

for(serie in c("ga_ts", "ct_ts", "bj_ts", "um_ts", "final", "final_tc", "pib", "ind_vac", "desempleo", "pea")) {
    for(test in c("adf", "kpss", "pp")) {
        print(c(serie, test, forecast::ndiffs(get(serie), alpha = 0.05, test = test, max.d = 2, type = "level")))
    }
}

for(serie in c("ga_ts", "ct_ts", "bj_ts", "um_ts", "final", "final_tc", "pib", "ind_vac", "desempleo", "pea")) {
    for(test in c("hegy", "ocsb", "ch")) {
        print(c(serie, test, forecast::nsdiffs(get(serie), alpha = 0.05, test = test)))
    }
}

# Series mensuales
bj_ct_mensual <- readRDS(here::here("Datos", "Finales", "serie_mensual_bj_ct.rds"))
gallito_mensual <- readRDS(here::here("Datos", "Finales", "serie_mensual_ga_13-19.rds"))

ct_ts <- ts(bj_ct_mensual[, av_ct_s_dup], start = c(2003, 5), frequency = 12)
bj_ts <- ts(bj_ct_mensual[av_bj_s_dup > 0, av_bj_s_dup], start = c(2007, 6), frequency = 12)
ga_ts <- ts(gallito_mensual[, avisos_s_dup], start = c(2013, 7), frequency = 12)
# ceres_ts <- ts(dt[!is.na(av_ceres), av_ceres], start = c(1998, 4), frequency = 12)

for(serie in c("ga_ts", "ct_ts", "bj_ts")) {
    for(test in c("adf", "kpss", "pp")) {
        print(c(serie, test, forecast::ndiffs(get(serie), alpha = 0.05, test = test, max.d = 2, type = "level")))
    }
}

for(serie in c("ga_ts", "ct_ts", "bj_ts")) {
    for(test in c("hegy", "ocsb", "ch")) {
        print(c(serie, test, forecast::nsdiffs(get(serie), alpha = 0.05, test = test)))
    }
}
```

(ref:adf) @DickeyFuller1979
(ref:kpss) @KPSS1992
(ref:pp) @PhillipsPerron1988
(ref:hegy) @hegy1990
(ref:ocsb) @Osborn1988
(ref:ch) @Canova1995

```{r test-ru-trim, results='asis', fig.cap="Test RU regulares y estacionales"}
texto = "\\\\footnotesize Test de raíces unitarias regulares y estacionales. Los test de raíces unitarias regulares son: ADF es el test de Dickey-Fuller aumentado (ref:adf), el test KPSS corresponde a (ref:kpss) y el test PP refiere a (ref:pp). En la medida que las hipótesis nulas en los tres test no son iguales, la columna \\\\textit{Diferencias} refiere a la cantidad de diferencias regulares necesarias para que el proceso estócastico sea estacionario en covarianza. En todos los casos a excepción de \\\\textit{Buscojobs} los test coinciden en que es necesaria una diferencia regular. Los test de raíces unitarias estacionales son: HEGY corresponde a (ref:hegy), OCSB a (ref:ocsb) y CH al test de (ref:ch). A excepción de las series de UM (Urrestarazu-Molina), \\\\textit{Computrabajo} y \\\\textit{Buscojobs} donde los test difieren en sus conclusiones, los test coinciden en que no es necesaria una diferencia estacional para el resto de las series.

\\\\textit{Fuentes}: Serie \\\\textit{Gallito} de elaboración propia en base a publicaciones semanales del portal laboral \\\\textit{Gallito}. UM es la serie combinada entre Urrestarazu-Molina. \\\\textit{Computrabajo} serie de construcción propia en base a los avisos publicados en portal laboral \\\\textit{Computrabajo}. \\\\textit{Buscojobs} serie de construcción propia en base a los avisos recabados del portal \\\\textit{Buscojobs}. Serie Final hace referencia a la unión de las series de \\\\textit{Gallito}, \\\\textit{Buscojobs} y \\\\textit{Computrabajo}. La población económicamente activa se obtiene procesamiento la encuesta continua de hogares (ECH) compatibilizada por el instituo de economía (IECON). La tasa de desempleo se obtiene del Instituto Nacional de Estadística (INE). El producto interno bruto (PIB) es calculado por el Banco Central del Uruguay (BCU) y la serie fue facilitada por el Centro de Investigaciones Económicas (CINVE). Por último el índice de vacantes se construye en este trabajo."

test_ru <- data.table::data.table(serie = c("\\\\textit{Gallito} 80-19", "\\\\textit{UM} 80-01", "\\\\textit{Computrabajo} 03-19", "\\\\textit{Buscojobs}", "Serie Final", "PEA", "Tasa Desempleo", "PIB", "Índice de Vacantes"),
                      test_regular = rep("ADF-KPSS-PP", 9),
                      dif  = c(rep("1",3), "0", rep("1", 5)),
                      test_estacional = rep("HEGY-OCSB-CH", 9),
                      dif_est = c("0", "1-0-0", "1-0-0", "1-0-0", rep("0", 5)))
kableExtra::kable(test_ru, row.names = F, align = "c", caption = "Test de raíces unitarias series trimestrales", escape = FALSE, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = c("Serie", "Test RU regular", "Diferencias", "Test RU estacional", "Diferencias")
                  ) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)
```

El Cuadro \@ref(tab:test-ru-trim) columna 2 muestra los resultados de la aplicación de los test Dickey Fuller Aumentado (ADF) [@DickeyFuller1979], Kwiatkowski–Phillips–Schmidt–Shin (KPSS) [@KPSS1992] y Phillips-Perron (PP) [@PhillipsPerron1988]. El test ADF y PP usan la hipótesis nula que la serie tiene una raíz unitaria versus una hipótesis alternativa de una raíz estacionaria. En el test KPSS la hipótesis nula es que la serie tiene una raíz estacionaria contra una hipótesis alternativa de raíz unitaria. La columna diferencias refiere a la cantidad de diferencias regulares necesarias para que la realización del proceso estócastico se vuelva estacionario. El resultado es que la serie de vacantes es un proceso integrado I(1), al igual que dos de las tres series que la componen, la serie de avisos de _Gallito_ (años 80 a 2019) y la serie de _Computrabajo_ (2003 a 2019), mientras _Buscojobs_ resulta ser I(0). Tanto el PIB como la tasa de desempleo, resultan procesos integrados de orden uno.

La columna test estacional, muestra los test de raíz unitaria estacional realizados. El test de @hegy1990 (HEGY) pone aprueba la hipótesis nula de que las raíces del polinomio autoregresivo caen dentro del circulo unitario versus la alternativa que caen fuera. El test @Osborn1988 (OCSB) utiliza la hipótesis nula de raíz unitaria estacional versus la alternativa de estacionariedad. Mientras el test @Canova1995 (CH) plantea la hipótesis nula de la no existencia de raíz unitaria en las frecuencias estacionales versus la alternativa de raíz unitaria en una frecuencia estacional o en un conjunto de frecuencias estacionales. La última columna de la tabla refiere a la cantidad de diferencias estacionales necesarias para que el proceso se vuelva estacionario, los tres test llevan a las mismas conclusiones tanto para el PIB, tasa de desempleo e índice de vacantes, no son necesarias diferencias estacionales. Sin embargo, en el caso de la serie de _Buscojobs_ y _Computraajo_ los resultados difieren, para ambas el test HEGY plantea la existencia de raíz unitaria en frecuencias estacional mientras los test OCSB y CH la descartan.

Por último el Cuadro \@ref(tab:test-ru-mensual) realiza lo mismos test pero con un subconjunto de series de frecuencia mensual. Las series de _Gallito_ y _Computrabajo_, muestran resultados sin ambigüedades, son procesos I(1) mientras _Buscojobs_ es un proceso I(1) para KPSS pero I(0) para ADF y PP, notar que cuando la serie es trimestralizada los tres test coinciden. Los test de raíz unitaria estacional plantean que no es necesario realizar diferencias estacionales tanto para _Computrabajo_ como _Buscojobs_, por el contrario, la serie de _Gallito_ según HEGY y CH necesita una diferencia estacional, no así para OCSB. Dicho resultado difiere cuando la serie es trimestralizada, en donde los tres test coinciden en la no existencia de raíz unitaria estacional.

```{r test-ru-mensual, fig.cap="Test RU mensual", results='asis'}
texto = "\\\\footnotesize Test de raíces unitarias regulares y estacionales. Los test de raíces unitarias regulares son: ADF es el test de Dickey-Fuller aumentado (ref:adf), el test KPSS corresponde a (ref:kpss) y el test PP refiere a (ref:pp). En la medida que las hipótesis nulas en los tres test no son iguales, la columna \\\\textit{Diferencias} refiere a la cantidad de diferencias regulares necesarias para que el proceso estócastico sea estacionario en covarianza. En todos los casos a excepción de \\\\textit{Buscojobs} los test coinciden en que es necesaria una diferencia regular. Los test de raíces unitarias estacionales son: HEGY corresponde a (ref:hegy), OCSB a (ref:ocsb) y CH al test de (ref:ch). A excepción de la serie de \\\\textit{Gallito} donde los test difieren en sus conclusiones, los test coinciden en que no es necesaria una diferencia estacional para el resto de las series. 

\\\\textit{Fuentes}: \\\\textit{Gallito} se calculada de una base de datos proporcionada por el diario El País y de información recaba de \\\\textit{Gallito} mediante scraping web. Las series de \\\\textit{Computrabajo} y \\\\textit{Buscojobs} son obtenidas a partir los portales laborales \\\\textit{Buscojobs} y \\\\textit{Computrabajo}."

test_ru_mensual <- data.table(serie = c("Gallito 13-19", "Computrabajo 03-19", "Buscojobs 07-19"),
                              test_regular = rep("ADF-KPSS-PP", 3),
                              dif_reg = c("1", "1", "0-1-0"),
                              test_estacional = rep("HEGY-OCSB-CH", 3),
                              dif_est = c("1-0-1", "0-0-0", "0-0-0"))
kableExtra::kable(test_ru_mensual, row.names = F, 
                  align = "c", 
                  caption = "Test de RU mensuales", 
                  escape = FALSE, 
                  booktabs = TRUE, format = "latex", 
                  col.names = c("Serie", "Test RU regular", "Diferencias", "Test RU estacional", "Diferencias")
                  ) %>%
  kableExtra::kable_styling(
      # latex_options = "scale_down"
      ) %>%
  kableExtra::footnote(general = texto,
                       general_title = "Notas:",
                       threeparttable = TRUE,
                       escape = FALSE)
```

<!-- # ```{r vcovHAC-andrews, fig.cap='Ponderadores Andrews', results='asis'} -->
<!-- # # Pesos default, weightsAndrews -->
<!-- # print(xtable(get_model(.mod = mod_rf, .mat = sandwich::vcovHAC)), comment = FALSE, digits = 2) -->
<!-- # ``` -->

### Procesos de fluctuación empíricos {#efpAnexo}

```{r testCUSUM, fig.cap="Test CUSUM", out.width='1\\linewidth'}
library(strucchange)
reg  <- log(ind_vac) ~ log(td) + 1
# mod1 <- strucchange::efp(formula = reg, type = "Score-CUSUM", data = dt_ts[, 2:3], h = .15, dynamic = F)
# plot(mod1, functional = NULL)
# print(sctest(mod1)) # Quiebre.
test_plot <- function(test, .data = dt_ts[, 2:3], .formula = reg, .h = 0.15, .dynamic = FALSE, .test = FALSE, 
                      .main = "", .ylab = "Proceso fluctuación empírico") {
    mod1 <- strucchange::efp(formula = .formula, type = test, data = .data, h = .h, dynamic = .dynamic)
    # Boundaries
    if(test == "Score-CUSUM"| test == "Score-MOSUM") {
      colnames(mod1$process) <- c("Intercepto", "log(td)", "Varianza")
    } else if (test == "fluctuation" | test == "ME") {
      colnames(mod1$process) <- c("Intercepto", "log(td)")
    }
    return(plot(mod1, functional = NULL, main = .main, ylab = .ylab, xlab = "Fecha"))
    # print(plot(mod1, functional = NULL, xlab = "Fecha"))
    # boundary(mod1, alpha = 0.05)
    # Test
    if(.test) {
      print(sctest(mod1))  
    }
}
# 1981.1
# 1ro. CUSUM en base al paper del 74.
notas = "Procesos de fluctuación empíricos bajo el marco de test de fluctuación generalizados. Se visualizan el proceso empírico y su frontera (con color rojo). El proceso supera la frontera indicando la posible existencia de un quiebre estructural en la media incondicional."
fuentes = "Los datos utilizados son el índice de vacantes de construcción propia y la tasa de desempleo generada por el INE."
par(mfrow = c(1,2))
test_plot(test = "Rec-CUSUM", .main = "Test MOSUM recursivo", .ylab = "Proceso fluctuación empírica") # Quiebre. Test CUSUM recursivo
# 2do. CUSUM-OLS
test_plot(test = "OLS-CUSUM", .main = "Test CUSUM MCO") # Quiebre. Test CUSUM OLS
```

```{r TestRecursivos, fig.cap="Test score", out.width='.3\\linewidth'}
# Recursivos
notas = "Procesos de fluctuación empíricos bajo el marco de test de fluctuación generalizados. Se visualizan el proceso empírico y su frontera (con color rojo). El proceso supera la frontera indicando la posible existencia de un quiebre estructural en la media incondicional. Adicionalmente se gráfica el proceso empírico y frontera para la varianza, y la misma cruza los límites indicado posibles quiebres."
fuentes = "Los datos utilizados son el índice de vacantes de construcción propia y la tasa de desempleo generada por el INE."
par(mfrow = c(1,2))
test_plot(test = "Score-CUSUM", .main = "Test score COSUM") # Quiebre. Test CUSUM basado en score
test_plot(test = "Score-MOSUM", .main = "Test score MOSUM")                       # Quiebre
```

```{r testMOSUM, fig.cap="Test MOSUM", out.width='.3\\linewidth'}
par(mfrow = c(1,2))
# 3ro. MOSUM-Recursive
notas = "Procesos de fluctuación empíricos bajo el marco de test de fluctuación generalizados. Se visualizan el proceso empírico y su frontera (con color rojo). El proceso supera la frontera indicando la posible existencia de un quiebre estructural en la media incondicional."
test_plot(test = "Rec-MOSUM", .main = "Test MOSUM recursivo")                         # Quiebre.
# 4to. MOSUM-OLS
test_plot(test = "OLS-MOSUM", .main = "Test MOSUM MCO")                         # Quiebre.
```

```{r testME, fig.cap="test fluctuación ME", out.width='.3\\linewidth'}
par(mfrow = c(1,2))
# 5to. Recursive
test_plot(test = "fluctuation", .main = "")                       # Quiebre
# 6to. ME
# test_plot(test = "ME", .main = "Test ME")                                # Quiebre.
```

```{r testME2, fig.cap="test ME", out.width='.3\\linewidth'}
# 6to. ME
test_plot(test = "ME", .main = "")                                # Quiebre.

```

\newpage

### Test de quiebres estructurales

(ref:Zeileis2002) @Zeileis2002

```{r quiebres, results='asis', fig.cap="Test de quiebes estructurales", fig.align='center'}
# Data
dt_ts <- ts(data = dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
                      .(pib, ind_vac, td)], 
            start = c(1981, 1), frequency = 4)
# Desestacionalizo con método x13 y conFiguración default
td      <- seasonal::final(seasonal::seas(dt_ts[, "td"]))
ind_vac <- seasonal::final(seasonal::seas(dt_ts[, "ind_vac"]))
pib     <- seasonal::final(seasonal::seas(dt_ts[, "pib"]))
# tasa de crecimiento del pib ~ log(diff(pib))
delta_pib <- diff(
            ts(log(dt[fecha >= "1980-04-01", pib]), start = c(1980, 2), frequency = 4),
            lag = 1, differences = 1
            )
delta_pib <- window(delta_pib, start = c(1981, 1), end = c(2018, 4))
pib <- window(pib, start = c(1981, 1), end = c(2018, 4))
dt_ts <- ts.union(pib, ind_vac, td, delta_pib)

# Modelo
reg <- log(ind_vac) ~ log(td) + 1
rt <- function(test, .data = dt_ts[, 2:3], .formula = reg, .h = 0.15, .dynamic = FALSE, pval = TRUE) {
    # Modelo
    mod1 <- strucchange::efp(formula = .formula, type = test, data = .data, h = .h, 
                             dynamic = .dynamic, vcov = sandwich::kernHAC)
    # Test
    if(pval) {
        round(sctest(mod1)$p.value[[1]],2)
    } else {
        round(sctest(mod1)$statistic[[1]],2)
    }
}
ft <- function(test = "supF", .formula = reg, .data = dt_ts[, 2:3], .from = .15, .to = NULL, pval = F) {
    mod <- strucchange::Fstats(formula = .formula, data = .data, from = .from, to = .to,
                               vcov = sandwich::kernHAC)
    if(pval) {
        round(sctest(mod, type = test)$p.value[[1]], 2)
    } else {
        round(sctest(mod, type = test)$statistic[[1]], 2)
    }
}
test = c("Rec-CUSUM", "OLS-CUSUM", "Score-CUSUM", "Rec-CUSUM(d)", "OLS-CUSUM(d)" ,"Score-CUSUM(d)", "Rec-MOSUM", 
         "OLS-MOSUM", "Score-MOSUM", "fluctuation", "ME", "expF", "aveF", "supF")
mat = matrix(data = NA, nrow = NROW(test), ncol = 4)
colnames(mat) <- c("Test", "est", "p-valor", "resultado")
mat <- as.data.frame(mat)
i = 0
for(t in test) {
    i = i + 1
    mat[i , 1] <- t
        for(bool in c(FALSE, TRUE)) {
            if(bool) {
                if(t %in% c("Rec-CUSUM(d)", "OLS-CUSUM(d)" ,"Score-CUSUM(d)")) {
                    mat[i, "p-valor"] <- rt(test = gsub(t, pattern = "\\(d\\)", replacement = ""), 
                                            pval = bool, .dynamic = T)
                } else if (t %in% c("expF", "aveF", "supF")) {
                    mat[i, "p-valor"] <- ft(test = t, pval = bool)
                } else {
                    mat[i, "p-valor"] <- rt(test = t, pval = bool, .dynamic = F)
                }
            } else {
                if(t %in% c("Rec-CUSUM(d)", "OLS-CUSUM(d)" ,"Score-CUSUM(d)")) {
                    mat[i, "est"] <- rt(test = gsub(t, pattern = "\\(d\\)", replacement = ""), 
                                        pval = bool, .dynamic = T)
                } else if (t %in% c("expF", "aveF", "supF")) {
                    mat[i, "est"] <- ft(test = t, pval = bool)
                } else {
                    mat[i, "est"] <- rt(test = t, pval = bool)
                }   
            }
        }
        if(mat[i, "p-valor"] > 0.1) {
        mat[i, "resultado"] <- ""
        } else if(mat[i, "p-valor"] >= 0.05 & mat[i, "p-valor"] < 0.1) {
            mat[i, "resultado"] <- "."
        }else if (mat[i, "p-valor"] >= 0.01 & mat[i, "p-valor"] < 0.05) {
            mat[i, "resultado"] <- "*"
        } else if (mat[i, "p-valor"] >= 0.001 & mat[i, "p-valor"] < 0.01) {
            mat[i, "resultado"] <- "**"
        } else {
            mat[i, "resultado"] <- "***"
        }
    mat[, 2:3] <- sapply(mat[,2:3], as.numeric)
}
colnames(mat) <- c("Test", "Estadístico", "p-valor", "Significación")
# comentario = list(pos = list(0), command = NULL)
# comentario$pos[[1]] = 1:11
texto = "\\\\footnotesize Test de quiebres estructurales de fluctuación generalizada, basados en residuos y estimadores. La columna \\\\textit{Test} presenta los test realizados siguiendo la nomenclatura usada por (ref:Zeileis2002). \\\\textit{Estadístico} muestra el valor de los estadísticos, \\\\textit{p-valor} refiere a dicho valor para cada prueba. \\\\textit{Significación} muestra los niveles de significación donde . quiere decir no significativo al 5\\\\% pero si al 10\\\\%. Un * es estadísticamente significativo al 5\\\\%, ** es significativo al 1\\\\% mientras *** es significativo al 0.1\\\\%. Todos los test son estadísticamente significativos al 5\\\\% a excepción del test Score-CUSUM(d).

\\\\textit{Fuentes}: Se compatibiliza las tasas de desempleo trimestrales publicadas por el Instituto Nacional de Estadística (INE). La tasa de vacantes laborales es de construcción propia a partir de avisos de los portales laborales \\\\textit{Gallito}, \\\\textit{Buscojobs} y \\\\textit{Computrabajo}"

kableExtra::kable(mat[1:11,], digits = 2, row.names = FALSE, align = "c", caption = "Test de quiebres estructurales", escape = FALSE, booktabs = TRUE, format = "latex", longtable = F) %>% 
  kableExtra::kable_styling(full_width = F, latex_options = "hold_position") %>% 
  kableExtra::column_spec(column = 2:4, width = "2.5cm") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::footnote(general = texto, escape = FALSE, general_title = "Notas:", 
                       threeparttable = TRUE, fixed_small_size	= TRUE)
```


(ref:Andrews1993) @Andrews1993
(ref:Andrews1994) @Andrews1994
```{r ftest, fig.cap="Estadísticos F", results='asis'}
texto = "\\\\footnotesize Test de estadísticos F. Donde expF refiere al test exponencial, el test aveF refiere al test F utilizando la media mientras el test supF utiliza el supremo, para los tres casos ver (ref:Andrews1993) y (ref:Andrews1994). La columna \\\\textit{Test} presenta los test realizados siguiendo la nomenclatura usada por (ref:Zeileis2002). \\\\textit{Estadístico} muestra el valor de los estadísticos, \\\\textit{p-valor} refiere a dicho valor para cada prueba. \\\\textit{Significación} muestra los niveles de significación donde . quiere decir no significativo al 5\\\\% pero si al 10\\\\%. Un * es estadísticamente significativo al 5\\\\%, ** es significativo al 1\\\\% mientras *** es significativo al 0.1\\\\%. En todos los test se rechaza la hipótesis nula de no existencia de quiebre estructural utilizando un ancha de banda $h = 0.15$.

\\\\textit{Fuentes}: Se compatibiliza las tasas de desempleo trimestrales publicadas por el Instituto Nacional de Estadística (INE). La tasa de vacantes laborales es de construcción propia a partir de avisos de los portales laborales \\\\textit{Gallito}, \\\\textit{Buscojobs} y \\\\textit{Computrabajo}"
kableExtra::kable(mat[12:14,], digits = 2, row.names = FALSE, align = "c", caption = "Test de estadístico F", escape = F, booktabs = TRUE, format = "latex", longtable = F) %>% 
  kableExtra::kable_styling(full_width = F, latex_options = "hold_position") %>% 
  kableExtra::column_spec(column = 1:4, width = "3cm") %>%
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE, fixed_small_size	= TRUE)
```


\newpage

### Matrices robustas

```{r vcovHAC-lumley, fig.cap='Ponderadores de Lumley', results='asis'}
texto = "\\\\footnotesize Estimación para los cuatro periodos detectados mediante los test de quiebres estructurales. Se muestran los valores de los coeficientes estimados, sus respectivos errores estándar, estadísticos de la hipótesis y p-valor. Significación refiere a los niveles de significación, '.' quiere decir no significativo al 5\\\\% pero si al 10\\\\%. Un * es estadísticamente significativo al 5\\\\%, ** es significativo al 1\\\\% mientras *** es significativo al 0.1\\\\%.

\\\\textit{Fuentes}: Se compatibiliza las tasas de desempleo trimestrales publicadas por el Instituto Nacional de Estadística (INE). La tasa de vacantes laborales es de construcción propia a partir de avisos de los portales laborales \\\\textit{Gallito}, \\\\textit{Buscojobs} y \\\\textit{Computrabajo}"
kableExtra::kable(get_model(.mod = mod_rf, .mat = function(x) vcovHAC(x, weights = weightsLumley)), 
                  digits = 2, row.names = F, align = "c", 
                  caption = "Coeficientes de cada periodo (Lumley)", escape = F, booktabs = TRUE, 
                  format = "latex", longtable = F, 
                  col.names = c("Periodo", "Coeficiente", "Estimación", "Estándar error", "Estadístico", "p-valor", 
                                "Significación")
                  ) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)
```

```{r kernHAC, fig.cap='Ponderadores de Kernel', results='asis'}
kableExtra::kable(get_model(.mod = mod_rf, .mat = kernHAC), 
                  digits = 2, row.names = F, align = "c", 
                  caption = "Coeficientes de cada periodo (Kernel)", escape = F, booktabs = TRUE, 
                  format = "latex", longtable = F, 
                  col.names = c("Periodo", "Coeficiente", "Estimación", "Estándar error", "Estadístico", "p-valor", 
                                "Significación")
                  ) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE, 
                       escape = FALSE)

```

```{r NeweyWest, fig.cap='Ponderadors Newey West', results='asis'}
kableExtra::kable(get_model(.mod = mod_rf, .mat = NeweyWest), 
                  digits = 2, row.names = F, align = "c", 
                  caption = "Coeficientes de cada periodo (Newey West)", escape = F, booktabs = TRUE, 
                  format = "latex", longtable = F, 
                  col.names = c("Periodo", "Coeficiente", "Estimación", "Estándar error", "Estadístico", "p-valor", 
                                "Significación")
                  ) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)

```

\newpage
## Discusión

```{r molina96-98, fig.cap="Avisos y puestos laborales \\textit{Gallito} 1996-1998", fig.align="center"}
notas = "Publicaciones Gallito, series de avisos y puestos laborales de frecuencia mensual, construcción propia. Se observa una diferencia de nivel entre las series de puestos y avisos laborales, ambas siguen el mismo movimiento. La serie de puestos se construyo contabilizando todos los puestos solicitados en la sección de avisos destacados, en el resto de las secciones todos los avisos se contabilizaron con un puesto."
fuentes = "Datos obtenidos de los avisos clasificados semanales de \\textit{Gallito}, recolección propia."
# Este código gráfica los avisos totales y los puestos totales en el periodo.
# LOs puestos totales no son tal, sino que a los avisos totales se suman los puestos de los avisos destacados.
molina <- readxl::read_xlsx(here::here("Datos", "Originales", "Gallito-1996-1998.xlsx"), col_names = TRUE, sheet = "avisos_puestos", na = c(" ", ""))
setDT(molina)
molina$puestos_tot <- ifelse(molina$subseccion != "avisos destacados", molina$avisos, molina$total_puestos)
molina[data.table::year(f_ini) > 1995, 
       .(av = sum(avisos, na.rm = T),
         av_filtro = sum(puestos_tot, na.rm = TRUE),
         fecha = as.Date(paste(ano, mes, 1, sep = "-"))), 
       keyby = .(ano = data.table::year(f_fin), mes = data.table::month(f_fin))
       ][, ggplot(.SD) +
             geom_line(aes(x = fecha, y = av)) +
             geom_line(aes(x = fecha, y = av_filtro), linetype = "dashed") +
             geom_point(aes(x = fecha, y = av)) +
             geom_point(aes(x = fecha, y = av_filtro)) +
             scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
             scale_x_date(date_breaks = "3 month", date_labels = "%y %b") +
             labs(y = "Avisos laborales", x = "Fecha") +
             theme_Publication(position_legend = "bottom")
         ]
```

(ref:caption) @Alma2011

```{r iecon-avisos-puestos, fig.cap="Serie \\textit{Gallito} 2000-2009", fig.align="center"}

notas = "Serie de puestos y avisos laborales con frecuencia anual. Serie anual construida en base a los datos de las primeras dos semanas de los meses de marzo, junio y septiembre durante el periodo 2000-2009, construcción propia. Se observa un comovimiento entre avisos y puestos, donde la diferencia es de nivel."
fuentes = "Datos de (ref:caption), suministrado por autores."
ggplot(data = iecon_ts, aes(x = fecha, y = avisos)) +
    geom_line() +
    geom_point() +
    geom_line(aes(y = puestos), linetype = "dashed") +
    geom_point(aes(y = puestos)) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
    labs(y = "Cantidad de puestos", x = "Fecha") +
    theme_Publication()
```


\newpage
### PEA

```{r PeaMontevideo, fig.cap="PEA Montevideo 1980-2018"}

notas = "Serie trimestral de la población económicamente activa (PEA) del departamento de Montevideo, desde 1980 hasta 2018. Construcción propia"
fuentes = "Encuesta continua de hogadres (ECH) del Instituto Nacional de Estadística (INE) compatibilizada por el Instituto de Economía (IECON)."

ggplot(data = dt, mapping = aes(x = fecha, y = pea)) +
    geom_line() +
    geom_point(alpha = 0.3) +
    labs(y = "PEA", x = "Fecha") +
    theme_Publication()
```

<!--chapter:end:07-appendix.Rmd-->

