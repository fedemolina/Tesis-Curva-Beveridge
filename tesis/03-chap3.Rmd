# Estrategias Empíricas {#cap:Metodologia}

Se utilizan dos estrategias empíricas, primero se trabaja con test de quiebre estructural siguiendo a @Andrews1993, @Andrews1994, @BaiPerron2003, @BaiPerron1998, @Zeileis2002, @Zeileis2005, @Zeileis2010, ya que, que son capaces de captar quiebres discretos en los parámetros de un modelo lineal. Posteriormente se utilizan vectores autoregresivos estructurales con parámetros variables y volatilidad estocástica (TVP-VAR) siguiendo a @Primiceri2005, @Nakajima2011, @Lubik2016b y se utiliza el algoritmo corregido por @DelNegro2015.

Los TVP-VAR permiten relajar el supuesto de una relación invariante en los parámetros del modelo, mediante la modelización de parámetros que siguen un proceso autoregresivo.^[Se suele asumir que el orden del proceso es uno, por lo cual se modelan como paseos aleatorios.] El primer motivo para la utilización de los TVP-VAR con volatilidad estocástica se da porque gran parte de las series macroeconómicas muestran cierta no linealidad con comportamientos diferentes tanto en la persistencia como en la volatilidad.^[Por ejemplo, el desempleo tiende a aumentar más rápidamente al comienzo de una recesión que su caída ante una recuperación de la economía [@Lubik2016b]] Segundo, si solo se permite volatilidad en rezagos e interceptos, es posible obtener una gran variabilidad en los parámetros pese a que el verdadero PGD tenga únicamente volatilidad estocástica [@Sims2002]. Consecuentemente es preferible modelar ambos de forma conjunta y que los datos definan la fuente más importante [@Lubik2016b]. Tercero, la utilización de un TVP-VAR debería incluir volatilidad estocástica para poder representar de forma subyacente un modelo dinámico estocástico de equilibrio general, DSGE [@Lubik2016b].

Los test de quiebres estructurales para modelos de regresión lineal se pueden dividir en dos clases. Los test de fluctuación generalizada [@Kuan1995;@Ploberger1989;@Brown1975;@Society1988] y los test basados en el estadístico F [@Andrews1993;@Andrews1994;@Hansen1992]. Los primeros incluyen los test CUSUM, MOSUM y basados en estimadores. Mientras el test de Chow, test supF, aveF y expF corresponden al segundo tipo.^[@Zeileis2005 plantea un marco de referencia más general conocido como el marco de fluctuación M-generalizado que engloba a las dos clases anteriores y los scores por máxima verosimilitud (ML scores).]

Los test de tipo F permiten poner a prueba la hipótesis nula de parámetros invariables en el período, contra la hipótesis alternativa de cambio en los parámetros en un momento particular. Los mismos son generalizados (supF, aveF y expF) para poner a prueba Ho sin conocer el momento de quiebre estructural, sin embargo, se sigue planteando la existencia de dos particiones.^[La existencia de un quiebre estructural en los parámetros de la media condicional implica la existencia de dos periodos. En general si existen n quiebres se tienen n+1 periodos.] @BaiPerron1998 y @BaiPerron2003 extienden los test de tipo F para encontrar múltiples quiebres estructurales, al minimizar una función objetivo y aplicar el algoritmo de programación dinámica para encontrar un mínimo global sobre todas las posibles particiones. @Zeileis2010 lo adapta para modelos estimados por máxima verosimilitud y la inclusión de la varianza como un regresor (los casos anteriores lo tratan como un parámetro molesto).

Los test de fluctuación generalizada incluyen los test CUSUM, que contienen la suma acumulada de residuos estandarizados, mientras MOSUM refiere a suma de residuos móviles. Adicionalmente, estos procesos se repiten pero en vez de utilizar residuos, se utilizan las estimaciones de los parámetros, siendo procesos basados en los estimadores. La idea de los test de fluctuación generalizada es ajustar un modelo a los datos y derivar un proceso empírico que capture las fluctuaciones en los residuos o en las estimaciones de los coeficientes. Para esto se calculan límites del proceso (fronteras), lo que implica que el proceso límite es conocido bajo la hipótesis nula. Si el proceso  de fluctuación empírico (efp) cruza dichos límites en algún momento, la fluctuación del mismo es improbablemente elevado lo que lleva a rechazar la hipótesis nula al nivel de significación $\alpha$ [@Zeileis2002].

Como remarca @Benati2013, bien podría utilizarse únicamente test de quiebre estructural. Sin embargo, @Benati2007 muestra que los test de quiebres estructurales de @BaiPerron1998, @BaiPerron2003, @Bai1997 ofrecen poca evidencia de quiebres cuando el proceso generador de datos (PGD) evoluciona como un paseo aleatorio, en contraposición a una metodología más flexible como @Stock1996, @Stock1998 que logra captar dicha evolución, @Cogley2005 encuentran resultados similares. @Benati2013 remarcan que la utilización de TVP-VAR es robusta frente a la especificación de la variación temporal en los datos, mientras que los test de quiebres estructurales lo son solamente si el PGD tiene quiebres discretos. Sin embargo, @Lubik2016 obtiene que el TVP-VAR puede llevar a conclusiones erróneas al asociar corrimientos paralelos de la curva con shocks en la matriz de varianzas y covarianzas y no en los coeficientes rezagados de las variables endógenas.

Otra posible elección es la desarrollada por @Barnichon2012, @Hobijn2013. Como sugieren @Hobijn2013 el análisis no lineal es el método empírico más común en el análisis de la curva de Beveridge, sin embargo, no es el único, ellos utilizan una nueva forma basados en @Barnichon2012 en el cual estiman el logaritmo del ratio de contrataciones sobre el stock de vacantes usando como regresores las contrataciones, separaciones, número de desempleados, empleados y el stock de vacantes. Desafortunadamente, no todas esas variables están disponibles en Uruguay para el periodo considerado. Descartadas esta metodología, se sigue adelante con TVP-VAR y quiebres estructurales.

Para poder estimar un VAR estructural es necesario imponer restricciones de identificación sobre la matriz de varianzas y covarianzas para pasar de la forma reducida a la forma estructural. De esta forma, es posible descomponer el efecto de cada shock individual sobre las restantes variables endógenas del sistema y calcular las funciones de impulso respuesta [@Hamilton1994]. Las parametrizaciones que se deseen imponer sobre la matriz de varianzas y covarianzas puede provenir o no de la teoría económica. En el primer caso suele suceder cuando una variable es publicada con rezago respecto de otra, o bien responden de forma diferente, por ejemplo una variable financiera y otra relacionada a bienes y servicios. En cualquier caso, las restricciones pueden ser de corto plazo, de largo plazo o de signo y los shocks pueden ser tanto permanentes como transitorios. En el caso de un TVP-VAR el calculo es diferente y se obtiene una FIR para cada momento del tiempo.

La estimación de un TVP-VAR con volatilidad estocástica tiene como desafío principal como realizar la inferencia. Este trabajo utiliza un enfoque bayesiano en el cual se utiliza el muestreo de Gibbs en el cual el problema original de la estimación del vector de parámetros es intratable.^[@Primiceri2005 remarca cuatro puntos que hacen preferible la estimación con un enfoque bayesiano. El primero es que los parámetros de interés son componentes inobservables. Segundo, si la varianza de los parámetros variables es pequeña, la estimación por máxima verosimilitud (MV) de la varianza tiene un punto de masa en cero. Tercero, la estimación por MV en un modelo de alta dimensión y no lineal puede generar múltiples máximos locales implausibles o sin interés alguno. Por último, puede ser difícil maximizar la verosimilitud en un problema de alta dimensión, mientras los métodos bayesianos manejan esta problemática de forma natural, como el muestreo de Gibbs. @Lubik2016b remarca este último punto al mencionar que el enfoque bayesiano tiene desarrollados poderosos algoritmos computacionales que se adaptan particularmente bien al tratamiento de la variación temporal en los parámetros.] Por lo cual, se divide en pequeños bloques que pueden ser evaluados de forma secuencial e independiente.^[Para un detalle de la metodología ver @Nakajima2011, @Lubik2016b. Para ver esquemas de bloques desarrollados para los TVP-VAR ver @Primiceri2005, @DelNegro2015]

## TVP-VAR
@Primiceri2005 propone el siguiente modelo para un vector n-dimensional $y_t$:

\begin{equation}
y_t = c_t + B_{1,t}y_{t-1} + ...+ B_{k,t}y_{t-k} + u_t \ \ \ \ t = 1,....T.
(\#eq:var-reducido)
\end{equation}

Donde $y_t$ es un vector de variables endógenas n x 1; $c_t$ es un vector de parámetros variables n x 1 que multiplica términos constantes; $B_{i,t}$, $i = 1,....k$, son matrices n x n de coeficientes variables; $u_t$ son shocks inobservables, heterocedásticos con matriz de varianzas y covarianzas $\Omega_t$. 

Consideremos (sin perder generalidad) una reducción triangular de $\Omega_t$ definida por:

\begin{equation}
A_t\Omega_tA_t' = \Sigma_t\Sigma_t'
\end{equation}

Con $A_t$ una matriz triangular inferior con elementos ($\alpha_{ij,t}$) y unos en su diagonal. Y $\Sigma_t$ una matriz diagonal de elementos $\sigma_{i, t}$. El modelo \@ref(eq:var-reducido) pasa a ser:

\begin{align}
y_t &= c_t + B_{1,t}y_{t-1} + B_{2,t}y_{t-2} + ... + B_{p,t}y_{t-p} + A_t^{-1}\Sigma_t\epsilon_t (\#eq:svar-sv) \\
V(\epsilon_t) &= \mathbb{I}_t \notag
\end{align}

Apilando mediante el operador $vec$ todos los coeficientes del lado derecho de la ecuación \@ref(eq:svar-sv) en un vector $B_t$, se puede reescribir el modelo.

\begin{align}
y_t  &= X_t'B_t + A_t^{-1}\Sigma \epsilon_t (\#eq:svar-sv-final)
\end{align}

Donde $X_t' = I_n \otimes [1, y_{t-1}, ..., y_{t-p}]$ con $\otimes$ denotando el producto de Kronecker. Se mantiene que $y_t$ es un vector columna n-dimensional y  $B_t$ contiene los parámetros $\{B_{j,t}\}_{j=1}^p$ y $c_t$ de la ecuación \@ref(eq:svar-sv).
La estrategia de identificación consiste en modelar los coeficientes de la ecuación \@ref(eq:svar-sv-final) en lugar de \@ref(eq:var-reducido). El modelo VAR completo es:

\begin{align}
y_t  &= X_t'B_t + A_t^{-1}\Sigma\epsilon_t (\#eq:yt)\\
B_t &= B_{t-1} + \nu_t (\#eq:Bt)\\
\alpha_t &= \alpha_{t-1} + \zeta_t (\#eq:alphat)\\
log \ \sigma_t &= log \ \sigma_{t-1} + \eta_t (\#eq:sigmat)
\end{align}

La dinámica del modelo se resume en las ecuaciones \@ref(eq:Bt) a \@ref(eq:sigmat), donde $\alpha_t$ es el vector de elementos no negativos y no unos de la matriz $A_t$ los cuales están apilados por filas mediante el operador $vec$ mientras $\sigma_t = diag(\Sigma)$. Los elementos de $B_t$ se modelan como paseos aleatorios, supuestos que puede ser relajado, al igual que los elementos de la matriz $A_t$. Se supone que los desvíos estándar $\sigma_t$ evolucionan como un paseo aleatorio geométrico, lo cual lo hace pertenecer a la clase de modelos con volatilidad estocástica.^[La diferencia más importante respecto a los modelos ARCH es que las varianzas son componentes inobservables.]

La matriz de varianzas y covarianzas (VCV) de los residuos varía en el tiempo debido al término de error compuesto $A_t^{-1}\Sigma_t\epsilon_t$. Con $\epsilon_t$ siguiendo una distribución normal n-dimensional y $\{\nu, \zeta_t, \eta_t\}$ vectores normales homocedásticos, de media cero y mutuamente independientes.

Las innovaciones en el modelo se asume tienen una distribución normal conjunta con la siguiente matriz de varianzas y covarianzas:

\begin{equation}
V = Var 
\begin{pmatrix} 
\epsilon_t \\\nu_t \\ \zeta_t \\ \eta_t 
\end{pmatrix} =
\begin{pmatrix}
\mathbb{I}_n \ 0 \ 0 \ 0\\
0 \ Q \ 0 \ 0 \\
0 \ 0 \ S \ 0 \\
0\ 0\ 0\ W
\end{pmatrix}
\end{equation}

Donde $\mathbb{I}_n$ es una matriz identidad n-dimensional, Q, S, W son matrices semidefinidas positivas.^[@Primiceri2005 sustenta la elección de la la matriz V en que ya existe una gran cantidad de parámetros en el modelo y que permitir una estructura completa de autocorrelación entre las diferentes fuentes de incertidumbre inhibe cualquier interpretación estructural de los shocks. Adicionalmente se supone que S es diagonal por bloques, donde cada bloque corresponde a cada ecuación por lo cual los coeficientes de las relaciones contemporáneas se asumen evolucionan de forma independiente, esto se realiza para aumentar la eficiencia del algoritmo]

Para poder estudiar los efectos de un shock de productividad desde el producto al desempleo y vacantes, se computan las funciones de impulso respuesta (FIR). En el caso de un VAR de parámetros fijos, el computo de las FIR son independientes al proceso de estimación del modelo, mientras en un TVP-VAR ambas etapas son dependientes y se obtiene una FIR para momento del tiempo.
<!-- Especificación del test de Stock y Watson si soy capaz de armarlo -->


<!-- ESPECIFICACIÓN DE LOS MÉTODOS DE IMPUTACIÓN -->
<!-- Agrego la metodología de quiebres estructurales siguiendo a Zeilis. -->
## Quiebres estructurales

El modelo básico de regresión lineal es:

\begin{equation}
y_i = x_i^T\beta_i + u_i\space\space\space\space (i= 1...n)
\end{equation}

Para cada momento i, $y_i$ es la variable dependiente, $x_i = (1, x_{i2}, ..., x_{ik}^T)$ es un vector $k \times 1$ con $k-1$ observaciones de regresores o variables independientes. Los $u_i$ son $iid(o, \sigma^2)$ y $\beta_i$ es un vector k variado de parámetros. Los test de cambio estructural plantean la hipótesis nula de que los $\beta_i$ son invariantes en el tiempo, versus la hipótesis alternativa de que existe variabilidad en algún parámetro:

\begin{align}
H_0:\space\space \beta_i &= \beta_0 \space\space\space\space (i= 1...n) \\
H_1:\space\space \beta_i &\not= \beta_0 \space\space\space\space (i= 1...n)
\end{align}

<!-- Aca fue que corte y pase toda la parte explicativa de MOSUM y CUSUM al anexo -->
Se trabaja con dos estadísticos para poner a prueba la hipótesis nula. El estadístico $S_r$ se utilizada para los procesos basados en residuos, mientras el estadístico $S_e$ se utiliza para los procesos basados en estimaciones:

\begin{align}
S_r &= \max_t\frac{efp(t)}{f(t)}, \\
S_e &= \max \Vert efp(t) \Vert
\end{align}

Con f(t) dependiendo de la forma del límite, $b(t) = \lambda f(t)$. ^[Para los distintos cálculos de los p-valores para cada test consultar sección A en @Zeileis2002.]

Los estadísticos F difieren de los test anteriores en que se especifica la hipótesis nula a contrastar, se define una hipótesis alternativa de un quiebre en un momento particular.

\begin{equation}
\beta_i = \begin{cases} 
\beta_A &(1 \leq i \leq i_0) \\
\beta_B &(i_0 < i \leq n)
\end{cases}
\end{equation}

Con $i_0$ es algún punto en el intervalo $(k, n-k)$. Posteriormente son extendidos para no especificar el momento de quiebre, sino un intervalo de tiempo. Se utiliza una secuencia de estadísticos F para un quiebre en un momento i, se calculan los residuos de ese segmento $\hat{u}_i$ (una regresión para cada submuestra) y se comparan con los residuos de un modelo sin quiebres.

\begin{equation}
F_{i} = \frac{\hat{u}^T\hat{u}-\hat{e}(i)^T\hat{e}(i)} {\hat{e}(i)^T\hat{e}(i)/(n-2k)}
\end{equation}

<!-- El test original de Chow @Chow1960 necesita que se especifique el momento del quiebre en particular en la hipótesis alternativa. Su planteamiento es realizar dos regresiones, una restringida y otra sin restringir.^[Para testar la igualdad entre los conjuntos de coeficientes en las dos regresiones, se obtienen la suma cuadrado de los residuos (RSS) asumiendo igualdad (bajo $H_0$) y RSS sin asumir igualdad. El ratio de la diferencia entre las dos sumas y la última suma, ajustado por los correspondientes grados de libertad se distribuye como el estadístico F bajo $H_0$. Los resultados de @Chow1960 se pueden resumir en sus ecuaciones 50 y 51 y, son generalizables al caso de más de dos regresiones. Específicamente, $F_{i_0} \sim \chi^2_k$ y $F_{i_0}/k \sim \chi^2_{k, n-2k}$.] -->

<!-- La desventaja del test de Chow, es que el punto de quiebre debe ser conocido. Sin embargo, es posible plantear un test F generalizado para todos los potenciales puntos de quiebre en un intervalo de la muestra, cumpliendo que $k < \underline i \leq \overline i \leq n-k$^[@Andrews1993 recomienda 15-85% de la muestra al realizar los test de inestabilidad paramétrica cuando el punto de quiebre no es conocido.]. Rechazando $H_0$ si cualquier estadístico, $F_i$ sobrepasa los valores de tabla.  -->

Al igual que los efp, es posible plantear límites para el estadístico F. Asumiendo $H_0$, los límites se pueden calcular de tal forma que la probabilidad asintótica de alguna forma de agregación de los distintos estadísticos F calculados sobre los intervalos considerados $\underline i \leq i \leq \bar{i}$, superen dicho umbral con una significación $\alpha$. @Andrews1993, @Andrews1994 plantean utilizar los funcionales: supremo, media o exponencial, con lo cual plantean tres estadísticos para poner a prueba la hipótesis nula:

\begin{align}
supF &= \sup_{\underline i \leq i \leq \bar{i}} F_i \\
aveF &= \frac{1}{\bar{i} - \underline i + 1}\sum_{i = \underline i}^{\bar{i}}F_i \\
expF &= log \left( \frac{1}{\bar{i}-\underline i + 1}\sum_{i = \underline i}^{\bar{i}}exp(0.5\times F_i) \right)
\end{align}

Si sucede que el funcional supera dicho umbral, existe evidencia de un cambio estructural con una significación $\alpha$. 

<!-- @Liu1997 y @BaiPerron1998 plantean paralelamente este problema, su solución teórica y propiedades.  -->
@BaiPerron1998 plantean un marco general que engloba a un modelo estructural completo y parcial y @BaiPerron2003 implementan dicha solución utilizando un algoritmo de programación dinámica (ecuación de Bellman) y la función objetivo de MCO.\footnote{La diferencia entre un modelo estructural completo es que todos los parámetros pueden tener quiebres en la muestra, mientras un modelo estructural parcial permite a un subconjunto de los parámetros tener quiebres, mientras el resto son estimados con la muestra completa asumiéndolos invariables}

Sea $RSS(\{T_{r,n}\})$ la suma de cuadrados de residuos asociada con la partición optima conteniendo $r$ quiebres usando las primeras n observaciones. La partición óptima resuelve el siguiente problema recursivo[^estimation]:

$$
RSS(\{T_{m,T}\}) = \min_{mh\leq j\leq T-h}[RSS(\{T_{m-1,j}\}) + RSS(\{j+1, T\})]
$$

[^estimation]: Se evalúa primero, el primer quiebre óptimo para todas las submuestras desde $h$ hasta $T-mh$. Se guardan un conjunto de $T-(m+1)h+1$ particiones óptimas y sus RSS, donde cada partición corresponde a una submuestra terminando en $2h$ hasta $T-(m-1)h$.
Segundo, se buscan las particiones óptimas con dos quiebres, las cuales terminan en el periodo $3h$ hasta $T-(m-2)h$. Para cada uno de estas posibles fechas de término, se busca en que partición de un quiebre guardada previamente puede ser insertada para obtener un RSS mínimo. Se devuelve un conjunto de $T-(m+1)h+1$ con dos quiebres óptimos. El algoritmo continua de forma secuencial hasta que el conjunto $T-(m+1)h+1$ de $(m-1)$ particiones óptimas se obtiene con finalización desde $(m-1)h$ hasta $T-2h$. Se busca cual de esas $(m-1)$ particiones óptimas genera un mínimo global en RSS cuando se combina con un segmento adicional.

@Zeileis2010 extiende el trabajo de @BaiPerron2003, para trabajar con modelos de tipo de cambio, como @Frankel1994 en los cuales la varianza del error $\sigma^2$ es de crucial interés. Esto lleva a la inclusión del error de la varianza como un regresor adicional en vez de un parámetro molesto y, la estimación del modelo por máxima verosimilitud o cuasi-máxima verosimilitud. La inclusión de $\sigma^2$, como nota @BaiPerron2003, puede mejorar la estimación de los quiebres estructurales.

El modelo planteado es cuasi-normal y tiene densidad:
$$
f(y|x,\beta, \sigma^2) = \phi((y-x^T\beta)/\sigma)/\sigma
$$
Donde $\phi(.)$ es la función de densidad de una normal estándar. Con $\theta = (\beta^T, \sigma^2)^T$ de largo $k = c +2$, siendo c la cantidad de regresores, más intercepto y varianza.

El algoritmo para encontrar quiebres es exactamente el mismo que @BaiPerron2003, con la diferencia que en vez de usar estimaciones MCO se usan estimaciones QML y la función objetivo $RSS$ se cambia por la log-verosimilitud negativa $-logf(y_i|x_i, \theta)$.
