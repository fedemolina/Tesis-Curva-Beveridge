# Resultados {#cap:Resultados}

```{r}
impulse_response <- function (fit, impulse.variable = 1, response.variable = 2, 
    t = NULL, nhor = 20, scenario = 2, draw.plot = TRUE, ..main = .main) 
{
    Beta.draws <- fit$Beta.draws
    nd <- dim(Beta.draws)[3]
    if (is.null(t)) 
        t <- dim(Beta.draws)[2]
    out <- matrix(0, nd, nhor + 1)
    M <- fit$M
    p <- fit$p
    H.sel <- fit$H.draws[, ((t - 1) * M + 1):(t * M), ]
    A.sel <- fit$A.draws[, ((t - 1) * M + 1):(t * M), ]
    if (scenario == 3) {
        sig <- apply(exp(0.5 * fit$logs2.draws), 1, mean)
        sig <- diag(sig)
    }
    else {
        sig <- NULL
    }
    for (j in 1:nd) {
        if (scenario == 1) {
            H.chol <- NULL
        }
        else if (scenario == 2) {
            H.chol <- t(chol(H.sel[, , j]))
        }
        else if (scenario == 3) {
            H.chol <- t(solve(A.sel[, , j])) %*% sig
        }
        aux <- bvarsv:::IRFmats(A = bvarsv:::beta.reshape(Beta.draws[, t, j], 
            M, p)[, -1], H.chol = H.chol, nhor = nhor)
        aux <- aux[response.variable, seq(from = impulse.variable, 
            by = M, length = nhor + 1)]
        out[j, ] <- aux
    }
    if (draw.plot) {
        pdat <- t(apply(out[, -1], 2, function(z) quantile(z, 
            c(0.05, 0.25, 0.5, 0.75, 0.95))))
        xax <- 1:nhor
        matplot(x = xax, y = pdat, type = "n", ylab = "", xlab = "Horizonte", bty = "n", xlim = c(1, nhor))
        polygon(c(xax, rev(xax)), c(pdat[, 5], rev(pdat[, 4])), 
            col = "grey60", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 4], rev(pdat[, 3])), 
            col = "grey30", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 3], rev(pdat[, 2])), 
            col = "grey30", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 2], rev(pdat[, 1])), 
            col = "grey60", border = NA)
        lines(x = xax, y = pdat[, 3], type = "l", col = 1, lwd = 2.5)
        abline(h = 0, lty = 2)
        title(main = ..main, cex.main = 0.6, adj = 0, line = 0)
    }
    list(contemporaneous = out[, 1], irf = out[, -1])
}
# Carga del modelo svar-sv
fit <- readRDS(here::here("Datos", "Finales", "modelo.rds"))

# Función para generar los gráficos de parámetros y varianzas
matplot2 <- function(...) matplot(..., type = "l", lty = 1, lwd = 2, bty = "n", ylab = "")
stat.helper <- function(z) c(mean(z), quantile(z, c(0.16, 0.84)))[c(2, 1, 3)]
gp <- seq(1985, 2020, 5) # marks for vertical lines
# colors, taken from http://www.cookbook-r.com
cols <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
cols1 <- cols[c(2, 4, 2)]
make_plot <- function(.fit = fit, .type = "vcv", .var = 1, .title = "") {
    gp <- seq(1990, 2020, 5) # marks for vertical lines
    if(.type == "vcv") {
        # SD of unemployment residual
        # Get posterior draws
        sd_inf <- parameter.draws(.fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(sqrt(sd_inf), 2, stat.helper))    
    } else if (grepl(x = .type, pattern = "lag")) {
        beta <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1   <- t(apply(beta, 2, stat.helper))    
    } else {
        beta_0 <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(beta_0, 2, stat.helper))       
    }
    xax <- seq(1990, 2018, length.out = NROW(x1)) # x axis
    # Plot
    if(.type == "vcv") {
        var <- sd.residuals.ols[.var]
    } else {
        var <- NULL
    }
    matplot2(x = xax, y = x1, ylim = c(min(x1), max(x1)), col = cols1, main = .title , xlab = "Fecha")
    abline(h = seq(min(x1), max(x1), length.out = 10), v = gp, lty = 4, lwd = 0.3)
    if(.type == "vcv") {
        abline(h = var, col = cols[1], lwd = 1.4, lty = 5)
    }
}

# Función para generar los IRF
plot_irf <- function(.fit = fit, impulse, response, scenario = 2, .main = "") {
    ira <- impulse_response(fit, impulse.variable = impulse, response.variable = response, scenario = scenario, ..main = .main)
    # OLS impulse responses for comparison
    ira.ols <- irf(fit.ols, n.ahead = 20)[[impulse]][[response]][-1, 1]
    # Add to plot
    lines(x = 1:20, y = ira.ols, lwd = 1, lty = 5, col = "red")
}

# carga del modelo var
library(vars)
fit.ols <- readRDS(here::here("Datos", "Finales", "modelo-var-comun.rds"))
sd.residuals.ols <- apply(residuals(fit.ols), 2, sd)
```

<!-- Este capítulo presenta los resultados principales del trabajo. Primero se grafica la curva de Beveridge entre 1981 y 2018, se hace una partición por décadas para visualizar potenciales etapas. Realizamos una exploración de la relación entre vacantes laborales y el IVF del PIB y tasa de desempleo con producto. -->

<!-- Posteriormente analizamos las series utilizadas y sus propiedades estadísticas, tales como raíces unitarias, raíces estacionales y cointegración utilizando distintos test estadísticos, resultados que pueden consultarse en el anexo. -->

<!-- A continuación planteamos una búsqueda de quiebres estructurales mediante distintos tipos de test estadísticos, como procesos de fluctuación, test F y dating. Con los test de tipo dating logramos encontrar la cantidad de quiebes estructurales óptimos dada una función objetivo con lo cual obtenemos diferentes periodos de análisis. -->

<!-- Posteriormente estimamos un TVP-VAR con volatilidad estocástica, donde analizamos la evolución de los rezagos de las variables endógenas y los desvíos estándar de la matriz de varianzas y covarianzas. De esta forma analizamos si el PGD contiene modificaciones (suaves) en los parámetros bajo un modelo multivariado.  -->

<!-- Finalmente computamos las FIR para analizar el efecto que tienen shocks estructurales desde el producto hacia la tasa de desempleo y el índice de vacantes laborales. -->

<!-- Todo el proceso de quiebres estructurales y estimación de TVP-VAR ha sido realizado en el lenguaje _R_ utilizando los paquetes strucchange [@Zeileis2002], fxregime [@Zeileis2010] y bvarsv [@Kruger2015]. -->

<!-- \newpage -->

```{r include_data, include = FALSE}
# Base de datos final
dt <- readRDS(here::here("Datos", "Finales", "series_version_final.rds"))

# Serie iecon. Se usa en FUENTES y CRÍTICAS
iecon <- haven::read_dta(here::here("Datos", "Originales", "Gallito-2000-2009.dta"))
iecon$fecha <- as.Date(paste(iecon$aniog,iecon$mesg, iecon$diag, sep = "-"))
iecon_ts <- iecon %>% 
    dplyr::group_by(aniog, mesg) %>% 
    dplyr::summarise(puestos = sum(puestos, na.rm = TRUE),
                            avisos = dplyr::n())
iecon_ts$fecha <- as.Date(paste(iecon_ts$aniog, iecon_ts$mesg,"1", sep = "-"))
iecon_ano <- iecon %>% 
                dplyr::group_by(fecha = lubridate::make_date(aniog)) %>%                          dplyr::summarise(puestos = sum(puestos, na.rm = TRUE),
                                 avisos = n())

# Serie de CERES. Utilizada en FUENTES:Ceres
ceres <- readxl::read_xls(here::here("Datos", "Originales", "ICDL-1998-2014.xls"),
                          col_names = TRUE, sheet = "serie", 
                          col_types =c("date","numeric"),
                          readxl::cell_cols("A:B"))
ceres_ts <- ts(data = ceres[,2], start = c(1998,3), frequency = 12)
ceres_ano <- 
    ceres %>% 
        dplyr::group_by(fecha = lubridate::make_date(lubridate::year(fecha))) %>%
        dplyr::summarise(ind_vacantes = mean(vacantes, na.rm = TRUE))

```
<!-- ## Construcción indicador de vacantes  -->
Hasta los años 2000 la única fuente relevante de avisos laborales en papel fue _Gallito_, por lo cual basta con recabar dicha información para tener una muestra representativa de los avisos laborales.^[En Uruguay es posible visitar la Biblioteca Nacional donde por ley deben guardarse copias de todas las publicaciones de prensa. Al consultar a los encargados no se encuentran otras fuentes de publicaciones laborales para el departamento de Montevideo. Esta preponderancia de _Gallito_ sumado a la ausencia de encuestas es una de las causas de no seguir la metodología utilizada en @Barnichon2010, quien genera un índice de vacantes utilizando datos de encuestas, avisos en periódicos y portales web.] Sin embargo, con la revolución de Internet y los portales web comienza a perder representatividad. A partir de allí el problema se divide en tres: 1) Cuánta representatividad pierde 2) A partir de año comienza a perderla 3) Para responder 1 y 2, es necesario definir que otras fuente relevantes aparecen y la población de avisos a utilizar, la cual debe ser representativa de todos los portales laborales.

Las nuevas fuentes laborales consideradas en Uruguay son _Buscojobs_ portal laboral que nace a partir del año 2007 y rápidamente logra obtener un cuota relevante del mercado.^[Los datos de _Uruguay Concursa_ se obtienen indirectamente a partir de _Buscojobs_ debido a que dicha página los incluye entre sus publicaciones, por lo cual _Buscojobs_ debe interpretarse como la suma de ambos portales laborales.] _Computrabajo_ que opera en Uruguay a partir del año 2003 y, _Uruguay Concursa_ portal laboral que centraliza todos los llamados de empleos públicos.^[La página omitida más relevante es _LinkedIn_, sin embargo, su participación recién comienza en los últimos cuatro o cinco años y según la encuesta de uso de Internet no más del 15\% de personas consulta dicha página.] La preponderancia de _Gallito_ se mantiene hasta 2008-2009, a partir de allí su participación decrece de forma sistemática hasta estabilizarse en torno a un 40\% del total de publicaciones.

Las tres páginas web de las cuales se extrajeron los datos difieren en la estructura con la cual presentan los avisos laborales. El mismo aviso se presenta de forma diferente y se pueden encontrar variaciones en cuanto a la información brindada. Por ejemplo, puede que el nombre del puesto y la empresa difieran levemente, que no se brinde información respecto a la empresa, o se haga referencia a "importante empresa". 
<!-- Para poder identificar empresas compartidas por portales, primero se limpia el texto, se borran _stopwords_, se buscan patrones similares y finalmente se hace un filtro manual del cual se mapean 452 valores contenidos en una matriz de dimensión $\Re^{207*6}$ hacia un vector $x$ de dimensión $\Re^{207}$.  -->

Para compatibilizar *Gallito* primero se obtienen los datos de @Urrestarazu1997. Segundo, se recaba de forma exhaustiva los avisos laborales existentes en las publicaciones semanales de _Gallito_ entre octubre de 1995 y agosto de 1998. Tercero, se utilizan los datos de @Ceres2012. Dada la falta de especificaciones, se contrasta con la información generada por @Alma2011 con la cual se construye un índice de frecuencia anual.<!-- el cual no puede ser comparado gráficamente con ICDL debido a que las escalas no son compatible. --> La correlación lineal entre ambos índices es de 97\%, mientras sus tasas de crecimiento correlación lineal de 90\%. El resultado puede observarse en la Figura \@ref(fig:iecon-ceres). Cuarto, se recolectan muestras trimestrales de avisos entre 1999 y 2013 (denotados _gallito-BN_), de forma tal de corroborar la hipótesis que el ICDL debe estar generado a partir de publicaciones de _Gallito_.^[El término _gallito-BN_ refiere a recolecciones realizadas en la biblioteca nacional en los trimestres de 1999, 2000, 2009, 2010, 2011, 2012, 2013 y 2014 los cuales pueden verse en la Figura \@ref(fig:series-conjuntas) con color gris] En la Figura \@ref(fig:series-conjuntas) se puede ver como los _gallito-BN_ siguen el índice hasta el año 2012, a partir de allí se genera una diferencia en torno a los 1500-2000 avisos. Se asume que a partir de dicho año o bien cambio la metodología o se sumaron nuevas fuentes de datos. Por último se utiliza una base de datos confidencial entregada por el diario El País entre 2013 y 2018 y se realiza scraping de la página web _Gallito_ entre 2018 y 2019 de forma mensual.

En la Figura \@ref(fig:series-conjuntas) se puede observar las series trimestrales con las que se termina construyendo el índice de vacantes para el periodo 1980-2019. <!-- En todos los casos ha sido necesario imputar valores faltantes.  --> El proceso se detalla en la sección siguiente.

<!-- (ref:Urrestarazu1997) @Urrestarazu1997 -->

```{r series-conjuntas, fig.cap = "Series trimestrales sin corregir"}
notas = "Series individuales de avisos laborales sin corregir. La serie um corresponde a la serie combinada de (ref:Urrestarazu1997) (1980-1995), elaboración propia (1995-1998, 1999Q1, 2000) y una predicción de dos años. La serie Ceres son los avisos obtenidos a partir del Índice Ceres de Demanda Laboral (ICDL). Gallito refiere a los avisos de \\textit{Gallito} entre 2013 y 2018. Buscojobs y Computrabajos son los avisos obtenidos de los portales laborales \\textit{Buscojobs} y \\textit{Computrabajo} respectivamente. Gallito-BN (linea punteada gris) corresponde a los avisos trimestrales de \\textit{Gallito} de recolección propia. Todas las series son de construcción propia. Se observa un aumento permanente entre 1982 y 2000 (con leve caída en 1995). Luego una caída debido a la crisis de 2002, seguido de un aumento hasta 2012 y una caída permanente hasta 2019. Las series de Buscojobs y Computrabajo muestran un crecimiento casi constante."
fuentes = "Se utilizan datos de (ref:Urrestarazu1997) obtenidos de \\textit{Gallito}, de elaboración propia (1995-1998, 1999Q1, 2000, 2001Q1, 2009, 2010, 2011, 2012, 2013, recolectados de \\textit{Gallito} en biblioteca nacional. Base de datos de \\textit{Gallito} (2013-2018) facilitada por diario El País y datos de scraping web de los portales laborales \\textit{Buscojobs}, \\textit{Computrabajo} y \\textit{Gallito}."

col_um           = "black"
col_ceres        = "darkorange"
col_ga           = "darkgreen"
col_ct           = "darkblue"
col_bj           = "red"
col_muestreos    = "gray"
colores <-  c(col_um, col_ceres, col_ga, col_ct, col_bj, col_muestreos)
long_dt <- melt.data.table(data = dt, id.vars = "fecha", 
                measure.vars = c("av_urr_mol", "av_ceres", "av_ga_s_dup", 
                                 "av_ct_s_dup", "av_bj_s_dup", "muestreos"),
                variable.name = "series", value.name = "avisos")
ggplot(long_dt, aes(x = fecha, y = avisos, color = series)) +
    geom_line(linetype = "dashed") +
    geom_point(data = long_dt[series == "muestreos",], aes(x = fecha, y = avisos)) +
    scale_color_manual(values = colores, 
                       labels = c("um", "Ceres", 
                                  "Gallito", 
                                  "Computrabajo", "Buscojobs", 
                                  "Gallito-BN"),
                       name = "") +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "5 year", date_labels = "%Y") +
    labs(x = "Fecha", y = "Avisos") +
    theme_Publication(position_legend = "bottom")
```

## Construcción

Para despejar los avisos totales entre 1980 y 1995 de la serie de @Urrestarazu1997 ($v_t=\frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}$) se debe utilizar la *PEA* calculada por @Urrestarazu1997, posteriormente se debe calcular dicha variable desde 1995 hasta 2019 y finalmente unir ambas estimaciones.

Se calcula la PET desde 1997 con frecuencia trimestral a partir de la ECH, se estiman los valores ausentes en 1996 y 1997 y se obtiene $pet_{96-18}$. La estimación es un promedio no ponderado de una interpolación mediante splines y un modelo básico estructural estimado mediante un filtro de Kalman suavizado [@Durbin2013;@Moritz2017].
Segundo, se trimestraliza la serie anual de PET en Montevideo, usando el método de denthon-chollete [@Sax2013], con promedios, usando como serie guía la PET trimestral en el paso previo. Tercero la PEA, se obtiene usando la TA publicada por el INE, por definición $TA = PEA/PET$.
Cuarto se repondera la PEA de 1981-1995 y se unen las series. Finalmente se estiman los valores de 1980-1981 mediante un promedio simple de una estimación mediante un modelo estructural básico estimado por filtro de Kalman suavizado y un modelo Arima en representación de espacio estado.

<!-- ### 1980 -->
El objetivo es obtener una aproximación de la cantidad de avisos totales, por ello primero es necesario obtener $a_t$ desde el índice de vacantes Urrestarazu. Sin embargo, la base de dicho índice es 1980 y los datos disponibles en el trabajo son desde 1981 en adelante. El índice esta expresado en función de la PEA:
\begin{equation}
v_t = \frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}
\end{equation}

No se conoce $a_0$ ni $PEA_0$, ni se tienen datos de 1980. Por lo cual, se siguen los siguientes pasos:

1. Obtener $a_t$ en algún punto: $a_{1995_q4}$\footnote{La cantidad de avisos laborales del cuarto trimestre de 1995, fueron obtenidos a partir de las publicaciones del diario El País, sección Gallito}.
2. Imputar la PEA hacia atrás, obteniendo $pea_{t=0}$.
3. Despejar $a_{t=0}$ de 
$v_t$ = $(a_t/pea_t)/(a_{t=0}/pea_{t=0})100$.
4. Obtener la base $\alpha$ = $a_{t=0}$/$pea_{t=0}$.
5. Imputar las vacantes hacía atrás.
6. Calcular la serie de avisos

Se opta por realizar una predicción hacia atrás, planteando un modelo básico estructural mediante un filtro de Kalman suavizado [@Durbin2013].^[Se plantearon formulaciones alternativas, como imputación mediante filtro de Kalman sin suavizar, con 3 tipos de modelos estructurales: nivel-local, tendencia y básico estructural [@Harvey1989], además de modelos Arima.] A partir de ahora se trabaja con una serie trimestral de avisos.

<!-- ### 95-98 -->
A continuación se elige unir la serie de avisos $a_t^{80-95}$ con los avisos recolectados entre 1995 y 1998, dado que se utilizo $a_{1995}^{q4}$ para poder despejar la cantidad de avisos $a_t^{80-95}$ las series coinciden por construcción en el cuarto trimestre de 1995, por lo cual pueden ser unidas sin realizar ningún calculo adicional, la serie obtenida se denota $a_{um}^{80-98}$. Como se puede observar en la Figura \@ref(fig:series-conjuntas) las series combinadas mantienen el mismo nivel, aunque llama la atención la caída observada que se da entre 1995 y 1996. Sin embargo, dicha disminución no es causada por la combinación de las series, la misma comienza en el índice de vacantes de @Urrestarazu1997 donde los últimos cuatro trimestres tienen respectivamente una caída interanual de -17.3\%, -29.4\%, -25\% y -18.4\%, las cuales coinciden con la crisis del tequila. A partir de 1996 se comienza a revertir iniciandose un período de crecimiento que se mantiene por al menos 3 años.

<!-- ### 98-14 -->
Se utiliza el ICDL, $a_{ceres}^{98-14}$, de frecuencia mensual y sin factores estacionales.\footnote{No se conoce el método de desestacionalización, las notas metodológicas indican que esta ajustado por factores estacionales, sin embargo, por los análisis realizados lo más verosímil es que la serie publicada sea la tendencia-ciclo. En la sección siguiente se responden dichas críticas} El índice tiene la forma $v_t = a_t/a_0$, el problema es que observamos $\tilde{v_t}$ debido a la desestacionalización y no conocemos $a_0$. Aún conociendo $a_0$, no es posible recobrar el verdadero $a_t$, pero es posible encontrar una aproximación que mantenga el mismo movimiento y tendencia.

La serie es trimestralizada y se analiza si es necesario realizar alguna corrección. En la sección previa se realizaron distintas pruebas para validar la hipótesis que el ICDL corresponde, al menos hasta 2012, a publicaciones de _Gallito_. Es decir, estamos suponiendo que el proceso generador de datos es el mismo. La diferencia radica en que ha dicha serie le ha sido extraido (al menos) el componente estacional. 

Si suponemos que nuestro proceso generador de datos es^[Basandonos en @Harvey1989]:
\begin{equation}
Y_t = S_t + T_t + C_t + I_t
\end{equation}
Y observamos que $a_t^{ceres}$ es $T_t + C_t$ el hecho de que ajustemos el nivel de la serie mediante una corrección, por ejemplo, una reponderación, quiere decir que estaríamos reponderando la tendencia-ciclo de la serie. Si se observa la Figura \@ref(fig:series-conjuntas), tanto los muestreos como la serie $a_{um}^{80-98}$^[Notar que la serie se extiende hasta el año 2001. Esto se debe a que se realizaron predicciones de tal forma de tener más observaciones y observar el comovimiento de las series. Sin embargo, dicho período es difícil de predecir, ya que, coincide con la devaluación brasileña y posterior comienzo de recesión en la economía uruguaya. La predicción puede ser mejorada, pero no es el objetivo, sino simplemente ver si existe una diferencia insalvable en el nivel de las series, en la medida que esto no se observa, se deja de lado seguir probando proyecciones] acompañan el nivel de $a_t^{ceres}$, a tal punto que en algunos trimestres prácticamente coinciden. Suponemos que dichas diferencias se asocian a que es una serie filtrada, por lo cual se opta por unir las series a partir de 1998-III sin realizar ninguna corrección.^[Adicionalmente, se genero una serie de estacionalidad entre 1995 y 2018 utilizando los avisos recolectados entre 1995-1998 y los datos entregados por El País. Se imputo la estacionalidad para el periodo 1998-2013 y se agrego dicho componente a los avisos de _Ceres_, los resultados no varían.]
Se trimestraliza, y se realiza la unión, ambas series coinciden en el período compartido, se obtiene $a_{umc}^{80-14}$.

<!-- ### 13-19 Ga -->
El paso siguiente es la creación de una serie mensual de publicaciones sin avisos repetidos por link, utilizando los datos proporcionados por el diario El País, entre 2013 y 2018. Dicha serie se une con los avisos obtenidos para el año 2019 mediante el scraping web de la página web _Gallito_.^[La información de los últimos tres meses de 2018 se obtuvo desde @MTSS2018] Posteriormente se trimestralizan los datos.

```{r comparacion-gallito, fig.cap="comparacion-gallito"}
comparacion_gallito <- data.table(
    avisos_bd_s_dup = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), av_ga_s_dup],
    avisos_bd_c_dup = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), av_ga_c_dup],
    avisos_papel    = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), muestreos],
    fecha = seq.Date(as.Date("2013-10-01"), as.Date("2014-10-01"), "quarter")
)
comparacion_gallito[, `:=`(ratio_s_dup = avisos_papel/avisos_bd_s_dup,
                           ratio_c_dup = avisos_papel/avisos_bd_c_dup,
                           diferencia_s_dup = as.integer(avisos_papel - avisos_bd_s_dup)#,
                           # pct_s_dup = avisos_bd_s_dup/avisos_papel,
                           # pct_c_dup = avisos_bd_c_dup/avisos_papel
                           )]
data.table::setnames(comparacion_gallito, old = names(comparacion_gallito), new = c("avisos sin\n duplicados", "avisos con\n duplicados", "avisos\n papel", "fecha", "ratio sin\n duplicados", "ratio con\n duplicados", "diferencia sin\n duplicados"))
setkey(comparacion_gallito, "fecha")
notas <- "La primera columna avisos sin duplicados son los avisos web (base de datos) de El País una vez filtrados los avisos duplicados (link-ID repetido). La segunda toma en consideración los duplicados. La tercera columna avisos papel, refiere a los avisos recolectados en papel de Gallito, mientras fecha hace referencia al periodo de tiempo correspondiente. Las columnas ratio sin duplicados y ratio con duplicados indican ratio con duplicados y sin duplicados y corresponden a la división de la tercera columna con la primera o segunda respectivamente. La última columna, refiere a lo mismo pero tomando diferencias entre primera y segunda columna sobre tercera."
# fuentes <- "Datos de Gallito tanto en versión papel de recolección propia como datos web facilitados por diario El País, elaboración propia."

kableExtra::kable(comparacion_gallito, 
                  format = "latex", 
                  align = "c", 
                  booktabs = T, 
                  digits = 2, 
                  caption = "Comparación avisos laborales",
                  col.names = linebreak(colnames(comparacion_gallito)),
                  escape = FALSE) %>%
kableExtra::kable_styling(latex_options = "scale_down", position = "center") %>%
  kableExtra::footnote(general = notas, #number = notas,
                       # number_title = "Fuentes", 
                       general_title = "Notas:", 
                       footnote_as_chunk = F, 
                       threeparttable = TRUE, 
                       footnote_order = c("general"))
```

Se utilizan los avisos laborales facilitados por el diario El País para el período 2013-2018, con ellos se construye una serie trimestral de la cantidad de publicaciones.
Dichos datos es la variable no observable que deseamos cuantificar en los períodos previos pero que observamos con errores de medición, por ello lo denotamos $\tilde{a_t}$. Para obtener cuanto difieren los avisos en papel de los avisos de la base de datos, se muestrean los siguientes trimestres: $a_{q4}^{13}$, $a_{q1}^{14}$, $a_{q2}^{14}$, $a_{q3}^{14}$, $a_{q4}^{14}$.\footnote{En todos los casos existen semanas, donde la publicación semanal no estaba disponible. Por ello, se modelizaron serie de frecuencia semanal las cuales fueron imputadas mediante la modelización de un modelo básico estructural estimado por un filtro de Kalman suavizado} Como se puede observar en el Cuadro \@ref(tab:comparacion-gallito) la diferencia entre $\tilde{a_t}$ y ${a_t}$ es significativa.

Se elige corregir la serie $a_{umc}^{80-14}$ de forma de que la serie queda expresada en los mismos términos que los avisos actuales, $\tilde{a_t}$, por lo cual, es posible seguir extendiendo el periodo de análisis simplemente agregando futuras observaciones obtenidas mediante scraping. Se repondera la serie $a_{umc}^{80-14}$ en base al promedio de los avisos $a_{q2}^{14}$, $a_{q3}^{14}$, $\tilde{a}_{q2}^{14}$, $\tilde{a}_{q3}^{14}$, obteniendo $\tilde{a}_{umc}^{80-14}$.

Se utilizan los _gallito-BN_ para corroborar que la fuente del ICDL sea _Gallito_.
Dada la diferencia que se observa entre los _gallito-BN_ y $a_{ceres}^{98-14}$ a partir del año 2012, se considera dicha serie hasta el primer trimestre de 2012. El fundamento es que el ICDL coincide con los _gallito-BN_ realizados cuya diferencia puede ser atribuida a la desestacionalización del mismo, sin embargo, en 2013 y 2014 las diferencias son de un orden de magnitud tal que dicha hipótesis no puede ser mantenida. 
Como aproximación en base a las series disponibles se genera una serie de estacionalidad imputada desde 1995 hasta 2018 de frecuencia mensual plateando los siguientes modelos:

$$
\begin{aligned}
Y_t &= T_t + C_t + S_t + I_t \\
Y_t &= T_t C_t S_t I_t
\end{aligned}
$$
Se recupera el componente $S_t$ y $S_t + I_t$, se generan dos series, las cuales son imputadas entre 2001 y 2012. En ningún caso los valores absolutos de la estacionalidad superan los 600 avisos, si eso se trimestraliza la diferencia máxima que se observa es inferior a 1000 avisos.

Finalmente, se unen las series imputando los valores trimestrales $a_{q2}^{12}$, $a_{q3}^{12}$, $a_{q4}^{12}$, $a_{q2}^{13}$. Se divide la serie en estaciones (trimestres) y se realizan imputaciones en cada serie por separado, utilizando el filtro de Kalman suavizado. De esta forma se obtiene la serie final correspondiente a _Gallito_ denotada como $a_{umcg}$, la misma se puede observar en la Figura \@ref(fig:serie-final-gallito) con color azul (umcg).

```{r serie-final-gallito, fig.cap = "Serie de avisos final"}
notas = "Serie trimestral de avisos laborales. La serie umcg refiere a la serie final de gallito obtenida de la combinación de (ref:Urrestarazu1997), datos de recolección propia entre 1995-1998 y trimestres de 1999-2001, de CERES (obtenido a partir del Índice Ceres de Demanda Laboral), una base de datos facilitada por el diario El País (2013-2018). Ceres, son los datos de CERES obtenidos a partir del Índice Ceres de Demanda Laboral (ICDL). Gallito es la serie generada a partir de Gallito entre 2013 y 2018. Y um es la serie combinada de Urrestarazu y Molina (construcción propia)."
fuentes = "1980-1995 datos extraídos de (ref:Urrestarazu1997), entre 1995 y 1998 recolección propia, 1998-2012 se utilizan los datos de CERES referidos al ICDL, entre 2013 y 2018 datos provistos por diario El País"
# Generar otra serie urr_mol pero sin la predicción, agregarla acá.
long_dt <- melt.data.table(data = dt, id.vars = "fecha", 
                measure.vars = c("av_umcg", "av_ga_s_dup", 
                                 "av_urr_mol", "av_ceres"),
                variable.name = "series", value.name = "avisos")
ggplot(long_dt, aes(x = fecha, y = avisos, color = series)) +
    geom_line(linetype = "dashed") +
    scale_color_manual(values = c("darkblue", col_ga, col_um, col_ceres),
                       labels = c("umcg", "Gallito", "um", "Ceres"),
                       name = "") +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "5 year", date_labels = "%Y") +
    labs(y = "Avisos", x = "Fecha") +
    theme_Publication(position_legend = "bottom")
```

Los publicaciones laborales del portal *Buscojobs* se recaban desde 2007/06 hasta 2018/12 a través de la página web _Waybackmachine_. Se contabilizaron todas las publicaciones mensuales encontradas que hacen referencia al total de avisos publicados, los cuales son promediados mensualmente. Es decir, cada punto refiere a un año-mes y el total de avisos de publicados en dicho momento.

En el año 2019 se realiza scraping sobre el portal _Buscojobs_ extrayendo de forma mensual todos los avisos laborales disponibles y la información completa de cada aviso. La serie se modela con frecuencia mensual y los valores faltantes son imputados removiendo el componente estacional de la serie, imputando la serie desestacionalizada y luego agregando el componente estacional, finalmente la serie es trimestralizada, la misma puede obvervarse en la Figura \@ref(fig:serie-buscojobs).

Las publicaciones laborales del portal *Computrabajo* referido al total de avisos publicados, se promedia en los meses en que existen múltiples observaciones y se obtiene una serie de frecuencia mensual. Se agregan los datos de scraping desde octubre de 2018 hasta diciembre de 2019. Dado que _Computrabajo_, muestra una cantidad de avisos totales diferente de la cantidad de avisos publicados en los últimos treinta días es necesario realizar una corrección, en caso contrario se estará sobrestimando los avisos publicados. De los datos obtenidos, se observa que los avisos publicados en los últimos treinta días representan un 40-43\% del número de avisos totales publicados que muestra el portal. A partir de julio de 2011, los avisos de _Computrabajo_ son reponderados por 0.42 y en las fechas previas por 0.63.^[Esta decisión ad-hoc se toma puesto que a partir de julio de 2011 la serie de _Computrabajo_ comienza a tener un crecimiento exponencial, además en las fechas previas las cantidades de avisos son bajas comparativamente, por lo cual es menos probable que difieran la cantidad de avisos mostrados de los efectivamente publicados en los últimos treinta días.] La serie final se observa en \@ref(fig:serie-computrabajo).

Finalmente, las tres series son combinadas realizando un análisis del texto de los avisos laborales de forma de obtener el porcentaje de avisos compartidos entre páginas. Se construye un document-term matrix (DTM) con los avisos laborales, se vectoriza el texto mapeando palabras (1-gram) hacia un espacio vectorial, para esto es necesario crear un vocabulario común. Posteriormente, se calcula la similaridad de coseno:

$$
similaridad(doc1, doc2) = \cos(\theta) = \frac{doc1doc2}{|doc1||doc2|}
$$
entre los 3 portales web, obteniendo que los portales que más comparten avisos son _Computrabajo_ y _Buscojobs_, en torno a un 9\%, mientras con "El gallito" es torno a 3\%. Se combinan primero los avisos entre _Computrabajo_ y _Buscojobs_ ajustados por el \% de avisos compartidos, luego se combina con _Gallito_ en base al \% compartido.
<!-- % dic2018, 1485/3,755 -->

## Curva de Beveridge

```{r beveridge-curve, fig.cap="Curva de Beveridge 1981-2018", fig.ncol = 2, fig.height=5}
#out.width='.49\\linewidth'
# fig.fullwidth = TRUE
# fig.subcap=c('Curva de Beveridge trimestral', 'Curva de Beveridge anual')
notas = "Curva de Beveridge 1981-2018, Montevideo. Los colores representan las décadas de 1980, 1990, 2000 y 2010. El ratio de los ejes expresado como y/x es igual a 15. Se observa una curva con pendiente negativa y traslados paralelos entre 1980 y 2000. El periodo de 1990 muestra una transición hacia un punto más alejado del origen. El periodo de 2010 muestra un traslado hacia el origen."
fuentes = "Tasa de desempleo trimestral calculada por INE, índice de vacantes de elaboración propia."
# Reordenar década
dt$decada <- factor(dt$decada, levels = c(80, 90, 2000, 2010))

ochenta        = "gray"
noventa        = "orange"
dos_mil        = "darkgreen"
dos_mil_diez   = "skyblue"
color_decada <-  c(ochenta, noventa, dos_mil, dos_mil_diez)

y_lim <- c(min(dt$ind_vac)+1, max(dt$ind_vac)+1)
x_lim <- c(min(dt$td)+1, max(dt$td)+1)
p1 <- ggplot(dt[data.table::between(fecha, "1981-01-01", "2018-10-01"),], aes(y = ind_vac, x = td)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = y_lim) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(5, 20)) +
    geom_path() +
    # scale_color_manual(name = "", values = color_decada) +
    coord_fixed(ratio = 15) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = "Panel A. Datos trimestrales") +
    theme_Publication()

p3 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
         .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), 
           ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), 
         keyby = .(ano)] %>%
    ggplot(., aes(y = ind_vac, x = td, label = ano2)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = y_lim) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(5, 20)) +
    geom_path() +
    # position=position_jitter(width=.2,height=.02)
    ggrepel::geom_text_repel(fontface = "bold", size = 2, position=position_jitter(width=.1,height=.02), color = "black") +
    # scale_color_manual(name = "", values = color_decada) +
    coord_fixed(ratio = 15) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = "Panel B. Datos Anuales") +
    theme_Publication()
library(gridExtra)
library(grid)
# grid.arrange(p1,                             # First row with one plot spaning over 2 columns
#              arrangeGrob(p2, p3, ncol = 2), # Second row with 2 plots in 2 different columns
#              nrow = 2
#              )
# plot_grid_split(p1, p3)
library(patchwork)
# grid.arrange(p1, p3, nrow = 1)
(p1 + p3)
  
# lista = list(p1, p3)
# lista[[1]]
# cat('\n\n') 
# lista[[2]]
```

El primer hallazo del trabajo se observa en la Figura \@ref(fig:beveridge-curve) Panel A, es una relación negativa entre vacantes y desempleo en linea con la evidencia la literatura y teoría económica. Se observa que la curva ha sufrido traslados tanto hacia afuera como hacia el origen lo que debería relacionararse a modificaciones estructurales de distinta indole o bien shocks. También parecen existir movimiento diagonales relacionados al ciclo económico.
<!-- Para comenzar la exploración de la CB hemos coloreado la curva por décadas. El color azul denota la década de 1980, violeta años 90, rojo los 2000 y finalmente verde 2010 a 2020.  -->

<!-- El segundo resultado, es que se existen claros movimientos a lo largo de la curva (al parecer por décadas) y traslados de la misma (entre décadas).  -->
En la Figura \@ref(fig:beveridge-curve) Panel B observamos los promedio anuales del índice de vacantes y tasa de desempleo. Aquí queda un poco más claro que la década de los 80, esta en un cuadrante inferior hacia la izquierda en contraposición a la década del 2000 que es la más alejada del origen. Las observaciones de la década de los 90, se alejan del origen de forma oscilante, mientras las observaciones de los años 2010 se mueven de forma descendente y con una elevada pendiente moviendose entorno al 6-8% de desempleo. Claramente es la década con menor variabilidad en la tasa de desempleo.

<!-- si analizamos por década, podemos identificar cuatro fases, años 80, 90, 2000 y 2010. Donde los años 90 trasladan la curva hacia un nuevo estado con mayor desempleo ante igual cantidad de vacantes, el mismo se estabiliza en los 2000 y vuelve a cambiar de 2010 en adelante entrando en una fase con menor desempleo ante misma cantidad de vacantes. Limitandonos a un lustro podemos ver que a partir de 2005 parece haber un cambio en el mercado laboral, el cual se mantiene y vuelve a cambiar en 2010. Lo que se observa claramente, es que hay traslados paralelos en los últimos cuarenta años que podrían estar indicando un aumento y decenso de fricciones en el mercado de trabajo. Es llamativo que en la década de los ochenta y noventa bajo sistemáticamente la sindicalización, la negociación por rama en contraposición al aumento de la negociación por empresa y el estado dejo de participar en los consejos de salarios lo cual debería disminuir fricciones y generar movimientos hacia el origen, sin embargo, sucede lo contrario. Por otro lado, la importante cantidad de reformas aplicadas entre 2005 y 2015 que modificaron las reglas del juego de la economía no parece que haya aumentado las fricciones, puesto que la CB se ha trasladado hacia el origen. Es decir, parece ser que estamos en un mercado laboral que soporta una tasa de desempleo mayor lo que podría indicar una mayor eficiencia del mismo, resultado a priori inesperado. Otro resultado es que en los últimos diez años ha habido un movimiento sobre la curva, disminuyendo la cantidad de vacantes y aumentando el desempleo, indicando que la economía estaría en la parte baja del ciclo económico. -->

```{r td-vac-pib, fig.cap="Producto-Vacantes y Producto-Desempleo (1981-2018)", fig.ncol = 2, fig.height=4}
# , out.width='.49\\linewidth', fig.asp=1, fig.ncol = 2, fig.fullwidth = TRUE
notas = "Relación entre producto-vacantes y producto-desempleo, datos desde 1981 hasta 2018. Los colores representan las décadas de los años 1980 hasta 2010. La relación de los ejes medidos como y/x son respectivamente 1/70 y 1/6. En el panel A se observa el producto y el índice de vacantes, hasta el año 2012 la correlación es positiva, luego negativa. En el panel B observamos el producto y la tasa de desempleo. Las décadas de 1980 y 2000 muestran un comportamiento similar, una forma de U que hace referencia las crisis de 1982 y 2002."
fuentes = "Tasa de desempleo corresponde a publicaciones trimestrales de INE. La serie de producto es elaborada por el BCU, la misma fue facilitada por CINVE. Índice de vacantes de elaboración propia."
p1 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), keyby = .(ano)
   ] %>% 
ggplot(., aes(x = ind_vac, y = pib, label = ano2)) +
    geom_point() +
    geom_path() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(0.1, 1.2)) +
    # scale_color_manual(name = "", values = color_decada)+
    coord_fixed(ratio = 1/70) +
    labs(x = "Índice de vacantes", y = "IVF PIB", title = "Panel A. PIB-Vacantes") +
    ggrepel::geom_text_repel(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme_Publication()

p2 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
         .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), 
         keyby = .(ano)
   ] %>% 
    ggplot(., aes(x = td, y = pib, label = ano2)) +
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(5, 18)) +
    geom_path() +
    # scale_color_manual(name = "", values = color_decada)+
    coord_fixed(ratio = 1/6) +
    labs(x = "Tasa de desempleo", y = "IVF PIB", title = "Panel B. PIB-Desempleo") +
    ggrepel::geom_text_repel(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme_Publication()
(p1 + p2)
```

En la Figura \@ref(fig:td-vac-pib) Panel A podemos observar la relación entre vacantes y PIB. En la década de 2010 hay una notoria caída de vacantes laborales que no se ve acompañada por el PBI, como puede observarse en los años 80 y 2000. La correlación entre PIB y vacantes se torna negativa a partir del año 2012. La tasa de variación negativa en que caen las vacantes laborales entre 2011 y 2018, es prácticamente la misma entre 1998-2002 y 1981-1983, sin embargo el nivel de actividad no cae en ningún momento por lo que no se observa el movimiento de U típico de 80-82 y 98-2002. 

<!-- Esta observación en conjunto con el análisis de la CB podría estar indicando que el aumento del desempleo de los últimos años no se debe a mayores fricciones del mercado laboral, sino al desempleo estructural asociado a la diferencia sistemática entre las habilidades requeridas por el mercado y las ofrecidas por los trabajadores. Además debería haber un componente de cambio tecnológico e inversión en capital lo cual genera una menor demanda de trabajadores. -->

En la Figura \@ref(fig:td-vac-pib) panel B observamos el mismo comportamiento de la CB en los años 90, una transición hacia un nuevo estado desde una década de crecimiento con alto desempleo, hacia otra con crecimiento y caída del desempleo, los años 2000. Además, los 80 y 2000 vuelven a compartir la forma de U, solo que esta vez es en sentido contrario. La década de 2010 muestra a diferencia del gráfico anterior un comportamiento similar a los 90, observandose crecimiento económico con crecimiento del desempleo, aunque el nivel de actividad no cae en ningún momento. 
<!-- Es llamativo que periodos con políticas laborales tan diferentes como los 90 y 2010 tengan un comportamiento similar en cuanto al crecimiento de la actividad y aumento del desempleo. -->

## Quiebres estructurales

<!-- Es de interés buscar la existencia de quiebres estructurales tanto en la tasa de vacantes como en la tasa de desempleo, en la medida que trabajamos bajo la hipótesis de que se han producido cambios relevantes en el mercado laboral que han alterado su funcionamiento. Hemos probado que la tasa de vacantes y desempleo son procesos integrados de orden uno, sin media incondicional\footnote{No es posible que sean procesos I(1) y tengan media incondicional, en la medida que esto generaría series con crecimiento permanente en el largo plazo.}. Queremos probar si modelando cada ecuación como un proceso autoregresivo, obtenemos quiebres estructurales. -->

<!-- En la Figura \@ref(fig:beveridge-curve) Panel B gráficamos los datos de vacantes y desempleo junto a 4 modelos lineales, uno por cada década. La idea fue mostrar de forma intuitiva lo que parecen ser distintos periodos de la CB. Sin embargo, dicha agrupación no tiene sustento estadístico.  -->
A continuación, ponemos a prueba la hipótesis de existencia de algún quiebre estructural en la relación vacantes y desempleo y buscamos, en caso de existir, la fecha de dichos quiebres.

Planteamos:
\begin{equation}
log(ind\_vac_i) = \beta_i + \beta_i\log(td_i) + \epsilon_i
\end{equation}

Y sometemos a prueba:
\begin{align}
H_0: \beta_i &= \beta_0 \ \ \ (i = 1, ..., n) \\
H_1: \beta_i &\not= \beta_0 \ \ \ (i = 1, ..., n)
\end{align}

Los resultados se pueden ver en el Cuadro \@ref(tab:quiebres), donde se han llevado a cabo los test de fluctuación generalizada. En la columna _Test_, se identifican los test de quiebres estructural llevados a cabo siguiendo la nomenclatura usada por @Zeileis2002. La diferencia entre Rec-CUSUM y Rec-CUSUM(d) es que se permite la existencia de un rezago, ya que, @Society1988 muestran que los test CUSUM no pierden sus propiedades al relajar algunos supuestos, como trabajar con modelos dinámicos. En todos los casos que no se utilizan rezagos de la tasa de vacantes, se rechaza la hipótesis nula de invariabilidad en los parámetros, por tanto, no se rechaza la existencia de algún quiebre estructural en la relación entre vacantes y tasa de desempleo. Los únicos test que no rechazan $H_0$ son Score-CUSUM y OLS-CUSUM(d) modelos que incluyen un parámetro autoregresivo de vacantes. En todos los casos, siguiendo a @Zeileis2004 se estimo la matriz de varianzas y covarianzas robusta ante la heteroscedasticidad y autocorrelación usando un estimador de kernel cuadrático HAC [@Andrews1991] con un filtrado VAR(1) y una elección automática del ancho de banda basado en una aproximación AR(1)^[El kernel génerico es $\omega_l = K(\frac{l}{B})$ con K la función de kernel y B el ancho de banda. El kernel espectral tiene la siguiente forma $\omega_l = \frac{3}{z^2}(\frac{\sin(z)}{z} - \cos(z))$ siendo $l$ el rezago y $z = \frac{6\pi}{5}\frac{l}{B}$, ver @Andrews1991].

En las Figuras de la sección \@ref(efpAnexo) en el apéndice podemos observar las fluctuaciones del proceso empírico y su comparación con la fluctuación del proceso límite. Esto muestra en que periodo debería estar el o los quiebres en los parámetros, básicamente todos los test utilizados en el Cuadro \@ref(tab:quiebres) comparten que la hipótesis nula de la no existencia de cambio estructural debería ser rechazada cuando el proceso empírico se vuelve improbablemente superior a las fluctuaciones del proceso límite [@Zeileis2002]. 
<!-- DESCRIBIR! -->

Los test Score permiten observar variabilidad en la varianza, al sobrepasar el umbral esta es estadísticamente signifiticativa al 5\%. Tanto en el test Score-CUSUM como Score-MOSUM la varianza muestra fluctuaciones entre 1990 y 1995, y en torno a 2010-2011, sobrepasando el umbral. Por otra parte, el test Score-CUSUM con rezagos con p-valor 0.5, muestra una varianza al límite del umbral, pero sin sobrepasarlo. Es un indicio de que es posible plantear un modelo que no solo tome en cuenta los quiebres en la media condicional sino también en la varianza de los errores lo cual puede mejorar la estímación de los quiebres [@BaiPerron2003], por ello se estiman modelos de quiebres tanto de parámetros como varianza siguiendo a @Zeileis2010.

Planteamos los test de tipo F generalizados definiendo el tamaño mínimo del intervalo a considerar, en base a un parámetro de ancha de banda, h fijado en 0.15.

Utilizamos las tres variaciones propuestas por @Andrews1993 y @Andrews1994, supF, aveF y expF. En el Cuadro \@ref(tab:ftest) podemos observar el test, el valor del estadístico y el p-valor asociado. Se uso la misma matriz de variazas y covarianzas robusta igual que en el caso anterior. En los tres test, encontramos un quiebre estructural en torno a 1990-I, resultado en linea con @Urrestarazu1997.

```{r coefTestEstructural, fig.cap='Coeficientes de diferentes períodos', results="asis", fig.align='center', eval = TRUE, include=FALSE}
library(fxregime)
# FXREGIME 
reg <- log(ind_vac) ~ log(td) + 1
# Buscamos los quiebres cada 5 años
mod_reg <- fxregimes(formula = reg, data = zoo(dt_ts, frequency = 4), h = 20, 
                     breaks = 5)
# confint(mod_reg, level = 0.95, vcov = kernHAC)
# Resúmen completo, primero re-estimar el modelo en los subperiodos y luego aplicando summary
mod_rf <- refit(mod_reg)
# print(xtable(round(coef(mod_reg), 4)), comment = FALSE)
texto = "\\\\footnotesize Explicación de los coeficientes del modelo"
kableExtra::kable(coef(mod_reg), digits = 2, row.names = T, align = "c", caption = "Coeficientes de cada periodo. Resúmen", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = c("$\\beta_0$", "$\\beta_1$", "$\\sigma^2$")
                  ) %>% 
  kableExtra::kable_styling(full_width = F, latex_options = "hold_position") %>% 
  kableExtra::column_spec(column = 2:4, width = "2cm") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::footnote(general = texto, general_title = "Notas:", 
                       threeparttable = TRUE, fixed_small_size	= TRUE)
```

(ref:Zeileis2010) @Zeileis2010
(ref:BaiPerron2003) @BaiPerron2003
```{r vcovHAC-andrews, fig.cap='Coeficientes del período', results='asis', fig.align='center'}
get_model <- function(.mod, .mat = sandwich::vcovHAC) {
    a = data.table::rbindlist(
        lapply(.mod, function(x) {
        broom::tidy(
            lmtest::coeftest(x, .mat)
            )
    }), 
    use.names = TRUE, idcol = "modelo")
    setnames(a, old = names(a), 
            new = c("Modelo", "coeficiente", "Estimación", "Estándar error", 
                    "Estadístico", "p-valor"))
    a[`p-valor` > 0.1, sigf := ""]
    a[between(`p-valor`, lower = 0.05, 0.1),   sigf := "."]
    a[between(`p-valor`, lower = 0.01, 0.05),  sigf := "*"]
    a[between(`p-valor`, lower = 0.001, 0.01), sigf := "**"]
    a[between(`p-valor`, lower = 0, 0.001),    sigf := "***"]
    a[coeficiente == "(Intercept)", coeficiente := "$\\hat{\\beta}_0$"]
    a[coeficiente == "log(td)", coeficiente := "$\\hat{\\beta}_1$"]
    a[coeficiente == "(Variance)", coeficiente := "$\\hat{\\sigma}^2$"]
    a
}
texto = "\\\\footnotesize Estimación para los cuatro periodos detectados mediante los test de quiebres estructural siguiendo a (ref:BaiPerron2003) y (ref:Zeileis2010). Se muestran los valores de los coeficientes estimados $\\\\beta_0$, $\\\\beta_1$ y $\\\\sigma^2$ en la columna \\\\textit{Estimación}, sus errores estándar en \\\\textit{Estándar error} y el valor del estadístico en \\\\textit{Estadístico}. \\\\textit{P-valor} muestra los respectivos p-valores de cada prueba. \\\\textit{Significación} muestra los niveles de significación donde . quiere decir no significativo al 5\\\\% pero si al 10\\\\%. Un * es estadísticamente significativo al 5\\\\%, ** es significativo al 1\\\\% mientras *** es significativo al 0.1\\\\%. En todos los casos los coeficientes son estadísticamente significativos al 5\\\\% a excepción de $\\\\beta_0$ entre 1990 y 1996. Los signos de $\\\\beta_1$ son negativos para todos los períodos. En el período 2013 el valos de $\\\\sigma^2$ es diferente de 0, pero ha sido redondeado."
kableExtra::kable(get_model(.mod = mod_rf, .mat = sandwich::vcovHAC), digits = 2, row.names = F, align = "c", caption = "Coeficientes de cada periodo", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = c("Periodo", "Coeficiente", "Estimación", "Estándar error", "Estadístico", "p-valor", "Significación")
                  ) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)
```

Finalmente, pasamos a calcular todos los posibles periodos de quiebres estructurales de forma generalizada. Para ello se siguen los trabajos de @BaiPerron2003, @BaiPerron1998 y aplicaciones de @Zeileis2003, en especial a @Zeileis2010. Obtenemos tres quiebres estructurales en los años 1990-II, 1996-II y 2013-II usando como función objetivo la log-verosimilitud negativa. ^[Los quiebres son exactamente iguales si las variables se modelan en niveles o logaritmos. Es posible modelar otro cambio estructural eligiendo otra función objetivo como el BIC o RSS, sin embargo dicho intervalo es extremadamente amplio al computarlo con una matriz HAC, al elegir un máximo de 3 puntos de quiebres se obtienen los mismos resultados, si se fija un máximo de 4 se obtiene un quiebre adicional en 2004-I y se mantienen los restantes.] Por lo tanto, los 4 periodos de análisis son 1981-I.1990-II, 1990-III.1996-II, 1996-III.2013-I y 2013-II.2018.IV. En el Cuadro \@ref(tab:vcovHAC-andrews) observamos que en todos los periodos los parámetros muestran el signo correcto según la teoría (negativo). En todos los casos son estadísticamente significativos al 5\%, usando una matriz HAC con ponderadores de Andrews. Sin embargo, usando ponderadores de Kernel en el segundo periodo el p-valor es 0.12. Dada la existencia de autocorrelación y heteroscedasticidad, la elección de la matriz de ponderadores es relevante y puede generar variaciones en la significatividad estadística de algunos parámetros. Por ello los Cuadros \@ref(tab:vcovHAC-lumley), \@ref(tab:kernHAC) y \@ref(tab:NeweyWest) muestran diferentes ponderaciones para los distintos periodos. Es de notar que el único momento que podría ser discutible es entre 1990-III.1996-II, en el cual dependiendo la elección de la matriz el parámetro de la tasa de desempleo puede resultar no significativo, sin embargo, si se estima para el mismo periodo un modelo lineal pero sin varianza de los errores, la tasa de desempleo es estadísticamente significativa, con el signo del coeficiente negativo. En el resto de los casos los coeficientes son estadísticamente significativos al nivel $\alpha = 0.05$, al igual que las varianzas de cada periodo.

Las CB estimadas para los periodos obtenidos se pueden ver en la Figura \@ref(fig:BCtest). En todos los casos se mantiene una relación negativa entre vacantes y desempleo. A la vez que se observan cambios de nivel y pendiente. El primer periodo desde 1981-I hasta 1990-II muestra una curva comparativamente más cercana al origen que 1990-III a 1996-II donde se traslada de forma paralela, lo mismo vuelve a suceder en 1996-III a 2013-II (incluyendo un leve cambio de pendiente). <!-- denotando lo que podría ser un mercado laboral menos eficiente.  --> El comportamiento se modifica a partir de 2013-III en donde la curva tiene un cambio tanto paralelo como de pendiente hacia el origen.

<!-- Una interpretación es que las reformas estructurales llevadas a cabo a partir del año 2005, generaron un efecto negativo en el mercado laboral lo cual se revierte a partir de 2013. Aunque, es poco verosimil en la medida que los gobiernos entre 2005 y 2020 llevaron adelante medidas de protección hacia los trabajadores. Otra lectura sería que dichas reformas tuvieron un efecto impensado y no solo no generaron mayores fricciones en el mercado laborales, sino que las disminuyeron tornandolo más eficiente al aumentar el matching entre trabajadores y firmas^[Mantenemos siempre la condicionalidad sobre la mejora o no de la eficiencia en la medida que no tomamos en cuenta los flujos laborales desde el empleo al desempleo y del desempleo al empleo.]. Otro factor relevante son los portales laborales de Internet y el avance tecnológico que permiten una búsqueda y proceso de contratación a una velocidad comparativamente mayor que los procesos iniciados mediante prensa en papel. -->

```{r BCtest, fig.cap="Curva de Beveridge por periodo", fig.align="center"}
# Hacer una función, esto es muy manual.
dt[fecha <= "1990-04-01", decada_test := "1-periodo"
   ][between(fecha, "1990-07-01", "1996-04-01"), decada_test := "2-periodo"
     ][between(fecha, "1996-07-01", "2013-04-01"), decada_test := "3-periodo"
       ][between(fecha, "2013-07-01", "2019-10-01"), decada_test := "4-periodo"]
dt$decada_test <- factor(dt$decada_test,labels = c("81-90:Q4", "90:Q3-96:Q2", "96:Q3-2013:Q2", "2013:Q3-2018:Q4"))
notas = "CB para los periodos obtenidos mediante los test de quiebre estructural siguiendo a (ref:Zeileis2010) y (ref:BaiPerron2003). Se utiliza el paquete fxregime, resultados similares se obtienen con strucchange. En todas las etapas, la relación entre vacantes y desempleo es negativa. Se observan traslados de la CB y movimientos de pendiente. La segunda época entre los años 90-96 muestra un traslado hacia fuera, lo mismo sucede entre 96-2013. PEn 2013-2018 se da un corrimiento al origen, indicios de un mercado laboral que podría ser más eficiente."
fuentes = "Datos de vacantes laborales de elaboración propia. Tasa de desempleo obtenida de INE."

ggplot(dt, aes(y = ind_vac, x = td, color = decada_test)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = " \n ") +
    scale_color_manual(name = "", values = color_decada) +
    geom_smooth(method = "lm", formula = y ~ x) +
    theme_Publication()
# Agregar el caso de 4 quiebres y por tanto 5 particiones usando breakpoint.
```


## TVP-VAR

Por último, realizamos una estimación multivariada de un vector autoregresivo con parámetros variables y volatilidad estocástica siguiendo a @Primiceri2005, @Lubik2016 y @Lubik2016b, usando el algoritmo corregido por @DelNegro2015. Nos interesa analizar la variabilidad en los parámetros beta y la matriz de varianzas y covarianzas, sumado al efecto de un shock por parte del producto sobre vacantes y desempleo.
<!-- \footnote{Es posible utilizar el test desarrollado por @Stock1998.}. -->

Usamos las primeras 36 observaciones, nueve años, desde 1981-I hasta 1989-IV para calibrar las distribuciones a priori, quedando un periodo efectivo desde 1990 hasta 2018. Simulamos 50 mil veces y elegimos un orden de rezagos igual a uno, es decir, un TVP-VAR(1).^[Se corrieron 50 mil simulaciones con un orden de rezagos igual a dos y los resultados fueron los mismos. Por ello, se muestra un TVP-VAR(1) en la medida que se facilita la visualización.] Las series utilizadas de producto, tasa de desempleo e índice de vacantes son todas de frecuencia trimestral, lo cual (a excepción del PIB que se publica trimestralmente) va a generar series con menor variabilidad.

La restricción de identificación es que son los shocks desde el producto (shocks de productividad) los cuales afectan al desempleo y las vacantes laborales, con un rezago. Por lo tanto, el orden de exogeneidad de las variables es que el pib es la primer variable, seguido del índice de vacantes y la tasa de desempleo. El orden de la segunda y tercer variable, no es una restricción de identificación sino una normalización necesaria que puede modificar los resultados [@Primiceri2005], sin embargo, en este caso el orden no genera diferencias. La estructura de identificación elegida para las innovaciones es básicamente una especificación de Cholesky. En la Figura \@ref(fig:svar-parameters) observamos los desvíos estandar variables a lo largo del tiempo de los residuos del modelo, se gráfica la media (posterior) y los cuantiles 16 y 84\footnote{Normalidad}. 

El gráfico muestra dos resultados, el primero es que las vacantes laborales son estables para el periodo. El segundo es que si bien la magnitud es leve, parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante con una transición suave, donde el primero presenta una mayor varianza tanto para el pib como la tasa desempleo, mientras en el caso de las vacantes no se observan diferencias.^[En los test de quiebre estructural si se definen cuatro posibles quiebres estructurales, el cuarto se genera en torno a 2004.] Las modificaciones en las varianzas comienzan previo a 2005, lo cual se relaciona con la crisis de la economía en 2002 y el posterior crecimiento ininterrumpido a partir del tercer trimestre de 2003. 

<!-- Este último periodo (al menos hasta 2015), se caracteriza por un elevado crecimiento del producto y una reducción de la tasa de desempleo, en conjunto a múltiples reformas de carácter estructural. En la medida que un TVP-VAR con volatilidad estocástica es una forma reducida de un DSGE, esto podría dar indicio de dos períodos diferentes a evaluar bajo dicha metodología. -->
\begin{landscape}
```{r svar-parameters, fig.cap="Media posterior y volatilidades modelo TVP-VAR", fig.height=4.5, fig.width=8, fig.align="center"}
notas = "Se grafica las matrices $\\hat{A}_{jt}$ y $\\hat{\\Sigma}_{jt}$ desde 1990 hasta 2018 para el producto, índice de vacantes y tasa de desempleo. Observamos los desvíos estandar de los residuos del modelo, la media (posterior) y los cuantiles 16 y 84. Parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante, donde el primero presenta mayor varianza tanto para el pib como la tasa desempleo. En las vacantes, no se observan diferencias."
fuentes = "Serie de producto facilitada por CINVE. Tasa de desempleo obtenida de INE. Índice de vacantes construcción propia."
par(mfrow = c(3, 3))
for(i in c("intercept", "lag1", "vcv")) {
    for(j in 1:3) {
      if(j == 1) k <- "pib" else if (j == 2) k <- "vacantes" else k <- "desempleo"
      if (i == "intercept") {
        k <- eval(bquote(expression(.(k) ~ A[0][t])))
      } else if (i == "lag1") {
        k <- eval(bquote(expression(.(k) ~ A[1][t])))
      } else {
        k <- eval(bquote(expression(.(k) ~ Sigma[j][t])))
      }
      make_plot(.fit = fit, .type = i, .var = j, .title = k)
    }
}
```
\end{landscape}

En la Figura \@ref(fig:svar-parameters) se puede observar que los parámetros variables rezagados $B_{j,t}$ y los interceptos $c_{t}$ tienen poca variabilidad a lo largo del periodo de análisis^[se muestra la estimación de un TVP-VAR(1) en vez de un TVP-VAR(2), dado que los resultados no se modifican y se facilita su visualización]. Este resultado es común en la literatura de TVP-VAR [@Lubik2016b], sin embargo, como se noto en el párrafo anterior si existe variabilidad en las innovaciones.

El resultado es robusto frente a diferentes especificaciones, con variables en niveles o en logaritmos los resultados no cambian. Adicionalmente se estimo un modelo bivariado con desempleo y vacantes, tanto en niveles como en logaritmos, obteniendo las mismas conclusiones. Se podría pensar en el uso de un modelo de parámetros fijos y volatilidad estocástica, sin embargo, en la medida que el TVP-VAR no impone la restricción de parámetros fijos pero se obtiene dicho resultado no parece necesario.

```{r FIR, fig.cap="FIR", out.width='1\\linewidth'}
notas = "FIR desde producto hacia el índice de vacantes (A) y tasa de desempleo (B). La linea negra es la mediana, mientras las áreas grises refieren a los intervalos de confianza al 5-95\\% y 25-75\\%. Con color rojo la FIR de un VAR de parámetros fijos. Los signos de la mediana del TVP-VAR se ajusta a lo que se espera de un shocks desde el producto a vacantes y desempleo. En el primer caso un efecto positivo y en el segundo un efecto negativo. En el caso de un VAR los resultados son menos claro con valores en torno a cero."
# Tengo que modificar plot_irf para que sea en español y que los label de eje y estén rotados en 90 grados. O sino pasarlo a ggplot.
par(mfrow = c(1, 2))
plot_irf(impulse = 1, response = 2, .main = "Panel A. Vacantes")
plot_irf(impulse = 1, response = 3, .main = "Panel B. Desempleo")
```

Por último analizamos las FIR mediante la estimación de la mediana. Su visualización no es trivial, en la medida que en cada momento del tiempo existe una FIR. Una opción es visualizar distintos momentos y observar si existen diferencia (la que se elige), otra es mostrar una visualización en tres dimensiones. En nuestro caso, las FIR en los distintos momentos del tiempo se mantienen prácticamente iguales.

En la Figura \@ref(fig:FIR) Panel A tenemos shocks desde el producto hacia las vacantes laborales el efecto es positivo en todo momento, esto tiene sentido en la medida que una innovación de productividad, debería generar que la demanda laboral de las empresas se vea aumentado, en la medida que crezca el nivel de producción de la economía. Al contrario en la Figura \@ref(fig:FIR) Panel B observamos como el efecto del shock genera un efecto negativo en todo momento sobre la tasa de desempleo. Al mejorar la productividad de la economía, el desempleo debería disminuir en la medida que la economía es capaz de aumentar su producción, lo cual lleva a que las empresas aumenten su contratación (más o menos dependiendo de cuan sesgado sea hacia el uso de tecnología y capital). Los efectos de entrada o salida de personas a la PEA esta presente en ambos indicadores.
<!-- MEJORAR LA EXPLICACIÓN Y ARGUMENTACIÓN -->
