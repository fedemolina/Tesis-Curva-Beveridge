# Resultados {#cap:Resultados}

```{r FuncionesTVPVAR}
impulse_response <- function (fit, impulse.variable = 1, response.variable = 2, 
    t = NULL, nhor = 20, scenario = 2, draw.plot = TRUE, ..main = .main) 
{
    Beta.draws <- fit$Beta.draws
    nd <- dim(Beta.draws)[3]
    if (is.null(t)) 
        t <- dim(Beta.draws)[2]
    out <- matrix(0, nd, nhor + 1)
    M <- fit$M
    p <- fit$p
    H.sel <- fit$H.draws[, ((t - 1) * M + 1):(t * M), ]
    A.sel <- fit$A.draws[, ((t - 1) * M + 1):(t * M), ]
    if (scenario == 3) {
        sig <- apply(exp(0.5 * fit$logs2.draws), 1, mean)
        sig <- diag(sig)
    }
    else {
        sig <- NULL
    }
    for (j in 1:nd) {
        if (scenario == 1) {
            H.chol <- NULL
        }
        else if (scenario == 2) {
            H.chol <- t(chol(H.sel[, , j]))
        }
        else if (scenario == 3) {
            H.chol <- t(solve(A.sel[, , j])) %*% sig
        }
        aux <- bvarsv:::IRFmats(A = bvarsv:::beta.reshape(Beta.draws[, t, j], 
            M, p)[, -1], H.chol = H.chol, nhor = nhor)
        aux <- aux[response.variable, seq(from = impulse.variable, 
            by = M, length = nhor + 1)]
        out[j, ] <- aux
    }
    if (draw.plot) {
        pdat <- t(apply(out[, -1], 2, function(z) quantile(z, 
            c(0.05, 0.25, 0.5, 0.75, 0.95))))
        xax <- 1:nhor
        matplot(x = xax, y = pdat, type = "n", ylab = "", xlab = "Horizonte", bty = "n", xlim = c(1, nhor))
        polygon(c(xax, rev(xax)), c(pdat[, 5], rev(pdat[, 4])), 
            col = "grey60", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 4], rev(pdat[, 3])), 
            col = "grey30", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 3], rev(pdat[, 2])), 
            col = "grey30", border = NA)
        polygon(c(xax, rev(xax)), c(pdat[, 2], rev(pdat[, 1])), 
            col = "grey60", border = NA)
        lines(x = xax, y = pdat[, 3], type = "l", col = 1, lwd = 2.5)
        abline(h = 0, lty = 2)
        title(main = ..main, cex.main = 0.6, adj = 0, line = 0)
    }
    list(contemporaneous = out[, 1], irf = out[, -1])
}
# Carga del modelo svar-sv
fit <- readRDS(here::here("Datos", "Finales", "modelo.rds"))

# Función para generar los gráficos de parámetros y varianzas
matplot2 <- function(...) matplot(..., type = "l", lty = 1, lwd = 2, bty = "n", ylab = "")
stat.helper <- function(z) c(mean(z), quantile(z, c(0.16, 0.84)))[c(2, 1, 3)]
gp <- seq(1985, 2020, 5) # marks for vertical lines
# colors, taken from http://www.cookbook-r.com
cols <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
cols1 <- cols[c(2, 4, 2)]
make_plot <- function(.fit = fit, .type = "vcv", .var = 1, .title = "") {
    gp <- seq(1990, 2020, 5) # marks for vertical lines
    if(.type == "vcv") {
        # SD of unemployment residual
        # Get posterior draws
        sd_inf <- parameter.draws(.fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(sqrt(sd_inf), 2, stat.helper))    
    } else if (grepl(x = .type, pattern = "lag")) {
        beta <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1   <- t(apply(beta, 2, stat.helper))    
    } else {
        beta_0 <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(beta_0, 2, stat.helper))       
    }
    xax <- seq(1990, 2018, length.out = NROW(x1)) # x axis
    # Plot
    if(.type == "vcv") {
        var <- sd.residuals.ols[.var]
    } else {
        var <- NULL
    }
    matplot2(x = xax, y = x1, ylim = c(min(x1), max(x1)), col = cols1, main = .title , xlab = "")
    # abline(h = seq(min(x1), max(x1), length.out = 10), v = gp, lty = 4, lwd = 0.3)
    if(.type == "vcv") {
        abline(h = var, col = cols[1], lwd = 1.4, lty = 5)
    }
}

# Función para generar los IRF
plot_irf <- function(.fit = fit, impulse, response, scenario = 2, .main = "") {
    ira <- impulse_response(fit, impulse.variable = impulse, response.variable = response, scenario = scenario, ..main = .main)
    # OLS impulse responses for comparison
    ira.ols <- irf(fit.ols, n.ahead = 20)[[impulse]][[response]][-1, 1]
    # Add to plot
    lines(x = 1:20, y = ira.ols, lwd = 1, lty = 5, col = "red")
}

# carga del modelo var
library(vars)
fit.ols <- readRDS(here::here("Datos", "Finales", "modelo-var-comun.rds"))
sd.residuals.ols <- apply(residuals(fit.ols), 2, sd)
```
<!-- Este capítulo presenta los resultados principales del trabajo. Primero se grafica la curva de Beveridge entre 1981 y 2018, se hace una partición por décadas para visualizar potenciales etapas. Realizamos una exploración de la relación entre vacantes laborales y el IVF del PIB y tasa de desempleo con producto. -->
<!-- Posteriormente analizamos las series utilizadas y sus propiedades estadísticas, tales como raíces unitarias, raíces estacionales y cointegración utilizando distintos test estadísticos, resultados que pueden consultarse en el anexo. -->
<!-- A continuación planteamos una búsqueda de quiebres estructurales mediante distintos tipos de test estadísticos, como procesos de fluctuación, test F y dating. Con los test de tipo dating logramos encontrar la cantidad de quiebes estructurales óptimos dada una función objetivo con lo cual obtenemos diferentes periodos de análisis. -->
<!-- Posteriormente estimamos un TVP-VAR con volatilidad estocástica, donde analizamos la evolución de los rezagos de las variables endógenas y los desvíos estándar de la matriz de varianzas y covarianzas. De esta forma analizamos si el PGD contiene modificaciones (suaves) en los parámetros bajo un modelo multivariado.  -->
<!-- Finalmente computamos las FIR para analizar el efecto que tienen shocks estructurales desde el producto hacia la tasa de desempleo y el índice de vacantes laborales. -->
<!-- Todo el proceso de quiebres estructurales y estimación de TVP-VAR ha sido realizado en el lenguaje _R_ utilizando los paquetes strucchange [@Zeileis2002], fxregime [@Zeileis2010] y bvarsv [@Kruger2015]. -->
<!-- \newpage -->
## Índice de vacantes

```{r include_data, include = FALSE}
# Base de datos final
dt <- readRDS(here::here("Datos", "Finales", "series_version_final.rds"))

# Serie iecon. Se usa en FUENTES y CRÍTICAS
iecon <- haven::read_dta(here::here("Datos", "Originales", "Gallito-2000-2009.dta"))
iecon$fecha <- as.Date(paste(iecon$aniog,iecon$mesg, iecon$diag, sep = "-"))
iecon_ts <- iecon %>% 
    dplyr::group_by(aniog, mesg) %>% 
    dplyr::summarise(puestos = sum(puestos, na.rm = TRUE),
                            avisos = dplyr::n())
iecon_ts$fecha <- as.Date(paste(iecon_ts$aniog, iecon_ts$mesg,"1", sep = "-"))
iecon_ano <- iecon %>% 
                dplyr::group_by(fecha = lubridate::make_date(aniog)) %>%                          dplyr::summarise(puestos = sum(puestos, na.rm = TRUE),
                                 avisos = n())

# Serie de CERES. Utilizada en FUENTES:Ceres
ceres <- readxl::read_xls(here::here("Datos", "Originales", "ICDL-1998-2014.xls"),
                          col_names = TRUE, sheet = "serie", 
                          col_types =c("date","numeric"),
                          readxl::cell_cols("A:B"))
ceres_ts <- ts(data = ceres[,2], start = c(1998,3), frequency = 12)
ceres_ano <- 
    ceres %>% 
        dplyr::group_by(fecha = lubridate::make_date(lubridate::year(fecha))) %>%
        dplyr::summarise(ind_vacantes = mean(vacantes, na.rm = TRUE))

```
<!-- ## Construcción indicador de vacantes  -->
Hasta los años 2000 la única fuente relevante de avisos laborales en papel fue _Gallito_, por lo cual basta con recabar dicha información para tener una muestra representativa de los avisos laborales.^[En Uruguay es posible visitar la Biblioteca Nacional donde por ley deben guardarse copias de todas las publicaciones de prensa. Al consultar a los encargados no se encuentran otras fuentes de publicaciones laborales para el departamento de Montevideo. Esta preponderancia de _Gallito_ sumado a la ausencia de encuestas es una de las causas de no seguir la metodología utilizada en @Barnichon2010] Sin embargo, con la revolución de Internet y los portales web comienza a perder representatividad. A partir de allí el problema se divide en tres: 1) Cuánta representatividad pierde 2) A partir de año comienza a perderla 3) Para responder 1 y 2, es necesario definir que otras fuente relevantes aparecen y la población de avisos a utilizar, la cual debe ser representativa de todos los portales laborales.

Las nuevas fuentes laborales consideradas en Uruguay son _Buscojobs_ portal laboral que nace a partir del año 2007 y rápidamente logra obtener un cuota relevante del mercado.^[Los datos de _Uruguay Concursa_ se obtienen indirectamente a partir de _Buscojobs_ debido a que dicha página los incluye entre sus publicaciones] _Computrabajo_ que opera en Uruguay a partir del año 2003 y, _Uruguay Concursa_ portal laboral que centraliza todos los llamados de empleos públicos.^[La página omitida más relevante es _LinkedIn_, sin embargo, su participación comienza en los últimos cuatro o cinco años y según la encuesta uso de Internet no más del 15\% de personas la consultan.] La preponderancia de _Gallito_ se mantiene hasta 2008-2009, luego su participación decrece sistemáticamente hasta estabilizarse en 40\% del total de publicaciones.
En la Figura \@ref(fig:series-conjuntas) se observa las series trimestrales con las que se construye el índice de vacantes para el periodo 1980-2019. El proceso se detalla en la sección siguiente.

```{r series-conjuntas, fig.cap = "Series trimestrales sin corregir"}
notas = "Series individuales de avisos laborales sin corregir utilizadas en la creación del índice de vacantes laborales. La serie \\textbf{um} corresponde a la serie combinada de (ref:Urrestarazu1997) de los años 1980-1995 y una serie de elaboración propia desde 1995 a 2001-Q1. La serie \\textbf{Ceres} son los avisos obtenidos a partir del Índice Ceres de Demanda Laboral (ICDL), facilitada por el Centro de Estudios de la Realidad Económica y Social (CERES). \\textbf{Gallito} refiere a los avisos de \\textit{Gallito} entre 2013 y 2018. \\textbf{Buscojobs} y \\textbf{Computrabajo} son los avisos obtenidos de los portales laborales \\textit{Buscojobs} y \\textit{Computrabajo} respectivamente. \\textbf{Gallito-BN} (linea punteada gris) corresponde a los avisos trimestrales de \\textit{Gallito} de recolección propia. Todas las series graficadas son de construcción propia. Se observa una caída desde 1980 hasta 1983 debido a la crisis de la \\textit{tablita} y luego un aumento permanente hasta los 2000 (con una caída importante 1995 por la \\textit{crisis del tequila}) de la cantidad de avisos laborales. Luego hay una disminución abrupta debido a la crisis de 2002, seguido de un aumento hasta 2012 y posteriormente una caída permanente hasta 2019, a excepción de las las series de Buscojobs y Computrabajo muestran un crecimiento casi constante. Las series \\textbf{um}, \\textbf{Computrabajo} y \\textbf{Buscojobs} tienen imputaciones de datos faltantes."
fuentes = "Se utilizan datos de (ref:Urrestarazu1997) obtenidos de \\textit{Gallito}, datos de elaboración propia (1995-1998, 1999Q1, 2000, 2001Q1, 2009, 2010, 2011, 2012, 2013, recolectados de \\textit{Gallito} en biblioteca nacional. Base de datos de \\textit{Gallito} (2013-2018) facilitada por diario El País y datos de scraping web de los portales laborales \\textit{Buscojobs}, \\textit{Computrabajo} y \\textit{Gallito}."

col_um           = "black"
col_ceres        = "darkorange"
col_ga           = "darkgreen"
col_ct           = "darkblue"
col_bj           = "red"
col_muestreos    = "gray"
colores <-  c(col_um, col_ceres, col_ga, col_ct, col_bj, col_muestreos)
long_dt <- melt.data.table(data = dt, id.vars = "fecha", 
                measure.vars = c("av_urr_mol", "av_ceres", "av_ga_s_dup", 
                                 "av_ct_s_dup", "av_bj_s_dup", "muestreos"),
                variable.name = "series", value.name = "avisos")
ggplot(long_dt, aes(x = fecha, y = avisos, color = series)) +
    geom_line(linetype = "dashed") +
    geom_point(data = long_dt[series == "muestreos",], aes(x = fecha, y = avisos)) +
    scale_color_manual(values = colores, 
                       labels = c("um", "Ceres", 
                                  "Gallito", 
                                  "Computrabajo", "Buscojobs", 
                                  "Gallito-BN"),
                       name = "") +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "5 year", date_labels = "%Y") +
    labs(x = "Fecha", y = "Avisos") +
    theme_Publication(position_legend = "bottom")
```

### Gallito

Para compatibilizar *Gallito* primero se obtienen los datos de @Urrestarazu1997. Segundo, se recaba de forma exhaustiva los avisos laborales existentes en las publicaciones semanales de _Gallito_ entre octubre de 1995 y agosto de 1998. Tercero, se utilizan los datos de @Ceres2012. Dada la falta de especificaciones, se contrasta con la información generada por @Alma2011 con la cual se construye un índice de frecuencia anual. La correlación lineal entre ambos índices es de 97\%, mientras sus tasas de crecimiento correlación lineal de 90\%. El resultado puede observarse en la Figura \@ref(fig:iecon-ceres). Cuarto, se recolectan muestras trimestrales de avisos entre 1999 y 2013 (denotados _gallito-BN_), de forma tal de corroborar la hipótesis que el ICDL debe estar generado a partir de publicaciones de _Gallito_.^[El término _gallito-BN_ refiere a recolecciones realizadas en la biblioteca nacional en los trimestres de 1999, 2000, 2009, 2010, 2011, 2012, 2013 y 2014 los cuales pueden verse en la Figura \@ref(fig:series-conjuntas) con color gris] En la Figura \@ref(fig:series-conjuntas) se ve como los _gallito-BN_ siguen el índice hasta el año 2012, a partir de allí se genera una diferencia en torno a los 1500-2000 avisos. Se asume que a partir de dicho año o bien cambio la metodología o se sumaron nuevas fuentes de datos. Por último se utiliza una base de datos confidencial entregada por el diario El País entre 2013 y 2018 y se realiza scraping de la página web _Gallito_ entre 2018 y 2019 de forma mensual. A continuación se analiza con detalle.

Para despejar los avisos totales entre 1980 y 1995 de la serie de @Urrestarazu1997 ($v_t=\frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}$) se debe utilizar la *PEA* calculada por @Urrestarazu1997, posteriormente se debe calcular dicha variable desde 1995 hasta 2019 y finalmente unir ambas estimaciones.

Se calcula la PET desde 1997 con frecuencia trimestral a partir de la ECH, se estiman los valores ausentes en 1996 y 1997 y se obtiene $pet_{96-18}$. La estimación es un promedio no ponderado de una interpolación mediante splines y un modelo básico estructural estimado mediante un filtro de Kalman suavizado [@Durbin2013;@Moritz2017].
Segundo, se trimestraliza la serie anual de PET en Montevideo, usando el método de denthon-chollete [@Sax2013], con promedios, usando como serie guía la PET trimestral en el paso previo. Tercero la PEA, se obtiene usando la TA publicada por el INE, por definición $TA = PEA/PET$.
Cuarto se repondera la PEA de 1981-1995 y se unen las series. Finalmente se estiman los valores de 1980-1981 mediante un promedio simple de una estimación mediante un modelo estructural básico estimado por filtro de Kalman suavizado y un modelo Arima en representación de espacio estado.

<!-- ### 1980 -->
El objetivo es obtener una aproximación de la cantidad de avisos totales, por ello primero es necesario obtener $a_t$ desde el índice de vacantes Urrestarazu. Sin embargo, la base de dicho índice es 1980 y los datos disponibles en el trabajo son desde 1981 en adelante. El índice esta expresado en función de la PEA:
\begin{equation}
v_t = \frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}
\end{equation}

No se conoce $a_0$ ni $PEA_0$, ni se tienen datos de 1980. Por lo cual, se siguen los siguientes pasos:

1. Obtener $a_t$ en algún punto: $a_{1995_q4}$\footnote{La cantidad de avisos laborales del cuarto trimestre de 1995, fueron obtenidos a partir de las publicaciones del diario El País, sección Gallito}.
2. Imputar la PEA hacia atrás, obteniendo $pea_{t=0}$.
3. Despejar $a_{t=0}$ de 
$v_t$ = $(a_t/pea_t)/(a_{t=0}/pea_{t=0})100$.
4. Obtener la base $\alpha$ = $a_{t=0}$/$pea_{t=0}$.
5. Imputar las vacantes hacía atrás.
6. Calcular la serie de avisos

Se opta por realizar una predicción hacia atrás, planteando un modelo básico estructural mediante un filtro de Kalman suavizado [@Durbin2013].^[Se plantearon formulaciones alternativas, como imputación mediante filtro de Kalman sin suavizar, con 3 tipos de modelos estructurales: nivel-local, tendencia y básico estructural [@Harvey1989], además de modelos Arima.] A partir de ahora se trabaja con una serie trimestral de avisos.

<!-- ### 95-98 -->
A continuación se elige unir la serie de avisos $a_t^{80-95}$ con las publicaciones recolectados entre 1995 y 1998, dado que se utiliza $a_{1995}^{q4}$ para poder despejar la cantidad de avisos $a_t^{80-95}$ las series coinciden por construcción en el cuarto trimestre de 1995, por lo cual pueden ser unidas sin realizar ningún calculo adicional, la serie obtenida se denota $a_{um}^{80-98}$. Como se puede observar en la Figura \@ref(fig:series-conjuntas) las series combinadas mantienen el mismo nivel, aunque llama la atención la caída que se da entre 1995 y 1996. Sin embargo, dicha disminución no es causada por la combinación de las series, la misma comienza en el índice de vacantes de @Urrestarazu1997 donde los últimos cuatro trimestres tienen respectivamente una caída interanual de -17.3\%, -29.4\%, -25\% y -18.4\% debido a la crisis del tequila. A partir de 1996 se revierte iniciandose un período de crecimiento que se mantiene por 3 años.

<!-- ### 98-14 -->
Se utiliza el ICDL, $a_{ceres}^{98-14}$, de frecuencia mensual y sin factores estacionales. El índice tiene la forma $v_t = a_t/a_0$, el problema es que observamos $\tilde{v_t}$ debido a la desestacionalización y no conocemos $a_0$. Aún conociendo $a_0$, no es posible recobrar el verdadero $a_t$, pero es posible encontrar una aproximación que mantenga el mismo movimiento y tendencia.

La serie es trimestralizada y se analiza si es necesario realizar alguna corrección. Previamente valido la hipótesis que el ICDL corresponde, al menos hasta 2012, a publicaciones de _Gallito_. Suponemos que el proceso generador de datos es el mismo. La diferencia radica en que ha dicha serie le ha sido extraido (al menos) el componente estacional. 

Si suponemos que nuestro proceso generador de datos es:^[Basandonos en @Harvey1989]
\begin{equation}
Y_t = S_t + T_t + C_t + I_t
\end{equation}
Y observamos que $a_t^{ceres}$ es $T_t + C_t$ el hecho de que ajustemos el nivel de la serie mediante una corrección, por ejemplo, una reponderación, quiere decir que estaríamos reponderando la tendencia-ciclo de la serie. En la Figura \@ref(fig:series-conjuntas), tanto los _Gallito-BN_ como la serie $a_{um}^{80-98}$ acompañan el nivel de $a_t^{ceres}$, a tal punto que en algunos trimestres prácticamente coinciden. Suponemos que dichas diferencias se asocian al filtro aplicado, por lo cual se opta por unir las series a partir de 1998-III sin realizar ninguna corrección.^[Adicionalmente, se genera una serie de estacionalidad entre 1995 y 2018 utilizando los avisos recolectados entre 1995-1998 y los datos entregados por El País. Se imputa la estacionalidad para el periodo 1998-2013 y se agrega dicho componente a los avisos de _Ceres_, los resultados no varían.]
Se trimestraliza, se realiza la unión y se obtiene $a_{umc}^{80-14}$.

<!-- ### 13-19 Ga -->
El paso siguiente es la creación de una serie mensual de publicaciones sin avisos repetidos, utilizando los datos proporcionados por el diario El País, entre 2013 y 2018. Dicha serie se une con los avisos obtenidos para el año 2019 mediante el scraping web de la página web _Gallito_ y posteriormente se trimestralizan los datos..^[La información de los últimos tres meses de 2018 se obtuvo desde @MTSS2018]

```{r comparacion-gallito, fig.cap="Comparación avisos laborales"}
comparacion_gallito <- data.table(
    avisos_bd_s_dup = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), av_ga_s_dup],
    avisos_bd_c_dup = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), av_ga_c_dup],
    avisos_papel    = dt[data.table::between(fecha, "2013-10-01", "2014-10-01"), muestreos],
    fecha = seq.Date(as.Date("2013-10-01"), as.Date("2014-10-01"), "quarter")
)
comparacion_gallito[, `:=`(ratio_s_dup = avisos_papel/avisos_bd_s_dup,
                           ratio_c_dup = avisos_papel/avisos_bd_c_dup,
                           diferencia_s_dup = as.integer(avisos_papel - avisos_bd_s_dup)#,
                           # pct_s_dup = avisos_bd_s_dup/avisos_papel,
                           # pct_c_dup = avisos_bd_c_dup/avisos_papel
                           )]
data.table::setnames(comparacion_gallito, old = names(comparacion_gallito), new = c("Avisos sin\n duplicados", "Avisos con\n duplicados", "Avisos\n papel", "Fecha", "Ratio sin\n duplicados", "Ratio con\n duplicados", "Diferencia sin\n duplicados"))
setkey(comparacion_gallito, "Fecha")
notas <- "\\\\footnotesize Comparación de avisos laborales \\\\textbf{Gallito Base de datos} y \\\\textbf{Gallito papel}. La primera columna \\\\textbf{Avisos sin duplicados} son los avisos del portal \\\\textit{gallito} facilitados por el diario El País, filtrados de avisos duplicados (link-ID repetido). \\\\textbf{Avisos con duplicados} son los mismos datos pero sin filtrar los avisos duplicados. \\\\textbf{Avisos papel} refiere a los avisos recolectados manualmente de \\\\textit{gallito} en la Biblioteca Nacional. \\\\textbf{Fecha} hace referencia al periodo de tiempo correspondiente. Las columnas \\\\textbf{Ratio sin duplicados} y \\\\textbf{Ratio con duplicados} indican los ratios con duplicados y sin duplicados (cociente) y corresponden a la división de la tercera columna con la primera o segunda respectivamente. La última columna, refiere a la diferencia entre la primera y segunda columna con respecto a la tercera. No hay grandes diferencias en los ratios con o sin duplicados. El rango del ratio sin duplicados se mueve entre 1.26 y 1.51 indicando que los avisos en papel sobreestiman la cantidad de publicaciones reales en la base de datos de diario El País.

\\\\textit{Fuentes}: Avisos recolectados en Biblioteca Nacional de \\\\textit{gallito} y base de datos de publicaciones de \\\\textit{gallito} facilitada por el diario El País."

kableExtra::kable(comparacion_gallito, 
                  format = "latex", 
                  align = "c", 
                  booktabs = TRUE, 
                  digits = 2, 
                  caption = "Comparación avisos laborales",
                  col.names = linebreak(colnames(comparacion_gallito)),
                  escape = FALSE) %>%
kableExtra::kable_styling(latex_options = "hold_position",
                          # latex_options = "scale_down", 
                          position = "center",
                          font_size = 10,
  ) %>%
  kableExtra::footnote(general = notas, #number = notas,
                       # number_title = "Fuentes", 
                       general_title = "Notas:", 
                       # footnote_as_chunk = F, 
                       threeparttable = TRUE, 
                       escape = FALSE,
                       footnote_order = c("general"))
```

Se utilizan los avisos laborales facilitados por el diario El País para el período 2013-2018, con ellos se construye una serie trimestral de la cantidad de publicaciones.
Dichos datos es la variable no observable que deseamos cuantificar en los períodos previos pero que observamos con errores de medición, por ello lo denotamos $\tilde{a_t}$. Para obtener cuanto difieren los avisos en papel de los avisos de la base de datos, se recolectan los siguientes trimestres: $a_{q4}^{13}$, $a_{q1}^{14}$, $a_{q2}^{14}$, $a_{q3}^{14}$, $a_{q4}^{14}$.\footnote{En todos los casos existen semanas, donde la publicación semanal no estaba disponible. Por ello, se modelizaron serie de frecuencia semanal las cuales fueron imputadas mediante la modelización de un modelo básico estructural estimado por un filtro de Kalman suavizado} Como se puede observar en el Cuadro \@ref(tab:comparacion-gallito) la diferencia entre $\tilde{a_t}$ y ${a_t}$ es significativa.

Se elige corregir la serie $a_{umc}^{80-14}$ de forma que la serie quede expresada en los mismos términos que los avisos actuales, $\tilde{a_t}$, por lo cual, es posible seguir extendiendo el periodo de análisis simplemente agregando futuras observaciones obtenidas mediante scraping. Se repondera la serie $a_{umc}^{80-14}$ en base al promedio de los avisos $a_{q2}^{14}$, $a_{q3}^{14}$, $\tilde{a}_{q2}^{14}$, $\tilde{a}_{q3}^{14}$, obteniendo $\tilde{a}_{umc}^{80-14}$.

Se utilizan los _gallito-BN_ para corroborar que la fuente del ICDL sea _Gallito_.
Dada la diferencia que se observa entre los _gallito-BN_ y $a_{ceres}^{98-14}$ a partir del año 2012, se considera dicha serie hasta el primer trimestre de 2012. El fundamento es que el ICDL coincide con los _gallito-BN_ realizados cuya diferencia puede ser atribuida a la desestacionalización del mismo, sin embargo, en 2013 y 2014 las diferencias son de un orden de magnitud tal que dicha hipótesis no puede ser mantenida. 
Como aproximación en base a las series disponibles se genera una serie de estacionalidad imputada desde 1995 hasta 2018 de frecuencia mensual plateando los siguientes modelos:

$$
\begin{aligned}
Y_t &= T_t + C_t + S_t + I_t \\
Y_t &= T_t C_t S_t I_t
\end{aligned}
$$
Se recupera el componente $S_t$ y $S_t + I_t$, se generan dos series, las cuales son imputadas entre 2001 y 2012. En ningún caso los valores absolutos de la estacionalidad superan los 600 avisos, si eso se trimestraliza la diferencia máxima que se observa es inferior a 1000 avisos.

Finalmente, se unen las series imputando los valores trimestrales $a_{q2}^{12}$, $a_{q3}^{12}$, $a_{q4}^{12}$, $a_{q2}^{13}$. Se divide la serie en estaciones (trimestres) y se realizan imputaciones en cada serie por separado, utilizando el filtro de Kalman suavizado. De esta forma se obtiene la serie final correspondiente a _Gallito_ denotada como $a_{umcg}$, la misma se puede observar en la Figura \@ref(fig:serie-final-gallito) con color azul (umcg).

```{r serie-final-gallito, fig.cap = "Serie de avisos final"}
notas = "Serie trimestral de avisos laborales. La serie \\textbf{umcg} refiere a la serie final de gallito obtenida de la combinación de (ref:Urrestarazu1997), datos de recolección propia entre 1995-2001, avisos obtenidos del Índice Ceres de Demanda Laboral (ICDL) y avisos de \\textit{gallito}. \\textbf{Ceres} son los avisos obtenidos del ICDL. \\textbf{Gallito} es la serie generada a partir de \\textit{gallito} entre 2013 y 2018. Y \\textbf{um} es la serie combinada de Urrestarazu y de recolección propia. La diferencia de nivel se explica porque se reescalaron las cantidades de avisos desde 2013 hacia atrás para que correspondan a los avisos de la base de datos de el diario El País (\\textbf{Gallito})."
fuentes = "1980-1995 datos extraídos de (ref:Urrestarazu1997), entre 1995 y 2001 recolección propia, 1998-2012 se utilizan los datos del Centro de Estudios de la Realidad Económica y Social (CERES), entre 2013 y 2018 datos provistos por diario El País referidos a publicaciones de \\textit{gallito}."
# Generar otra serie urr_mol pero sin la predicción, agregarla acá.
long_dt <- melt.data.table(data = dt, id.vars = "fecha", 
                measure.vars = c("av_umcg", "av_ga_s_dup", 
                                 "av_urr_mol", "av_ceres"),
                variable.name = "series", value.name = "avisos")
ggplot(long_dt, aes(x = fecha, y = avisos, color = series)) +
    geom_line(linetype = "dashed") +
    scale_color_manual(values = c("darkblue", col_ga, col_um, col_ceres),
                       labels = c("umcg", "Gallito", "um", "Ceres"),
                       name = "") +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_date(date_breaks = "5 year", date_labels = "%Y") +
    labs(y = "Avisos", x = "Fecha") +
    theme_Publication(position_legend = "bottom")
```

### Buscojobs

Los publicaciones laborales del portal *Buscojobs* se recaban desde 2007/06 hasta 2018/12 a través de la página web _Waybackmachine_. Se contabilizaron todas las publicaciones mensuales encontradas que hacen referencia al total de avisos publicados, los cuales son promediados mensualmente. Es decir, cada punto refiere a un año-mes y el total de avisos de publicados en dicho momento.

En el año 2019 se realiza scraping sobre el portal _Buscojobs_ extrayendo de forma mensual todos los avisos laborales disponibles y la información completa de cada aviso. La serie se modela con frecuencia mensual y los valores faltantes son imputados removiendo el componente estacional de la serie, imputando la serie desestacionalizada y luego agregando el componente estacional. Por último la serie es trimestralizada, la misma puede obvervarse en la Figura \@ref(fig:serie-buscojobs).

### Computrabajo

Las publicaciones laborales del portal *Computrabajo* referidas al total de avisos publicados, se promedian los meses en que existen múltiples observaciones y se obtiene una serie de frecuencia mensual. Se agregan los datos de scraping desde octubre de 2018 hasta diciembre de 2019. Dado que _Computrabajo_, muestra una cantidad de avisos totales diferente de la cantidad de avisos publicados en los últimos treinta días es necesario realizar una corrección, en caso contrario se estará sobrestimando los avisos publicados. De los datos obtenidos, se observa que los avisos publicados en los últimos treinta días representan un 40-43\% del número de avisos totales publicados que muestra el portal. A partir de julio de 2011, los avisos de _Computrabajo_ son reponderados por 0.42 y en las fechas previas por 0.63.^[Esta decisión ad-hoc se toma puesto que a partir de julio de 2011 la serie de _Computrabajo_ comienza a tener un crecimiento exponencial, además en las fechas previas las cantidades de avisos son bajas comparativamente, por lo cual es menos probable que difieran la cantidad de avisos mostrados de los efectivamente publicados en los últimos treinta días.] La serie final se observa en la Figura \@ref(fig:serie-computrabajo).

### Unión

Las tres páginas web de las cuales se extrajeron los datos difieren en la estructura con la cual presentan los avisos laborales. El mismo aviso se presenta de forma diferente y se pueden encontrar variaciones en cuanto a la información brindada. Por ejemplo, puede que el nombre del puesto y la empresa difieran levemente, que no se brinde información respecto a la empresa, o se haga referencia a "importante empresa".

Para poder identificar empresas compartidas por portales, primero se limpia el texto, se borran _stopwords_ [@Jurasky2019], se buscan patrones similares y finalmente se hace un filtro manual del cual se mapean 452 valores contenidos en una matriz de dimensión $\Re^{207*6}$ hacia un vector $x$ de dimensión $\Re^{207}$.

Finalmente, las tres series son combinadas realizando un análisis del texto de los avisos laborales de forma de obtener el porcentaje de avisos compartidos entre páginas. Siguiendo a @Jurasky2019 y utilizando el paquete text2vec [@text2vec] se construye un document-term matrix (DTM) con los avisos laborales, se vectoriza el texto mapeando palabras (1-gram) hacia un espacio vectorial, para esto es necesario crear un vocabulario común. Posteriormente, se calcula la similaridad de coseno:

$$
similaridad(doc1, doc2) = \cos(\theta) = \frac{doc1doc2}{|doc1||doc2|}
$$
entre los 3 portales web, obteniendo que los portales que más comparten avisos son _Computrabajo_ y _Buscojobs_, en torno a un 9\%, mientras con "El gallito" es torno a 3\%. Se combinan primero los avisos entre _Computrabajo_ y _Buscojobs_ ajustados por el \% de avisos compartidos, luego se combina con _Gallito_ en base al \% compartido. El indicador de cantidad de avisos final y su estimación en tendencia-ciclo entre 1980 y 2019 se puede ver en la Figura \@ref(fig:IndiceAvisosTC). Por último se calcula el índice de vacantes entre 1980 y 2018 como $v_t=\frac{\frac{a_t}{a_0}}{\frac{PEA_t}{PEA_0}}$ tomando como base el año 2010. El resultado se visualiza en la Figura \@ref(fig:IndiceVacantesAvisos).^[El índice de vacantes, contiene un año menos que el indicador de avisos en la medida que se utiliza la ECH compatibilizada por el IECON la cual finaliza en 2018, por lo cual no se tiene el valor de la PEA para el año 2019. Obtenido dicho valor, se seguirá expandiendo el índice.] 

```{r IndiceVacantesAvisos, fig.cap = "Índice de vacantes laborales 1980-2018"}
notas = "Serie trimestral \\textbf{índice de vacantes laborales} e \\textbf{índice de avisos laborales}. El índice de vacantes laborales corresponde a la linea solida, tiene base 2010 y esta normalizado por un índice de  población económicamente activa (PEA) con base 2010. La linea punteada es el índice de avisos con base 2010. Las series tienen frecuencia trimestral. Las caídas observadas entorno a 1982 y 2002 corresponden a crisis económicas. La disminución en 1995 se debe a la crisis del tequila. Las series tienen la misma dinámica y leves diferencias. La serie de avisos termina en 2019 mientras la de vacantes en 2018. Esto se debe a que se utilizó la encuesta continua de hogares (ECH) compatibilizada por el Instituto de Economía (IECON) para calcular la PEA, la cual finaliza en 2018. Ambas series son de construcción propia."

fuentes = "1980-1995 datos extraídos de (ref:Urrestarazu1997), entre 1995 y 2001 recolección propia, 1998-2012 se utilizan los datos del Centro de Estudios de la Realidad Económica y Social (CERES), entre 2013 y 2018 datos provistos por diario El País referidos a publicaciones de \\textit{gallito}. Además se utilizan los datos de \\textit{Buscojobs} y \\textit{Computrabajo} bases de datos generadas propiamente. Los datos de \\textit{Buscojobs} contienen la información del portal laboral \\textit{Uruguay Concursa}. También se utiliza la ECH compatibilizada por el IECON desde 1980 a 2018."


dt[, ggplot(.SD, aes(x = fecha)) + 
       geom_line(aes(y = ind_vac), linetype = "solid", color = "black") + 
       geom_line(aes(y = ind_av), color = "black", alpha = 1, size = .4, linetype = "dashed") +
       scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
       scale_x_date(date_breaks = "10 year", date_labels = "%Y") +
       # geom_point(aes(y = ind_av), color = "black", alpha = 1, size = .7) +
       geom_text(aes(label = "- - - Índice avisos", y = 0.35, x = as.Date("2015-06-01")), color = "black") +
       geom_text(aes(label = "--- Índice vacantes", y = 0.25, x = as.Date("2016-01-01")), color = "black") +
       labs(y = "Avisos", x = "Fecha") +
       theme_Publication()
   ]
```

## Curva de Beveridge

```{r beveridge-curve, fig.cap="Curva de Beveridge 1981-2018", fig.ncol = 2, fig.height=5}
#out.width='.49\\linewidth'
# fig.fullwidth = TRUE
# fig.subcap=c('Curva de Beveridge trimestral', 'Curva de Beveridge anual')
notas = "Curva de Beveridge 1981-2018, Montevideo. El Panel A es la Curva de Beveridge (CB) con frecuencia trimestral, mientras el Panel B es la CB con frecuencia anual. Se observa una curva con pendiente negativa y traslados paralelos en ambas direcciones. La década de 1980 tiene un movimiento de U por la crisis de la \\textit{tablita} de 1982. El periodo de 1990 muestra una transición hacia un punto más alejado del origen. El movimiento de U se vuelve a repetir entre 1999 y 2005 por la crisis económica de 2002. El periodo de 2010 hasta 2008 son movimientos sobre la curva. Luego parece haber un traslado hacia el origen y desde 2011 en adelante hay una caída de las vacantes pero con una tasa de desempleo estable (Panel B)."
fuentes = "Compatibilización de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE). Índice de vacantes de elaboración propia."
# Reordenar década
dt$decada <- factor(dt$decada, levels = c(80, 90, 2000, 2010))

ochenta        = "gray"
noventa        = "orange"
dos_mil        = "darkgreen"
dos_mil_diez   = "skyblue"
color_decada <-  c(ochenta, noventa, dos_mil, dos_mil_diez)

# y_lim <- c(min(dt$ind_vac)+1, max(dt$ind_vac)+1)
y_lim <- c(0.1, 1.2)
# x_lim <- c(min(dt$td)+1, max(dt$td)+1)
x_lim <- c(5, 20)
p1 <- ggplot(dt[data.table::between(fecha, "1981-01-01", "2018-10-01"),], aes(y = ind_vac, x = td)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = y_lim) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10), limits = x_lim) +
    geom_path() +
    # scale_color_manual(name = "", values = color_decada) +
    # coord_fixed(ratio = 15) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = "Panel A. Datos trimestrales") +
    theme_Publication()

p3 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
         .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), 
           ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), 
         keyby = .(ano)] %>%
    ggplot(., aes(y = ind_vac, x = td, label = ano2)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10), limits = y_lim) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10), limits = x_lim) +
    geom_path() +
    # position=position_jitter(width=.2,height=.02)
    ggrepel::geom_text_repel(fontface = "bold", size = 2, position=position_jitter(width=.1,height=.02), color = "black") +
    # scale_color_manual(name = "", values = color_decada) +
    # coord_fixed(ratio = 15) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = "Panel B. Datos Anuales") +
    theme_Publication()
library(gridExtra)
library(grid)
# grid.arrange(p1,                             # First row with one plot spaning over 2 columns
#              arrangeGrob(p2, p3, ncol = 2), # Second row with 2 plots in 2 different columns
#              nrow = 2
#              )
# plot_grid_split(p1, p3)
library(patchwork)
# grid.arrange(p1, p3, nrow = 1)
(p1 + p3)
  
# lista = list(p1, p3)
# lista[[1]]
# cat('\n\n') 
# lista[[2]]
```

El primer hallazo del trabajo se observa en la Figura \@ref(fig:beveridge-curve) Panel A, es una relación negativa entre vacantes y desempleo en linea con la literatura económica. Se observa que la curva ha tenido traslados tanto hacia afuera como hacia el origen lo que debería relacionararse a modificaciones estructurales de distinta indole o bien shocks. También existen movimiento diagonales relacionados al ciclo económico.

En la Figura \@ref(fig:beveridge-curve) Panel B observamos los promedio anuales del índice de vacantes y tasa de desempleo. Aquí la década de 1980, esta en un cuadrante inferior hacia la izquierda en contraposición a la década del 2000 que es la más alejada del origen. Las observaciones de la década de 1990, se alejan del origen de forma oscilante, mientras las observaciones de los años 2010 se mueven de forma descendente y con una elevada pendiente moviendose entorno al 6-8% de desempleo. Siendo la década con menor variabilidad en la tasa de desempleo.

```{r td-vac-pib, fig.cap="Producto-Vacantes y Producto-Desempleo (1981-2018)", fig.ncol = 2, fig.height=4}
# , out.width='.49\\linewidth', fig.asp=1, fig.ncol = 2, fig.fullwidth = TRUE
notas = "Relación entre producto-vacantes y producto-desempleo, datos desde 1981 hasta 2018 con frecuencia trimestral. En el panel A se observa el producto y el índice de vacantes, hasta el año 2012 la correlación es positiva, luego negativa. En el panel B observamos el producto y la tasa de desempleo. Las décadas de 1980 y 2000 muestran un comportamiento similar, una forma de U que hace referencia las crisis de 1982 y 2002."
fuentes = "Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE). La serie de producto es elaborada por el Banco Central del Uruguay y fue facilitada por el Centro de Investigación Económica (CINVE). Índice de vacantes de elaboración propia."
p1 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), keyby = .(ano)
   ] %>% 
ggplot(., aes(x = ind_vac, y = pib, label = ano2)) +
    geom_point() +
    geom_path() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(0.1, 1.2)) +
    # scale_color_manual(name = "", values = color_decada)+
    coord_fixed(ratio = 1/70) +
    labs(x = "Índice de vacantes", y = "IVF PIB", title = "Panel A. PIB-Vacantes") +
    ggrepel::geom_text_repel(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme_Publication()

p2 <- dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
         .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = unique(decada), ano2 = gsub(pattern = "\\d{2,2}(\\d{2,2})", replacement = "\\1", x = ano)), 
         keyby = .(ano)
   ] %>% 
    ggplot(., aes(x = td, y = pib, label = ano2)) +
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8), limits = c(5, 18)) +
    geom_path() +
    # scale_color_manual(name = "", values = color_decada)+
    coord_fixed(ratio = 1/6) +
    labs(x = "Tasa de desempleo", y = "IVF PIB", title = "Panel B. PIB-Desempleo") +
    ggrepel::geom_text_repel(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme_Publication()
(p1 + p2)
```

En la Figura \@ref(fig:td-vac-pib) Panel A se observa la relación entre vacantes y PIB, en la cual existe una correlación positiva hasta el año 2012 momento en que se vuelve negativa. La tasa de variación negativa en que caen las vacantes laborales entre 2011 y 2018, es prácticamente la misma entre 1998-2002 y 1981-1983, sin embargo el nivel de actividad no cae en ningún momento por lo que no se observa el movimiento de U típico de 80-82 y 98-2002. 
<!-- Esta observación en conjunto con el análisis de la CB podría estar indicando que el aumento del desempleo de los últimos años no se debe a mayores fricciones del mercado laboral, sino al desempleo estructural asociado a la diferencia sistemática entre las habilidades requeridas por el mercado y las ofrecidas por los trabajadores. Además debería haber un componente de cambio tecnológico e inversión en capital lo cual genera una menor demanda de trabajadores. -->

En la Figura \@ref(fig:td-vac-pib) panel B observamos transición desde una década de crecimiento con alto desempleo, hacia otra con crecimiento y caída de la desocupación (2000). Además, los años 1980 y 2000 vuelven a compartir la forma de U, solo que esta vez es en sentido contrario. La década de 2010 muestra a diferencia del gráfico anterior un comportamiento similar a los 90, observandose crecimiento económico con crecimiento del desempleo, aunque el nivel de actividad no cae en ningún momento. <!-- Es llamativo que periodos con políticas laborales tan diferentes como los 90 y 2010 tengan un comportamiento similar en cuanto al crecimiento de la actividad y aumento del desempleo. -->

## Quiebres estructurales

A continuación, ponemos a prueba la hipótesis de existencia de algún quiebre estructural en la relación vacantes y desempleo y buscamos, en caso de existir, la fecha de los quiebres.

Planteamos:
\begin{equation}
log(ind\_vac_i) = \beta_i + \beta_i\log(td_i) + \epsilon_i
\end{equation}

Y sometemos a prueba:
\begin{align}
H_0: \beta_i &= \beta_0 \ \ \ (i = 1, ..., n) \\
H_1: \beta_i &\not= \beta_0 \ \ \ (i = 1, ..., n)
\end{align}

Los resultados se pueden ver en el Cuadro \@ref(tab:quiebres), donde se han llevado a cabo los test de fluctuación generalizada. En la columna _Test_, se identifican los test de quiebres estructural llevados a cabo siguiendo la nomenclatura usada por @Zeileis2002. La diferencia entre Rec-CUSUM y Rec-CUSUM(d) es que se permite la existencia de un rezago, ya que, @Society1988 muestran que los test CUSUM no pierden sus propiedades al relajar algunos supuestos, como trabajar con modelos dinámicos. En todos los casos que no se utilizan rezagos de la tasa de vacantes, se rechaza la hipótesis nula de invariabilidad en los parámetros, por tanto, no se rechaza la existencia de algún quiebre estructural en la relación entre vacantes y tasa de desempleo. Los únicos test que no rechazan $H_0$ son Score-CUSUM y OLS-CUSUM(d) modelos que incluyen un parámetro autoregresivo de vacantes. En todos los casos, siguiendo a @Zeileis2004 se estimo la matriz de varianzas y covarianzas robusta ante la heteroscedasticidad y autocorrelación usando un estimador de kernel cuadrático HAC [@Andrews1991] con un filtrado VAR(1) y una elección automática del ancho de banda basado en una aproximación AR(1).^[El kernel génerico es $\omega_l = K(\frac{l}{B})$ con K la función de kernel y B el ancho de banda. El kernel espectral tiene la siguiente forma $\omega_l = \frac{3}{z^2}(\frac{\sin(z)}{z} - \cos(z))$ siendo $l$ el rezago y $z = \frac{6\pi}{5}\frac{l}{B}$ [@Andrews1991]]

En las Figuras de la sección \@ref(efpAnexo) en el apéndice podemos observar las fluctuaciones del proceso empírico y su comparación con la fluctuación del proceso límite. Esto muestra en que periodo debería estar el o los quiebres en los parámetros, básicamente todos los test utilizados en el Cuadro \@ref(tab:quiebres) comparten que la hipótesis nula de la no existencia de cambio estructural debería ser rechazada cuando el proceso empírico se vuelve improbablemente superior a las fluctuaciones del proceso límite [@Zeileis2002]. 
<!-- DESCRIBIR! -->

Los test Score permiten observar variabilidad en la varianza, al sobrepasar el umbral esta es estadísticamente signifiticativa al 5\%. Tanto en el test Score-CUSUM como Score-MOSUM la varianza muestra fluctuaciones entre 1990 y 1995, y en torno a 2010-2011, sobrepasando el umbral. Por otra parte, el test Score-CUSUM con rezagos con p-valor 0.5, muestra una varianza al límite del umbral, pero sin sobrepasarlo. Es un indicio de que es posible plantear un modelo que no solo tome en cuenta los quiebres en la media condicional sino también en la varianza de los errores lo cual puede mejorar la estímación de los quiebres [@BaiPerron2003].<!-- , por ello se estiman modelos de quiebres tanto de parámetros como varianza siguiendo a @Zeileis2010. -->

Planteamos los test de tipo F generalizados definiendo el tamaño mínimo del intervalo a considerar, en base a un parámetro de ancha de banda h fijado en 0.15.

Utilizamos las tres variaciones propuestas por @Andrews1993 y @Andrews1994, supF, aveF y expF. En el Cuadro \@ref(tab:ftest) podemos observar el test, el valor del estadístico y el p-valor asociado. Se usa la misma matriz de variazas y covarianzas robusta igual que en el caso anterior. En los tres test, encontramos un quiebre estructural en torno a 1990-I, resultado en linea con @Urrestarazu1997.



```{r coefTestEstructural, fig.cap='Coeficientes de diferentes períodos', results="asis", fig.align='center', eval = TRUE, include=FALSE}

library(fxregime)
# FXREGIME 
dt_ts <- ts(data = dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
                      .(pib, ind_vac, td)],
            start = c(1981, 1), frequency = 4)
reg <- log(ind_vac) ~ log(td) + 1
# Buscamos los quiebres cada 5 años
mod_reg <- fxregimes(formula = reg, data = zoo(dt_ts, frequency = 4), h = 20, 
                     breaks = 5)
# confint(mod_reg, level = 0.95, vcov = kernHAC)
# Resúmen completo, primero re-estimar el modelo en los subperiodos y luego aplicando summary
mod_rf <- refit(mod_reg)
# print(xtable(round(coef(mod_reg), 4)), comment = FALSE)
texto = "\\\\footnotesize Explicación de los coeficientes del modelo"
kableExtra::kable(coef(mod_reg), digits = 2, row.names = T, align = "c", caption = "Coeficientes de cada periodo. Resúmen", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = c("$\\beta_0$", "$\\beta_1$", "$\\sigma^2$")
                  ) %>% 
  kableExtra::kable_styling(full_width = F, latex_options = "hold_position") %>% 
  kableExtra::column_spec(column = 2:4, width = "2cm") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::footnote(general = texto, general_title = "Notas:", 
                       threeparttable = TRUE, fixed_small_size	= TRUE)
```

(ref:Zeileis2010) @Zeileis2010
(ref:BaiPerron2003) @BaiPerron2003

```{r vcovHAC-andrews, fig.cap='Coeficientes del período', results='asis', fig.align='center'}

get_model <- function(.mod, .mat = sandwich::vcovHAC) {
    a = data.table::rbindlist(
        lapply(.mod, function(x) {
        broom::tidy(
            lmtest::coeftest(x, .mat)
            )
    }), 
    use.names = TRUE, idcol = "modelo")
    setnames(a, old = names(a), 
            new = c("Modelo", "coeficiente", "Estimación", "Estándar error", 
                    "Estadístico", "p-valor"))
    a[`p-valor` > 0.1, sigf := ""]
    a[between(`p-valor`, lower = 0.05, 0.1),   sigf := "."]
    a[between(`p-valor`, lower = 0.01, 0.05),  sigf := "*"]
    a[between(`p-valor`, lower = 0.001, 0.01), sigf := "**"]
    a[between(`p-valor`, lower = 0, 0.001),    sigf := "***"]
    a[coeficiente == "(Intercept)", coeficiente := "$\\hat{\\beta}_0$"]
    a[coeficiente == "log(td)", coeficiente := "$\\hat{\\beta}_1$"]
    a[coeficiente == "(Variance)", coeficiente := "$\\hat{\\sigma}^2$"]
    a
}

texto = "\\\\footnotesize Estimación para los cuatro periodos detectados mediante los test de quiebres estructural siguiendo a (ref:BaiPerron2003) y (ref:Zeileis2010). Se muestran los valores de los coeficientes estimados $\\\\beta_0$, $\\\\beta_1$ y $\\\\sigma^2$ en la columna \\\\textit{Estimación}, sus errores estándar en \\\\textit{Estándar error} y el valor del estadístico en \\\\textit{Estadístico}. \\\\textit{P-valor} muestra los respectivos p-valores de cada prueba. \\\\textit{Significación} muestra los niveles de significación donde . quiere decir no significativo al 5\\\\% pero si al 10\\\\%. Un * es estadísticamente significativo al 5\\\\%, ** es significativo al 1\\\\% mientras *** es significativo al 0.1\\\\%. En todos los casos los coeficientes son estadísticamente significativos al 5\\\\% a excepción de $\\\\beta_0$ entre 1990 y 1996. Los signos de $\\\\beta_1$ son negativos para todos los períodos. En el período 2013 el valos de $\\\\sigma^2$ es diferente de 0, pero ha sido redondeado. Se usaron variables desestacionalizadas, aunque los resultados se mantienen si se utilizan variables sin desestacionalizar.

\\\\textit{Fuentes}: Datos de vacantes laborales de elaboración propia. Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE)"

kableExtra::kable(get_model(.mod = mod_rf, .mat = sandwich::vcovHAC), digits = 2, row.names = F, align = "c", caption = "Coeficientes de cada periodo", escape = F, booktabs = TRUE, format = "latex", longtable = F, 
                  col.names = linebreak(c("Periodo", "Coeficiente", "Estimación", "Estándar\nerror", "Estadístico", "p-valor", "Significación")
                  )) %>% 
  kableExtra::kable_styling(latex_options = "hold_position",
                            font_size = 10,
    # latex_options = "scale_down"
    ) %>% 
  kableExtra::footnote(general = texto, 
                       general_title = "Notas:", 
                       threeparttable = TRUE,
                       escape = FALSE)
```

Finalmente, calculamos todos los posibles periodos de quiebres estructurales de forma generalizada. Para ello seguimos los trabajos de @BaiPerron2003, @BaiPerron1998 y aplicaciones de @Zeileis2003, en especial a @Zeileis2010. Obtenemos tres quiebres estructurales en los años 1990-II, 1996-II y 2013-II usando como función objetivo la log-verosimilitud negativa. ^[Los quiebres son exactamente iguales si las variables se modelan en niveles o logaritmos. Es posible modelar otro cambio estructural eligiendo otra función objetivo como el BIC o RSS, sin embargo dicho intervalo es extremadamente amplio al computarlo con una matriz HAC, al elegir un máximo de 3 puntos de quiebres se obtienen los mismos resultados, si se fija un máximo de 4 se obtiene un quiebre adicional en 2004-I y se mantienen los restantes.] Por lo tanto, los 4 periodos de análisis son 1981-I.1990-II, 1990-III.1996-II, 1996-III.2013-I y 2013-II.2018.IV. En el Cuadro \@ref(tab:vcovHAC-andrews) observamos que en todos los periodos los parámetros muestran el signo correcto según la literatura (negativo). En todos los casos son estadísticamente significativos al 5\%, usando una matriz HAC con ponderadores de Andrews. Sin embargo, usando ponderadores de Kernel en el segundo periodo el p-valor es 0.12. Dada la existencia de autocorrelación y heteroscedasticidad, la elección de la matriz de ponderadores es relevante y puede generar variaciones en la significatividad estadística de algunos parámetros. Por ello los Cuadros \@ref(tab:vcovHAC-lumley), \@ref(tab:kernHAC) y \@ref(tab:NeweyWest) muestran diferentes ponderaciones para los distintos periodos. Es de notar que el único momento que podría ser discutible es entre 1990-III.1996-II, en el cual dependiendo la elección de la matriz el parámetro de la tasa de desempleo puede resultar no significativo, sin embargo, si se estima para el mismo periodo un modelo lineal pero sin varianza de los errores, la tasa de desempleo es estadísticamente significativa, con el signo del coeficiente negativo. En el resto de los casos los coeficientes son estadísticamente significativos al nivel $\alpha = 0.05$, al igual que las varianzas de cada periodo.

Las CB estimadas para los periodos obtenidos se pueden ver en la Figura \@ref(fig:BCtest). En todos los casos se mantiene una relación negativa entre vacantes y desempleo. A la vez que se observan cambios de nivel y pendiente. El primer periodo desde 1981-I hasta 1990-II muestra una curva comparativamente más cercana al origen que 1990-III a 1996-II donde se traslada de forma paralela, lo mismo vuelve a suceder en 1996-III a 2013-II (incluyendo un leve cambio de pendiente). <!-- denotando lo que podría ser un mercado laboral menos eficiente.  --> El comportamiento se modifica a partir de 2013-III en donde la curva tiene un cambio tanto paralelo como de pendiente hacia el origen.
<!-- Una interpretación es que las reformas estructurales llevadas a cabo a partir del año 2005, generaron un efecto negativo en el mercado laboral lo cual se revierte a partir de 2013. Aunque, es poco verosimil en la medida que los gobiernos entre 2005 y 2020 llevaron adelante medidas de protección hacia los trabajadores. Otra lectura sería que dichas reformas tuvieron un efecto impensado y no solo no generaron mayores fricciones en el mercado laborales, sino que las disminuyeron tornandolo más eficiente al aumentar el matching entre trabajadores y firmas^[Mantenemos siempre la condicionalidad sobre la mejora o no de la eficiencia en la medida que no tomamos en cuenta los flujos laborales desde el empleo al desempleo y del desempleo al empleo.]. Otro factor relevante son los portales laborales de Internet y el avance tecnológico que permiten una búsqueda y proceso de contratación a una velocidad comparativamente mayor que los procesos iniciados mediante prensa en papel. -->

```{r BCtest, fig.cap="Curva de Beveridge por periodo", fig.align="center"}

# Hacer una función, esto es muy manual.
dt[fecha <= "1990-04-01", decada_test := "1-periodo"
   ][between(fecha, "1990-07-01", "1996-04-01"), decada_test := "2-periodo"
     ][between(fecha, "1996-07-01", "2013-04-01"), decada_test := "3-periodo"
       ][between(fecha, "2013-07-01", "2019-10-01"), decada_test := "4-periodo"]
dt$decada_test <- factor(dt$decada_test,labels = c("81-90:Q4", "90:Q3-96:Q2", "96:Q3-2013:Q2", "2013:Q3-2018:Q4"))
notas = "Curva de Beveridge (CB) para los periodos obtenidos mediante los test de quiebre estructural siguiendo a (ref:Zeileis2010) y (ref:BaiPerron2003). Se utiliza el paquete fxregime aunque resultados similares se obtienen con el paquete strucchange. En todas las etapas, la relación entre vacantes y desempleo es negativa. Se observan traslados de la CB y movimientos de pendiente. El segundo periodo entre los años 1990 y 1996 muestra un traslado paralelo hacia fuera y sucede entre los años 1996 y 2013, lo que podría estar indicando la posibilidad de un mercado laboral menos eficiente. En 2013-2018 se da un corrimiento al origen, indicios de un mercado laboral que podría ser más eficiente. En ningún caso puede afirmarse categóricamente que el mercado laboral sea más o menos eficiente por un traslado de la CB hacia fuera o hacia el origen, sin incluir como mínimo análisis de los flujos de creación y destrucción de puestos de trabajo."
fuentes = "Datos de vacantes laborales de elaboración propia. Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE)."

ggplot(dt, aes(y = ind_vac, x = td, color = decada_test)) + 
    geom_point() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes", title = " \n ") +
    scale_color_manual(name = "", values = color_decada) +
    geom_smooth(method = "lm", formula = y ~ x) +
    theme_Publication()
# Agregar el caso de 4 quiebres y por tanto 5 particiones usando breakpoint.
```

## TVP-VAR

Por último, realizamos una estimación multivariada de un vector autoregresivo con parámetros variables y volatilidad estocástica siguiendo a @Primiceri2005, @Lubik2016 y @Lubik2016b, usando el algoritmo corregido por @DelNegro2015. Nos interesa analizar la variabilidad en los parámetros beta y la matriz de varianzas y covarianzas, sumado al efecto de un shock por parte del producto sobre vacantes y desempleo. <!-- \footnote{Es posible utilizar el test desarrollado por @Stock1998.}. -->

Usamos las primeras 36 observaciones, nueve años, desde 1981-I hasta 1989-IV para calibrar las distribuciones a priori, quedando un periodo efectivo desde 1990 hasta 2018. Simulamos 50 mil veces y elegimos un orden de rezagos igual a uno, es decir, un TVP-VAR(1).^[Se corrieron 50 mil simulaciones con un orden de rezagos igual a dos y los resultados fueron los mismos. Por ello, se muestra un TVP-VAR(1) en la medida que se facilita la visualización.] Las series utilizadas de producto, tasa de desempleo e índice de vacantes son todas de frecuencia trimestral, lo cual (a excepción del PIB que se publica trimestralmente) va a generar series con menor variabilidad.

La restricción de identificación es que son los shocks desde el producto (shocks de productividad) los cuales afectan al desempleo y las vacantes laborales, con un rezago. Por lo tanto, el orden de exogeneidad de las variables es que el pib es la primer variable, seguido del índice de vacantes y la tasa de desempleo. El orden de la segunda y tercer variable, no es una restricción de identificación sino una normalización necesaria que puede modificar los resultados [@Primiceri2005], sin embargo, en este caso el orden no genera diferencias. La estructura de identificación elegida para las innovaciones es básicamente una especificación de Cholesky. En la Figura \@ref(fig:svar-parameters) observamos los desvíos estandar variables a lo largo del tiempo de los residuos del modelo, se gráfica la media (posterior) y los percentiles 16 y 84.\footnote{Bajo normalidad los percentiles 16-84 corresponden a las cotas de una desviación estándar en los intervalos de confianza}. Adicionalmente se estima un VAR de parámetros fijos y sin volatilidad estocástica el cual se puede ver en la Figura \@ref(fig:svar-parameters) fila 3 columna 3. En el resto de los casos no se observa debido a que no se encuentra en el rango de valores mostrados. 

El gráfico muestra dos resultados, el primero es que las vacantes laborales son estables para el periodo. El segundo es que si bien la magnitud es leve, parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante con una transición suave, donde el primero presenta una mayor varianza tanto para el pib como la tasa desempleo, mientras en el caso de las vacantes no se observan diferencias.^[En los test de quiebre estructural si se definen cuatro posibles quiebres estructurales, el cuarto se genera en torno a 2004.] Las modificaciones en las varianzas comienzan previo a 2005, lo cual se relaciona con la crisis de la economía en 2002 y el posterior crecimiento ininterrumpido a partir del tercer trimestre de 2003. 

<!-- \begin{landscape} -->
```{r svar-parameters, fig.cap="Rezagos, coeficientes y volatilidades modelo TVP-VAR", fig.height=6, fig.width=8, fig.align="center"}
notas = "Se grafican las matrices $\\hat{c}_{t}$, $\\hat{B}_{jt}$ y $\\hat{\\Sigma}_{jt}$ desde 1990 hasta 2018 para la tasa de crecimiento del producto, índice de vacantes y tasa de desempleo. Observamos los desvíos estandar de los residuos del modelo, la media (posterior) y los cuantiles 16 y 84. Parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante, donde el primero presenta mayor varianza tanto para el pib como la tasa desempleo. En las vacantes, no se observan diferencias en coeficientes y rezagos ni en varianzas. Producto y tasa de desempleo muestran variaciones en la matriz de varianzas-covarianzas. La linea punteada en la tercera fila-columna corresponde a la estimación de un Vector Autorregresivo (VAR) de parámetros fijos y sin volatilidad estocástica. En el resto de las figuras la linea punteada no se observa por el rango de valores mostrados."
fuentes = "Serie de producto trimestral calculada por el Banco Central del Uruguay (BCU) y facilitada por el Centro de Investigaciones Económicas (CINVE). Compatibilización propia de tasas de desempleo trimestrales calculadas por el Instituto Nacional de Estadística (INE). Índice de vacantes de construcción propia."
par(mfrow = c(3, 3))
for(i in c("intercept", "lag1", "vcv")) {
    for(j in 1:3) {
      if(j == 1) k <- "pib" else if (j == 2) k <- "vacantes" else k <- "desempleo"
      if (i == "intercept") {
        k <- eval(bquote(expression(.(k) ~ c[j][t])))
      } else if (i == "lag1") {
        k <- eval(bquote(expression(.(k) ~ B[j][t])))
      } else {
        k <- eval(bquote(expression(.(k) ~ Sigma[j][t])))
      }
      make_plot(.fit = fit, .type = i, .var = j, .title = k)
    }
}
```
<!-- \end{landscape} -->

En la Figura \@ref(fig:svar-parameters) se observa que los parámetros de las variables rezagados $B_{j,t}$ y los interceptos $c_{t}$ tienen poca variabilidad a lo largo del periodo de análisis.^[se muestra la estimación de un TVP-VAR(1) en vez de un TVP-VAR(2), dado que los resultados no se modifican y se facilita su visualización] Este resultado es común en la literatura de TVP-VAR [@Lubik2016b], sin embargo, como se notó anteriormente si existe variabilidad en las innovaciones del producto y desempleo.

El resultado es robusto frente a diferentes especificaciones, porque con variables en niveles o en logaritmos los resultados no cambian. Adicionalmente se estima un modelo bivariado con desempleo y vacantes, tanto en niveles como en logaritmos, obteniendo las mismas conclusiones. Se podría pensar en el uso de un modelo de parámetros fijos y volatilidad estocástica, sin embargo, en la medida que el TVP-VAR no impone la restricción de parámetros fijos pero se obtiene dicho resultado no parece necesario.

```{r FIR, fig.cap="FIR", out.width='1\\linewidth'}

notas = "FIR desde producto hacia el índice de vacantes (A) y tasa de desempleo (B). La linea negra es la mediana, mientras las áreas grises refieren a los intervalos de confianza al 5-95\\% y 25-75\\%. Con color rojo la FIR de un VAR de parámetros fijos. Los signos de la mediana del TVP-VAR se ajusta a lo que se espera de un shocks desde el producto a vacantes y desempleo. En el primer caso un efecto positivo y en el segundo un efecto negativo. En el caso de un VAR los resultados son menos claro con valores en torno a cero."
# Tengo que modificar plot_irf para que sea en español y que los label de eje y estén rotados en 90 grados. O sino pasarlo a ggplot.
par(mfrow = c(1, 2))
plot_irf(impulse = 1, response = 2, .main = "Panel A. Vacantes")
plot_irf(impulse = 1, response = 3, .main = "Panel B. Desempleo")
```

Por último analizamos las FIR mediante la estimación de la mediana. Su visualización no es trivial, en la medida que en cada momento del tiempo existe una FIR. Una opción es visualizar distintos momentos y observar si existen diferencia (la que se elige), otra es mostrar una visualización en tres dimensiones. En nuestro caso, las FIR en los distintos momentos del tiempo se mantienen prácticamente iguales.

En la Figura \@ref(fig:FIR) Panel A tenemos shocks desde el producto hacia las vacantes laborales el efecto es positivo en todo momento, esto tiene sentido en la medida que una innovación de productividad, debería generar que la demanda laboral de las empresas se vea aumentado, en la medida que crezca el nivel de producción de la economía. Al contrario en la Figura \@ref(fig:FIR) Panel B observamos como el efecto del shock genera un efecto negativo en todo momento sobre la tasa de desempleo. Al mejorar la productividad de la economía, el desempleo debería disminuir en la medida que la economía es capaz de aumentar su producción, lo cual lleva a que las empresas aumenten su contratación (más o menos dependiendo de cuan sesgado sea hacia el uso de tecnología y capital). Los efectos de entrada o salida de personas a la PEA esta presente en ambos indicadores.

```{r FIRs, fig.cap="FIR diferentes años", out.width='1\\linewidth'}

notas = "FIR desde producto hacia el índice de vacantes (A) y tasa de desempleo (B). Se grafican las medianas de la FIR para 8 periodos diferentes y se observa que siguen la misma dinámica con leves diferencias. Los signos de la mediana del TVP-VAR se ajusta a lo que se espera de un shocks desde el producto a vacantes y desempleo. En el primer caso un efecto positivo y en el segundo un efecto negativo."
abline2 <- function(...){ 
    abline(..., lty = 4, lwd = 0.3)
}
fir_periodos <- function(fit, respuesta, impulso = 1, titulo) {
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
cols1 <- cbPalette#[c(2, 4, 2)]
cols2 <- cbPalette#[c(2, 4, 6)]

fir <- list()
j = 0
for(i in seq(1, 100, 8)) {
    j = j + 1
    fir[[j]] <- impulse.responses(fit, 
                      impulse.variable = impulso, 
                      response.variable = respuesta, 
                      t = i, scenario = 2, 
                      draw.plot = FALSE)$irf
}

gdat <- rbind(0, sapply(fir, function(z) apply(as.matrix(z), 2, median))) 

yb <- c(min(gdat), max(gdat))
matplot2(x = 0:20, y = gdat, ylim = yb, xlab = "Horizonte",
         col = cols2) 
title(main = titulo, cex.main = 0.6, adj = 0, line = 0)
abline2(v = seq(5, 20, 5), h = seq(yb[1], yb[2], (yb[2]-yb[1])/10)) # 0.0001
}
par(mfrow = c(1, 2))
fir_periodos(fit = fit, respuesta = 2, impulso = 1, titulo = "Panel A. Vacantes")
fir_periodos(fit = fit, respuesta = 3, impulso = 1, titulo = "Panel B. Desempleo")

```

