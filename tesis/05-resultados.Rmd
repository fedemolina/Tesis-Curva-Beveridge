# Resultados {#Resultados}

```{r}
# Carga del modelo svar-sv
fit <- readRDS(here::here("Datos", "Finales", "modelo.rds"))

# Función para generar los gráficos de parámetros y varianzas
matplot2 <- function(...) matplot(..., type = "l", lty = 1, lwd = 2, bty = "n", ylab = "")
stat.helper <- function(z) c(mean(z), quantile(z, c(0.16, 0.84)))[c(2, 1, 3)]
gp <- seq(1985, 2020, 5) # marks for vertical lines
# colors, taken from http://www.cookbook-r.com
cols <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
cols1 <- cols[c(2, 4, 2)]
make_plot <- function(.fit = fit, .type = "vcv", .var = 1, .title = "") {
    gp <- seq(1990, 2020, 5) # marks for vertical lines
    if(.type == "vcv") {
        # SD of unemployment residual
        # Get posterior draws
        sd_inf <- parameter.draws(.fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(sqrt(sd_inf), 2, stat.helper))    
    } else if (grepl(x = .type, pattern = "lag")) {
        beta <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1   <- t(apply(beta, 2, stat.helper))    
    } else {
        beta_0 <- parameter.draws(fit, type = .type, row = .var, col = .var)
        x1     <- t(apply(beta_0, 2, stat.helper))       
    }
    xax <- seq(1990, 2018, length.out = NROW(x1)) # x axis
    # Plot
    if(.type == "vcv") {
        var <- sd.residuals.ols[.var]
    } else {
        var <- NULL
    }
    matplot2(x = xax, y = x1, ylim = c(min(x1), max(x1)), col = cols1, main = .title , xlab = "Fecha")
    abline(h = seq(min(x1), max(x1), length.out = 10), v = gp, lty = 4, lwd = 0.3)
    if(.type == "vcv") {
        abline(h = var, col = cols[1], lwd = 1.4, lty = 5)
    }
}

# Función para generar los IRF
plot_irf <- function(.fit = fit, impulse, response, scenario = 2) {
    ira <- impulse.responses(fit, impulse.variable = impulse, response.variable = response, scenario = scenario)
    # OLS impulse responses for comparison
    ira.ols <- irf(fit.ols, n.ahead = 20)[[impulse]][[response]][-1, 1]
    # Add to plot
    lines(x = 1:20, y = ira.ols, lwd = 1, lty = 5, col = "red")
}

# carga del modelo var
library(vars)
fit.ols <- readRDS(here::here("Datos", "Finales", "modelo-var-comun.rds"))
sd.residuals.ols <- apply(residuals(fit.ols), 2, sd)
```

Este capítulo presenta los resultados principales del trabajo. Primero se grafica la curva de Beveridge entre 1981 y 2018, se hace una partición por décadas para visualizar potenciales etapas. Realizamos una exploración de la relación entre vacantes laborales y el IVF del PIB y tasa de desempleo con producto.

Posteriormente analizamos las series utilizados y sus propiedades estadísticas, tales como raíces unitarias, raíces estacionales y cointegración utilizando distintos test estadísticos.

Realizamos una búsqueda de quiebres estructurales mediante distintos tipos de test estadísticos, como procesos de fluctuación, test F y dating.

Finalmente estimamos un TVP-VAR con volatilidad estocástica, donde analizamos la evolución de los rezagos de las variables endógenas y los desvíos estándar de la matriz de varianzas y covarianzas. Finalmente computamos las FIR por parte del producto sobre vacantes y desempleo.

## Curva de Beveridge

```{r beveridge-curve, fig.cap="Curva de Beveridge 1981-2018", fig.subcap=c('Curva de Beveridge trimestral','Curva de Beveridge por década', 'Curva de Beveridge anual'), out.width='.99\\linewidth', fig.asp=1, fig.ncol = 3, fig.fullwidth = TRUE, fig.height=7, fig.width=7}

# xaxis = list(title = "Tasa de desempleo")
# yaxis = list(title = "Tasa de vacantes")
# t <- list(
#     family = "sans serif",
#     size = 8,
#     color = toRGB("grey50"))
# 
# dt[, plot_ly(x = td, y = ind_vac, type = "scatter", mode = "lines", color = decada) %>% 
#        layout(showlegend = FALSE,
#               axis = axis,
#               yaxis = yaxis)
#    ]
# plot_ly(dt[, .(td = mean(td), ind_vac = mean(ind_vac), decada = min(decada)), keyby = .(ano)], 
#         x = ~td, y = ~ind_vac, text =~ ano, color =~ decada) %>%
#     add_trace(mode = "lines+markers", type = "scatter") %>% 
#     add_text(textfont = t, textposition = "top right") %>%
#     layout(showlegend = FALSE,
#               axis = axis,
#               yaxis = yaxis)
p1 <- ggplot(dt, aes(y = ind_vac, x = td, color = decada)) + 
    geom_point() +
    geom_path() +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes") +
    theme(legend.position = "none")

p2 <- ggplot(dt, aes(y = ind_vac, x = td, color = decada)) + 
    geom_point() +
    # geom_path() +
    # geom_text(data = dt, aes(label = ano)) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes") +
    geom_smooth(method = "lm", formula = y ~ x) +
    theme(legend.position = "none")

p3 <- dt[, .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = min(decada)), keyby = .(ano)] %>% 
    ggplot(., aes(y = ind_vac, x = td, color = decada, label = ano)) + 
    geom_point() +
    geom_path() +
    # position=position_jitter(width=.2,height=.02)
    geom_text(fontface = "bold", size = 2, position=position_jitter(width=.1,height=.02), color = "black") +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes") +
    # geom_smooth(method = "lm", formula = y ~ x) +
    theme(legend.position = "none")
library(gridExtra)
grid.arrange(p1,                             # First row with one plot spaning over 2 columns
             arrangeGrob(p2, p3, ncol = 2), # Second row with 2 plots in 2 different columns
             nrow = 2
             )
```

El primer hallazo del trabajo se observa en la figura \@ref(fig:beveridge-curve)a, con una relación negativa y convexa entre vacantes y desempleo en linea con la teoría económica para mercados laborales de economías de mercado. El color azul denota la década de 1980, rosado años 90, verde los 2000 y finalmente naranja 2010 a 2020. 

El segundo resultado, es que se existen claros movimientos a lo largo de la curva (por décadas) y traslados de la misma (entre décadas). 
En la figura \@ref(fig:beveridge-curve)b observamos los promedio anuales del índice de vacantes y tasa de desempleo, si analizamos por década, podemos identificar 4 fases, donde los años 90 trasladan la curva hacia un nuevo estado con mayor desempleo ante igual cantidad de vacantes, el mismo se estabiliza en los 2000 y vuelve a cambiar de 2010 en adelante entrando en una nueva fase con menor desempleo ante misma cantidad de vacantes. Si nos limitamos a un lustro, podemos ver que a partir de 2005 parece haber un cambio en el mercado laboral, el cual se mantiene y vuelve a cambiar en 2010. Lo que se observa claramente en la curva, es que el mercado laboral debió transitar un cambio profundamente negativo en la década del 90 y que en los últimos 15 ha sucedido lo mismo pero en sentido inverso, es decir, estaríamos en un mercado laboral que soporta una tasa de desempleo mayor lo que podría indicar una mayor eficiencia del mismo.


```{r td-vac-pib, fig.cap="Relación entre producto y vacantes y producto y desempleo", fig.subcap=c('PIB y Vacantes', 'PIB y Desempleo'), out.width='.99\\linewidth', fig.ncol = 2}
# plotly::subplot(
# dt[, .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = min(decada)), keyby = .(ano)
#    ][, plot_ly(x = ind_vac, y = pib, type = "scatter", mode = "markers+lines", color = decada, text = ~ ano) %>% 
#          add_text(textfont = t, textposition = "top right") %>%
#          layout(showlegend = FALSE,
#                 xaxis = list(title = "Índice de vacantes"),
#                 yaxis = list(title = "IVF PIB"))],
# dt[, .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = min(decada)), keyby = .(ano)
#    ][, plot_ly(x = td, y = pib, type = "scatter", mode = "markers+lines", color = decada, text =~ ano) %>% 
#          add_text(textfont = t, textposition = "top right") %>%
#          layout(showlegend = FALSE,
#                 xaxis = list(title = "Tasa de desempleo"),
#                 yaxis = list(title = "IVF PIB"))],
# nrows = 2
# )

p1 <- dt[, .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = min(decada)), keyby = .(ano)
   ] %>% 
ggplot(., aes(x = ind_vac, y = pib, color = decada, label = ano)) +
    geom_point() +
    geom_path() +
    labs(x = "Índice de vacantes", y = "IVF PIB") +
    geom_text(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme(legend.position = "none")

p2 <- dt[, .(td = mean(td), ind_vac = mean(ind_vac), pib = mean(pib), decada = min(decada)), keyby = .(ano)
   ] %>% 
    ggplot(., aes(x = td, y = pib, color = decada, label = ano)) +
    geom_point() +
    geom_path() +
    labs(x = "Tasa de desempleo", y = "IVF PIB") +
    geom_text(size = 2, position=position_jitter(width=.02,height=.2), color = "black") +
    theme(legend.position = "none")
grid.arrange(p1, p2, ncol = 2)
```

En la figura \@ref(fig:td-vac-pib)a podemos observar la relación entre vacantes y PIB. Es interesante notar la diferencia en la década de 2010, la notoria caída en las vacantes laborales no se ve acompaña por una caída del PBI, como puede observarse en los años 80 y 2000. La tasa de variación en la cual caen las vacantes laborales entre 2011 y 2018, es prácticamente la misma entre 1998-2002 y  1981 y 1983, sin embargo el nivel no cae en ningún momento por lo que no se observa el movimiento de U típico de 80-82 y 98-2002.

En la figura \@ref(fig:td-vac-pib)b, observamos el mismo comportamiento de la CB en los años 90, una transición hacia lo que podríamos catalogar como un nuevo estado, una década de crecimiento con alto desempleo, hacia otra con crecimiento y caída del desempleo, los años 2000. Además, los 80 y 2000 vuelven a compartir la forma de U, solo que esta vez es en sentido contrario. La década de 2010, muestra a diferencia del gráfico anterior un comportamiento similar a los 90, se observa crecimiento económico con crecimiento del desempleo, aunque el nivel de actividad no cae en ningún momento.

## Caracterización de las series
<!-- Acá hay que realizar todo un análisis exploratorio, raíces unitarias, raíces estacionales, cointegración de desempleo y vacantes. -->
```{r include=FALSE}
# Series trimestrales
# readRDS(here::here("Datos", "Finales", "serie_trimestral_ga_13-19.rds"))
ga_ts <- ts(dt[, av_umcg], start = c(1980, 1), frequency = 4)
um_ts <- ts(dt[!is.na(av_urr_mol), av_urr_mol], start = c(1980, 1), frequency = 4)
ct_ts <- ts(dt[!is.na(av_ct_s_dup), av_ct_s_dup], start = c(2003, 3), frequency = 4)
bj_ts <- ts(dt[av_bj_s_dup > 0, av_bj_s_dup], start = c(2007, 2), frequency = 4)
final <- ts(dt[, av_final], start = c(1980, 1), frequency = 4)
final_tc <- ts(dt[, av_final_tc], start = c(1980, 1), frequency = 4)
pib   <- ts(dt[!is.na(pib), pib], start = c(1981, 1), frequency = 4) 
ind_vac   <- ts(dt[!is.na(ind_vac), ind_vac], start = c(1980, 1), frequency = 4)
desempleo <- ts(dt[!is.na(td), td], start = c(1981, 1), frequency = 4)
pea <- ts(dt[!is.na(pea), pea], start = c(1980, 1), frequency = 4)

for(serie in c("ga_ts", "ct_ts", "bj_ts", "um_ts", "final", "final_tc", "pib", "ind_vac", "desempleo", "pea")) {
    for(test in c("adf", "kpss", "pp")) {
        print(c(serie, test, forecast::ndiffs(get(serie), alpha = 0.05, test = test, max.d = 2, type = "level")))
    }
}

for(serie in c("ga_ts", "ct_ts", "bj_ts", "um_ts", "final", "final_tc", "pib", "ind_vac", "desempleo", "pea")) {
    for(test in c("hegy", "ocsb", "ch")) {
        print(c(serie, test, forecast::nsdiffs(get(serie), alpha = 0.05, test = test)))
    }
}

# Series mensuales
bj_ct_mensual <- readRDS(here::here("Datos", "Finales", "serie_mensual_bj_ct.rds"))
gallito_mensual <- readRDS(here::here("Datos", "Finales", "serie_mensual_ga_13-19.rds"))

ct_ts <- ts(bj_ct_mensual[, av_ct_s_dup], start = c(2003, 5), frequency = 12)
bj_ts <- ts(bj_ct_mensual[av_bj_s_dup > 0, av_bj_s_dup], start = c(2007, 6), frequency = 12)
ga_ts <- ts(gallito_mensual[, avisos_s_dup], start = c(2013, 7), frequency = 12)
# ceres_ts <- ts(dt[!is.na(av_ceres), av_ceres], start = c(1998, 4), frequency = 12)

for(serie in c("ga_ts", "ct_ts", "bj_ts")) {
    for(test in c("adf", "kpss", "pp")) {
        print(c(serie, test, forecast::ndiffs(get(serie), alpha = 0.05, test = test, max.d = 2, type = "level")))
    }
}

for(serie in c("ga_ts", "ct_ts", "bj_ts")) {
    for(test in c("hegy", "ocsb", "ch")) {
        print(c(serie, test, forecast::nsdiffs(get(serie), alpha = 0.05, test = test)))
    }
}

```

Serie               | Test regular    | Diferencias |  Test estacional    | Diferencias
--------------------|-----------------|-------------|---------------------|-------------
_Gallito_ 80-19       | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      |    0
urr_mol 80-01       | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      | 1, 0, 0
_Computrabajo_ 03-19  | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      | 1, 0, 0
_Buscojobs_ 07-19     | ADF, KPSS, PP   |       0     | HEGY, OCSB, CH      | 1, 0, 0
serie_final         | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      |    0
PEA                 | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      |    0
tasa desempleo      | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      |    0
PIB                 | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      |    0
Índice de vacantes  | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      |    0

Table: Test de raíces unitarias ADF, KPSS y PP. Test de raíces unitarias estacionales HEGY, OCSB y CH. Series con frecuencia trimestral.\label{series-trimestrales}

El cuadro \ref{series-trimestrales} columna 2 muestra los resultados de la aplicación de los test Dickey Fuller Aumentado (ADF) [@ADF1979], Kwiatkowski–Phillips–Schmidt–Shin (KPSS) [@KPSS1992] y Phillips-Perron (PP) [@PhillipsPerron1988]. El test ADF y PP usan la hipótesis nula que la serie tiene una raíz unitaria versus una hipótesis alternativa de una raíz estacionaria. En el test KPSS la hipótesis nula es que la serie tiene una raíz estacionaria contra una hipótesis alternativa de raíz unitaria. La columna diferencias refiere a la cantidad de diferencias regulares necesarias para que la realización del proceso estócastico se vuelva estacionario. El resultado es que la serie de vacantes es un proceso integrado I(1), al igual que dos de las tres series que la componen, la serie de avisos de _Gallito_ (años 80 a 2019) y la serie de _Computrabajo_ (2003 a 2019), mientras _Buscojobs_ resulta ser I(0). Tanto el PIB como la tasa de desempleo, resultan procesos integrados de orden uno.

La columna test estacional, muestra los test de raíz unitaria estacional realizados. El test de @Hylleberg1990 (HEGY) pone aprueba la hipótesis nula de que las raíces del polinomio autoregresivo caen dentro del circulo unitario versus la alternativa que caen fuera. El test @Osborn1988 (OCSB) utiliza la hipótesis nula de raíz unitaria estacional versus la alternativa de estacionariedad. Mientras el test @Canova1995 (CH) plantea la hipótesis nula de la no existencia de raíz unitaria en las frecuencias estacionales versus la alternativa de raíz unitaria en una frecuencia estacional o en un conjunto de frecuencias estacionales . La última columna de la tabla refiere a la cantidad de diferencias estacionales necesarias para que el proceso se vuelva estacionario, los tres test llevan a las mismas conclusiones tanto para el PIB, tasa de desempleo e índice de vacantes, no son necesarias diferencias estacionales. Sin embargo, en el caso de la serie de buscojobs y computrabajo los resultados difieren, para ambas el test HEGY plantea la existencia de raíz unitaria en frecuencias estacional mientras los test OCSB y CH la descartan.

Por último el cuadro \ref{series-mensuales} realiza lo mismos test pero con un subconjunto de series de frecuencia mensual. Las series de _Gallito_ y _Computrabajo_, muestran resultados sin ambiguedades, son procesos I(1) mientras _Buscojobs_ es un proceso I(1) para KPSS pero I(0) para ADF y PP, notar que cuando la serie es trimestralizada los tres test coinciden. Los test de raíz unitaria estacional plantean que no es necesario realizar diferencias estacionales tanto para _Computrabajo_ como _Buscojobs_, por el contrario, la serie de _Gallito_ según HEGY y CH necesita una diferencia estacional, no así para OCSB. Dicho resultado difiere cuando la serie es trimestralizada, en donde los tres test coinciden en la no existencia de raíz unitaria estacional.


Serie               | Test regular    | Diferencias |   Test estacional   | Diferencias
--------------------|-----------------|-------------|---------------------|-------------
_Gallito_ 13-19       | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      | 1, 0, 1
_Computrabajo_ 03-19  | ADF, KPSS, PP   |       1     | HEGY, OCSB, CH      | 0, 0, 0
_Buscojobs_ 07-19     | ADF, KPSS, PP   |  0, 1, 0    | HEGY, OCSB, CH      | 0, 0, 0

Table: Test de raíces unitarias ADF, KPSS y PP. Test de raíces unitarias estacionales HEGY, OCSB y CH. Series con frecuencia mensual.\label{series-mensuales}

## Cointegración
<!-- http://onlinelibrary.wiley.com/doi/10.1002/jae.616/pdf
TEST DE PESARAN 2001 PARA SERIES I(1) E I(0)
-->
Dada la correlación negativa entre vacantes y desempleo, y el orden de integración igual a 1 no es esperable que exista una relación de largo plazo entre las series, cuando un proceso aumenta el otra disminuye, por lo cual los residuos de una regresión lineal entre ambas serán diferentes de un proceso I(0), lo que si debería pasar es que la velocidad de las series, es decir, sus tasas de crecimiento tengan una relación estable.
<!-- Siguiendo a @Phillips1990 <!-- quienes construyen sobre ENGLEA-GRANGER  -->se lleva a cabo el test de cointegración de Phillips-Ouliaris entre el índice de vacantes y la tasa de desempleo. Dado que el test no es invariante a la formulación de la ecuación de regresión, se realizan dos test utilizando como variable dependiente al índice de vacantes y luego la tasa de desempleo. En ambos casos, no se rechaza la hipótesis nula de no cointegración al 10\%. <!--EL TEST PZ NO TIENE ESE PROBLEMA?? REVISAR -->
Una vez que se toman logaritmos y se diferencian las series, se rechaza la hipótesis nula de no cointegración al 1\% indicando que las tasas de crecimiento del índice de vacantes laborales y la tasa de desempleo están cointegradas. El resultado se mantiene para la aceleración de las series. Por lo tanto, tanto la velocidad como la aceleración de las vacantes laborales y la tasa de desemeplo muestran una relación estable en el largo plazo, lo cual era esperable.

Se realiza el mismo procedimiento pero esta vez testeando la hipotesis nula de no cointegración entre pib y vacantes, posteriormente pib y desempleo. En ambos casos no se rechaza la hipótesis nula de no cointegración al 10\%. Realizamos el mismo procedimiento con la tasa de crecimiento del pib y vacantes, luego con la tasa de crecimiento del pib y tasa de desempleo. En ambos se rechaza la hipótesis nula de no cointegración al 1\%, el procedimiento devuelve el mismo resultado si se usa la tasa de variación de vacantes o desempleo.

Finalmente siguiendo a @Johansen1991, se plantean los test de traza y valor propio <!-- \footnote{SI tiene sentido plantearlo para el caso de dos variables, porque al realizar el test ADF o PO se esta imponiendo un vector de cointegración por lo cual toda la incertidumbre asociada con la estimación del vector de cointegración desaparece. Por el contrario, cuando se utiliza el esquema de Johansen, se estima el vector de cointegración por lo cual la incertidumbre asociada a la estimación se incorpora en el test. En nuestro caso, NO tiene sentido plantear que el vector existe desde la teoría. Como resultado, el test tiene MENOS poder, es decir, mayor probabilidad de rechazar la hipótesis nula cuando la hipótesis alternativa es verdadera versus el test de Johansen.} -->para las series del pib, tasa de desempleo e índice de vacantes, el resultado es que no se rechaza la hipótesis nula de no cointegración al 10\%. \footnote{Sin embargo, si se utiliza la tasa de crecimiento del pib, se rechaza la hipótesis nula de no cointegración al 1%, y no se rechaza que exista 1 relación de cointegración. Finalmente, si se utilizan solamente las tasas de crecimiento de las variables, no se rechaza la existencia de 3 relaciones de cointegración. Dichos resultados son robustos frente a la especificación de los vectores de cointegración con tendencia, constante o ninguna. Es decir, existe una relación estable de largo plazo entre las tasas de crecimiento del PIB, índice de vacantes y tasa de desempleo.}

## Quiebres estructurales

Es de interés buscar la existencia de quiebres estructurales tanto en la tasa de vacantes como en la tasa de desempleo, en la medida que trabajamos bajo la hipótesis de que se han producido cambios relevantes en el mercado laboral que han alterado su funcionamiento. Hemos probado que la tasa de vacantes y desempleo son procesos integrados de orden uno, sin media incondicional\footnote{No es posible que sean procesos I(1) y tengan media incondicional, en la medida que esto generaría series con crecimiento permanente en el largo plazo.}. Queremos probar si modelando cada ecuación como un proceso autoregresivo, obtenemos quiebres estructurales.

En la figura \@ref(fig:beveridge-curve)b gráficamos los datos de vacantes y desempleo junto a 4 modelos lineales, uno por cada década. La idea fue mostrar de forma intuitiva lo que parecen ser distintos periodos de la CB. Sin embargo, dicha agrupación no tiene sustento estadístico. A continuación, ponemos a prueba la hipótesis de existencia de algún quiebre estructural en la relación vacantes y desempleo y luego buscamos, en caso de existir, la fecha de dichos quiebres.

Planteamos:
\begin{equation}
log(ind\_vac_i) = \beta_i + \beta_i\log(td_i) + \epsilon_i
\end{equation}

Y testeamos:
\begin{align}
H_0: \beta_i &= \beta_0 \ \ \ (i = 1, ..., n) \\
H_1: \beta_i &\not= \beta_0 \ \ \ (i = 1, ..., n)
\end{align}


```{r quiebres, results='asis', fig.cap="Test de quiebres estructurales basados en residuos y estimadores. La columna Test presenta los test realizados, la columna Estadístico el valor de los estadísticos, la columna p-valor refiere a dicho valor para cada prueba. Significación refiere a los niveles de significación, . quiere decir no significativo al 5% pero si al 10%. Un * es estadísticamente significativo al 5%, ** es significativo al 1% mientras *** es significativo al 0.1%.", fig.align='center'}
library(xtable)
# Data
dt_ts <- ts(data = dt[data.table::between(fecha, "1981-01-01", "2018-10-01"), 
                      .(pib, ind_vac, td)], 
            start = c(1981, 1), frequency = 4)
# Desestacionalizo con método x13 y configuración default
td      <- seasonal::final(seasonal::seas(dt_ts[, "td"]))
ind_vac <- seasonal::final(seasonal::seas(dt_ts[, "ind_vac"]))
pib     <- seasonal::final(seasonal::seas(dt_ts[, "pib"]))
# tasa de crecimiento del pib ~ log(diff(pib))
delta_pib <- diff(
            ts(log(dt[fecha >= "1980-04-01", pib]), start = c(1980, 2), frequency = 4),
            lag = 1, differences = 1
            )
delta_pib <- window(delta_pib, start = c(1981, 1), end = c(2018, 4))
pib <- window(pib, start = c(1981, 1), end = c(2018, 4))
dt_ts <- ts.union(pib, ind_vac, td, delta_pib)

# Modelo
reg <- log(ind_vac) ~ log(td) + 1
rt <- function(test, .data = dt_ts[, 2:3], .formula = reg, .h = 0.15, .dynamic = FALSE, pval = TRUE) {
    # Modelo
    mod1 <- strucchange::efp(formula = .formula, type = test, data = .data, h = .h, 
                             dynamic = .dynamic, vcov = sandwich::kernHAC)
    # Test
    if(pval) {
        round(sctest(mod1)$p.value[[1]],2)
    } else {
        round(sctest(mod1)$statistic[[1]],2)
    }
}
ft <- function(test = "supF", .formula = reg, .data = dt_ts[, 2:3], .from = .15, .to = NULL, pval = F) {
    mod <- strucchange::Fstats(formula = .formula, data = .data, from = .from, to = .to,
                               vcov = sandwich::kernHAC)
    if(pval) {
        round(sctest(mod, type = test)$p.value[[1]], 2)
    } else {
        round(sctest(mod, type = test)$statistic[[1]], 2)
    }
}
test = c("Rec-CUSUM", "OLS-CUSUM", "Score-CUSUM", "Rec-CUSUM(d)", "OLS-CUSUM(d)" ,"Score-CUSUM(d)", "Rec-MOSUM", 
         "OLS-MOSUM", "Score-MOSUM", "fluctuation", "ME", "expF", "aveF", "supF")
mat = matrix(data = NA, nrow = NROW(test), ncol = 4)
colnames(mat) <- c("Test", "est", "p-valor", "resultado")
mat <- as.data.frame(mat)
i = 0
for(t in test) {
    i = i + 1
    mat[i , 1] <- t
        for(bool in c(FALSE, TRUE)) {
            if(bool) {
                if(t %in% c("Rec-CUSUM(d)", "OLS-CUSUM(d)" ,"Score-CUSUM(d)")) {
                    mat[i, "p-valor"] <- rt(test = gsub(t, pattern = "\\(d\\)", replacement = ""), 
                                            pval = bool, .dynamic = T)
                } else if (t %in% c("expF", "aveF", "supF")) {
                    mat[i, "p-valor"] <- ft(test = t, pval = bool)
                } else {
                    mat[i, "p-valor"] <- rt(test = t, pval = bool, .dynamic = F)
                }
            } else {
                if(t %in% c("Rec-CUSUM(d)", "OLS-CUSUM(d)" ,"Score-CUSUM(d)")) {
                    mat[i, "est"] <- rt(test = gsub(t, pattern = "\\(d\\)", replacement = ""), 
                                        pval = bool, .dynamic = T)
                } else if (t %in% c("expF", "aveF", "supF")) {
                    mat[i, "est"] <- ft(test = t, pval = bool)
                } else {
                    mat[i, "est"] <- rt(test = t, pval = bool)
                }   
            }
        }
        if(mat[i, "p-valor"] > 0.1) {
        mat[i, "resultado"] <- ""
        } else if(mat[i, "p-valor"] >= 0.05 & mat[i, "p-valor"] < 0.1) {
            mat[i, "resultado"] <- "."
        }else if (mat[i, "p-valor"] >= 0.01 & mat[i, "p-valor"] < 0.05) {
            mat[i, "resultado"] <- "*"
        } else if (mat[i, "p-valor"] >= 0.001 & mat[i, "p-valor"] < 0.01) {
            mat[i, "resultado"] <- "**"
        } else {
            mat[i, "resultado"] <- "***"
        }
    mat[, 2:3] <- sapply(mat[,2:3], as.numeric)
}
colnames(mat) <- c("Test", "Estadístico", "p-valor", "Significación")
print(xtable::xtable(mat[1:11,]), comment = FALSE)
```

En la columna Test, se identifican los test de quiebres estructural llevados a cabo siguiendo la nomenclatura usada por @Zeileis2002. La diferencia entre Rec-CUSUM y Rec-CUSUM(d) es que se permite la existencia de un rezago, ya que, @Society1988 muestran que los test CUSUM no pierden sus propiedades al relajar algunos supuestos, como trabajar con modelos dinámicos. En todos los casos que no se utilizan rezagos de la tasa de vacantes, se rechaza la hipótesis nula de invariabilidad en los parámetros, por tanto, no se rechaza la existencia de algún quiebre estructural en la relación entre vacantes y tasa de desempleo. Los únicos test que no rechazan H0 son Score-CUSUM y OLS-CUSUM(d) modelos que incluyen un parámetro autoregresivo de vacantes. En todos los casos, siguiendo a @Zeileis2004 se estimo la matriz de varianzas y covarianzas robusta ante la heteroscedasticidad y autocorrelación usando un estimador de kernel cuadrático HAC @Andrews1991 con un filtrado VAR(1) y una elección automática del ancho de banda basado en una aproximación AR(1)\footnote{El kernel génerico es $\omega_l = K(\frac{l}{B})$ con K la función de kernel y B el ancho de banda. Especificamente el kernel espectral tiene la siguiente forma $\omega_l = \frac{3}{z^2}(\frac{\sin(z)}{z} - \cos(z))$ siendo $l$ el rezago y $z = \frac{6\pi}{5}\frac{l}{B}$, ver @Andrews1991}.

En las figuras XXX en el apéndice podemos observar las fluctuaciones del proceso empírico y su comparación con la fluctuación del proceso límite. Esto nos da una idea de en que periodo debería estar el o los quiebres en los parámetros, básicamente todos los test utilizados en la tabla XXX comparten que la hipótesis nula de la no existencia de cambio estructural debería ser rechazada cuando el proceso empírico  se vuelve improbablemente superior a las fluctuaciones del proceso límite @Zeileis2002. 
<!-- DESCRIBIR! -->

Adicionalmente los test Score, permiten observar variabilidad en la varianza, al sobrepasar el umbral esta es estadísticamente signifiticativa al 5\%. Tanto en el test Score-CUSUM como Score-MOSUM la varianza muestra fluctuaciones entre 1990 y 1995, y en torno a 2010-2011, sobrepasando el umbral. Por otra parte, el test Score-CUSUM con rezagos con p-valor 0.5, muestra una varianza al límite del umbral, pero sin sobrepasarlo. Esto da indicio de que es posible plantear un modelo que no solo tome en cuenta los quiebres en los parámetros sino también en la varianza de los errores lo cual puede mejorar la estímación de los quiebres @BaiPerron2003, por ello se estiman modelos de quiebres tanto de parámetros como varianza siguiendo a @Zeileis2010.

```{r ftest, fig.cap="Estadísticos F", results='asis'}
print(xtable::xtable(mat[12:14,]), comment = FALSE)

```

Un segundo tipo de test, es el F, originalmente el test de @Chow1960. El cual define la hipótesis alternativa y require saber a priori en que momento cambian los parámetros. Sin embargo, es posible calcular el estadístico F para un rango de valores especificados, definiendo el tamaño mínimo del intervalo a considerar, en base a un parámetro de ancha de banda, h.

@Andrews1993 y @Andrews1994 sugieren 3 tipos de test F a considerar, supF, aveF y expF. En la tabla \@ref(fig:ftest) podemos observar el test, el valor del estadístico y el p-valor asociado. Se uso la misma matriz de variazas y covarianzas robusta que en el caso anterior. En los 3 casos, encontramos un quiebre estructural en torno a 1990-I, resultado en linea con @Urrestarazu1997.

```{r coefTestEstructural, fig.cap='Coeficientes de los diferentes períodos', results="asis", fig.align='center'}
library(fxregime)
# FXREGIME 
reg <- log(ind_vac) ~ log(td) + 1
# Buscamos los quiebres cada 5 años
mod_reg <- fxregimes(formula = reg, data = zoo(dt_ts, frequency = 4), h = 20, 
                     breaks = 5)
# confint(mod_reg, level = 0.95, vcov = kernHAC)
# Resúmen completo, primero re-estimar el modelo en los subperiodos y luego aplicando summary
mod_rf <- refit(mod_reg)
print(xtable(round(coef(mod_reg), 4)), comment = FALSE)
```

```{r vcovHAC-andrews, fig.cap='Ponderadores Andrews', results='asis', fig.align='center'}
get_model <- function(.mod, .mat = sandwich::vcovHAC) {
    a = data.table::rbindlist(
        lapply(.mod, function(x) {
        broom::tidy(
            lmtest::coeftest(x, .mat)
            )
    }), 
    use.names = TRUE, idcol = "modelo")
    setnames(a, old = names(a), 
            new = c("Modelo", "Término", "Estimación", "Estándar error", 
                    "Estadístico", "p-valor"))
    a[`p-valor` > 0.1, sigf := ""]
    a[between(`p-valor`, lower = 0.05, 0.1),   sigf := "."]
    a[between(`p-valor`, lower = 0.01, 0.05),  sigf := "*"]
    a[between(`p-valor`, lower = 0.001, 0.01), sigf := "**"]
    a[between(`p-valor`, lower = 0, 0.001),    sigf := "***"]
    a
}
print(xtable(get_model(.mod = mod_rf, .mat = sandwich::vcovHAC)), comment = FALSE, digits = 2)
```

Finalmente, pasamos a calcular todos los posibles periodos de quiebres estructurales de forma generalizada. Para ello se siguen los trabajos de @BaiPerron2003, @BaiPerron1998 y aplicaciones de @Zeileis2003, en especial a @Zeileis2010. Obtenemos 3 quiebres estructurales en los años 1990-II, 1996-II y 2013-II usando como función objetivo la log-verosimilitud negativa\footnote{Los quiebres son exactamente iguales si las variables se modelan en niveles o logaritmos. Es posible modelar otro cambio estructural eligiendo otra función objetivo como el BIC o RSS, sin embargo dicho intervalo es extremadamente amplio al computarlo con una matriz HAC, al elegir un máximo de 3 puntos de quiebres se obtienen los mismos resultados, si se fija un máximo de 4 se obtiene un quiebre adicional en 2004-I}. Por lo tanto, los 4 periodos de análisis son 1981-I.1990-II, 1990-III.1996-II, 1996-III.2013-I y 2013-II.2018.IV. En la tabla \@ref(fig:vcovHAC-andrews) observamos que en todos los periodos los parámetros muestran el signo correcto según la teoría, negativo. En todos los casos son estadísticamente significativos al 5\%, usando una matriz HAC con ponderadores de Andrews. Sin embargo, usando ponderadores de Kernel en el segundo periodo el p-valor es 0.12. Dada la existencia de autocorrelación y heteroscedasticidad, la elección de la matriz de ponderadores es relevante y puede generar variaciones en la significatividad estadística de algunos parámetros. Por ello las tablas \@ref(fig:vcovHAC-lumley), \@ref(fig:kernHAC), \@ref(fig:kernHAC-parzen) y \@ref(fig:NeweyWest) muestran diferentes ponderaciones para los distintos periodos. Es de notar que el único momento que podría ser discutible es entre 1990-III.1996-II, en el cual dependiendo la elección de la matriz el parámetro de la tasa de desempleo puede resultar no significativo, sin embargo, si se estima un modelo lineal pero sin varianza de los errores ni intercepto, la tasa de desempleo es estadísticamente significativa, con el signo del coeficiente negativo. En el resto de los casos los coeficientes son estadísticamente significativos al nivel $\alpha = 0.05$, al igual que las varianzas de cada periodo.

```{r BCtest, fig.cap="Curvas de Beveridge para cada periodo. Obtenidas mediante test de quiebre estructural", fig.align="center"}
# Hacer una función, esto es muy manual.
dt[fecha <= "1990-04-01", decada_test := "1-periodo"
   ][between(fecha, "1990-07-01", "1996-04-01"), decada_test := "2-periodo"
     ][between(fecha, "1996-07-01", "2013-04-01"), decada_test := "3-periodo"
       ][between(fecha, "2013-07-01", "2019-10-01"), decada_test := "4-periodo"]
p2 <- ggplot(dt, aes(y = ind_vac, x = td, color = decada_test)) + 
    geom_point() +
    # geom_path() +
    # geom_text(data = dt, aes(label = ano)) +
    labs(x = "Tasa de desempleo", y = "Índice de vacantes") +
    geom_smooth(method = "lm", formula = y ~ x) +
    theme(legend.position = "none")
p2
# Agregar el caso de 4 quiebres y por tanto 5 particiones usando breakpoint.
```

La figura @\ref(fig:BCtest) muestras las CB estimadas para cada periodo. En todos los casos se mantiene una relación negativa entre vacantes y desempleo. A la vez que se observan cambios de nivel y pendiente. El primer periodo desde 1981-I hasta 1990-II muestra una curva comparativamente más cercana al origen, que para 1990-III a 1996-II se aleja de forma paralela, lo mismo vuelve a suceder en 1996-III a 2013-II, denotando lo que podría ser un mercado laboral menos eficiente. El comportamiento se modifica a partir de 2013-III en donde la curva tiene un cambio paralelo y levamente de pendiente hacia el origen.
<!-- Profundizar esta explicación y cuales pueden ser las causas -->

## SVAR-SV

Por último, realizamos una estimación multivariada de un vector autoregresivo con parámetros variables y volatilidad estocástica siguiendo a @Primiceri2005, @Lubik2016 y @Lubik2016b, usando el algoritmo corregido por @Primiceri2015. Nos interesa analizar la variabilidad en los parámetros beta y en la matriz de varianzas y covarianzas, sumado al efecto de un shock por parte del producto sobre vacantes y desempleo\footnote{Es posible utilizar el test desarrollado por @Stock1998.}.

Usamos las primeras 36 observaciones, 9 años, desde 1981-I hasta 1989-IV para calibrar las distribuciones a priori, quedando un periodo efectivo desde 1990 hasta 2018. La restricción de identificación que se impone es que son los shocks desde el producto (shocks de productividad) los cuales afectan al desempleo y las vacantes laborales, con un rezago. Por lo tanto, el pib es la primer variable, seguido del índice de vacantes y la tasa de desempleo. El orden de la segunda y tercer variable, no es una restricción de identificación sino una normalización necesaria que puede modificar los resultados @Primiceri2005, sin embargo, en este caso el orden no genera diferencias.

\newpage
```{r svar-varianzas, fig.cap="Media posterior, cuantiles 16 y 84 de los desvíos estándar", fig.align="center", fig.height=8, fig.width=6}
# Análisis de la varianza de los parámetros
par(mfrow = c(3, 1))
make_plot(.fit = fit, .type = "vcv", .var = 1, .title = "pib")
make_plot(.fit = fit, .type = "vcv", .var = 2, .title = "vacantes")
make_plot(.fit = fit, .type = "vcv", .var = 3, .title = "desempleo")
```
\newpage

En la figura \@ref(fig:svar-varianzas) observamos los desvíos estandar variables a lo largo del tiempo de los residuos del modelo, se gráfica la media (posterior) y los cuantiles 16 y 84\footnote{Normalidad}. El gráfico presenta dos interesantes características. La primera es que las vacantes laborales son estables para el periodo, por lo cual bien podria imponerse una restricción sobre la matriz de varianzas y covarianzas. El segundo y más interesante es que si bien la magnitud es leve, parecen existir dos periodos desde 1990 hasta 2005 y desde 2005 en adelante con una transición suave, donde el primero presenta una mayor varianza tanto para el pib como la tasa desempleo. En el caso de las vacantes, no se observan diferencias.

```{r svar-parameters, fig.cap="Parámetros rezagados de las variables del modelo TVP-VAR", fig.align="center", out.width='1\\linewidth'}
par(mfrow = c(3, 2))
for(i in c("intercept", "lag1")) {
    for(j in 1:3) {
        make_plot(.fit = fit, .type = i, .var = j)
    }
}
```

En la figura \@ref(fig:svar-parameters) se observa que los parámetros se mantienen invariantes a lo largo del periodo de análisis, se muestra la estimación de un TVP-VAR(1) en vez de un TVP-VAR(2), dado que los resultados no se modifican y se facilita su visualización.

El resultado es robusto frente a diferentes especificaciones, con variables en niveles o en logaritmos los resultados no cambian. Adicionalmente se estimo un modelo bivariado con desempleo y vacantes, tanto en niveles como en logaritmos, obteniendo los mismos resultados. Esto estaría indicando la posibilidad de trabajar con un modelo de parámetros fijos y con volatilidad estocástica.

```{r FIR, fig.cap="FIR", fig.subcap=c('Shock en vacantes', 'Shock en desempleo'), out.width='.49\\linewidth', fig.asp=1, fig.ncol = 2}
# par(mfrow = c(1,2))
plot_irf(impulse = 1, response = 2)
plot_irf(impulse = 1, response = 3)
```

Por último analizamos el efecto de un shock en el producto, interpretado como un shock de productividad sobre vacantes y desempleo. En la figura \@ref(fig:FIR)a tenemos el shocks desde el producto hacia las vacantes laborales el efecto es positivo en todo momento, esto tiene sentido en la medida que una innovación de productividad, debería generar que la demanda laboral de las empresas se vea aumentado, en la medida que aumente el nivel de producción de la economía. Al contrario en la figura \@ref(fig:FIR)b observamos como el efecto del shock genera un efecto negativo en todo momento sobre la tasa de desemeplo, nuevamente esto es satisfactorio. Al mejorar la productividad de la economía, el desempleo debería disminuir en la medida que la economía es capaz de aumentar su producción, lo cual lleva a que las empresas aumenten su contratación. Los efectos de entrada o salida de personas a la PEA esta presente en ambos casos puesto que el índice de vacantes esta normalizado por la PEA, al igual que la tasa de desempleo.
<!-- MEJORAR LA EXPLICACIÓN Y ARGUMENTACIÓN -->
